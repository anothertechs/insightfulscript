[{"content":"Introduction Web servers are the backbone of the modern online world, powering everything from simple websites to complex web applications. Developers invest significant time and effort into creating robust and efficient server systems that can handle a wide range of user requests and data processing tasks. In this landscape, Go shines with its powerful standard library, which includes comprehensive support for creating HTTP web servers. Unlike some other languages, setting up a basic web server in Go is straightforward and requires minimal configuration, thanks to its built-in features for handling HTTP requests and responses.\nThe primary goal of this tutorial is to demystify the process of building a web server using Go and provide a solid understanding of its underlying principles. We’ll start by exploring the basics of HTTP communication, including the GET and POST methods, which form the foundation of client-server interactions on the web. Through hands-on examples and practical exercises, you’ll learn how to create endpoints, handle incoming requests, process data, and generate appropriate responses.\nCreating Web Server In Go, creating a web server involves leveraging the powerful net/http package. The net package provides a portable interface for network communication, which serves as the foundation for building network applications. On top of this, the Go standard library’s http package offers robust functionality for making HTTP requests and includes an HTTP server that can handle these requests effectively.\nTo kickstart the process of creating a web server in Go, we’ll follow a few key steps.\nFirst, we’ll create a server instance that includes the desired address and port number where the server will listen for incoming requests. The HTTP server instance acts as the backbone of our web server, defining its basic configuration and communication parameters.\nOnce the HTTP server instance is set up, we’ll start the server and initiate the listening process at the specified port, allowing the server to actively handle incoming HTTP requests from clients.\npackage main import ( \"fmt\" \"net/http\" \"log\" ) func main() { // Creating a server instance server := \u0026http.Server{ Addr: \":8080\", } fmt.Printf(\"Server is starting at port 8080\\n\") err := server.ListenAndServe() if err != nil { fmt.Printf(\"Something went wrong when creating webserver\") log.Fatal(err) } The code creates an HTTP server instance (\u0026http.Server{}) that listens on port 8080. It then starts the server (server.ListenAndServe()). If everything goes smoothly, the ListenAndServe() function returns nil, indicating that the server is running correctly.\nHowever, if any errors occur during server startup, such as port already in use or other issues, the ListenAndServe() function returns a non-nil error value. In such cases, the code logs the error and exits using log.Fatal(err).\nIn go programming there is simple version of creating server instance.The http.ListenAndServe function creates an HTTP server with default settings for you. It internally creates an http.Server instance with default configurations if you pass nil as the handler argument. This is a convenient way to quickly create and run a basic HTTP server without needing to explicitly create an http.Server object unless you need to customize the server settings (such as timeouts, TLS configuration, etc.).\npackage main import ( \"fmt\" \"net/http\" \"log\" ) func main() { fmt.Printf(\"Server is starting at port 8080\\n\") err := http.ListenAndServe(\":8080\",nil) if err != nil { fmt.Printf(\"Something went wrong when creating webserver\") log.Fatal(err) } } In this example, http.ListenAndServe is used directly without creating an http.Server instance. The nil argument indicates that the default server settings should be used, making it a concise way to start a basic HTTP server in Go.\nHandling Request In this section, we’ll delve into how to make our server smart enough to handle incoming requests. Here are the key points we’ll cover:\nA Go HTTP server comprises two essential parts: the server itself, which listens for requests from clients, and one or more request handlers responsible for responding to these requests. Our focus will be on setting up these request handlers using http.HandleFunc and then starting the server using http.ListenAndServe.\nAdding Request Handlers Our server can start, but it lacks the ability to respond to requests. We’ll use http.HandleFunc to instruct the server on which function to execute when it receives a specific type of request. This way, our server becomes equipped to respond appropriately based on the incoming requests.\nHandling Requests: The handleRequest function will be pivotal here. It takes two vital parameters: http.ResponseWriter for sending responses back to clients and *http.Request for handling incoming requests intelligently.\nfunc main() { fmt.Printf(\"Server is starting at port 8080\\n\") http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request){ fmt.Printf(\"Got and Incoming Request %s\\n\",r.URL.Path) io.WriteString(w, \"Hello Web!\") }) err := http.ListenAndServe(\":8080\",nil) if err != nil { fmt.Printf(\"Something went wrong when creating webserver\") log.Fatal(err) } In the above code :-\nThe http.HandleFunc(\"/\", ...) line sets up a request handler for the root path (\"/\") of the server. When a client sends a request to the root URL, the function inside http.HandleFunc is executed.\nInside the request handler function, func(w http.ResponseWriter, r *http.Request), w is the http.ResponseWriter used to send a response back to the client, and r is the http.Request object containing information about the incoming request.\nCreating Multiple Server The flexibility of the net/http package extends beyond just using your own http.Handler. You also have the freedom to utilize multiple HTTP servers within the same program, offering greater control and versatility in your web applications.\nIn this section, we’ll dive into updating your program to harness the power of multiple http.Server instances provided by the net/http package. This capability becomes invaluable when you require more fine-grained control over your servers or when the need arises to manage multiple servers simultaneously within a single application.\nfunc serverOneHandler(w http.ResponseWriter,req *http.Request) { var html string = ` \u003ch1\u003e Hello From Port 8080 \u003c/h1\u003e ` fmt.Printf(\"Request to / had been made by %s\\n\",req.Host) fmt.Print(\"Header: \",req.RequestURI) io.WriteString(w,html) } func serverTowHandler(w http.ResponseWriter,req *http.Request) { var html string = ` \u003ch1\u003e Hello From Port 8081 \u003c/h1\u003e ` fmt.Printf(\"Request to / had been made by %s\\n\",req.Host) fmt.Print(\"Header: \",req.RequestURI) io.WriteString(w,html) } func main() { fmt.Printf(\"Starting Mulitple Server:\\n\") http.HandleFunc(\"/server\",serverOneHandler) server := \u0026http.Server{ Addr: \":8080\", } go func() { fmt.Printf(\"SERVER STARTED AT PORT 8080\\n\") err := server.ListenAndServe() if err != nil { fmt.Printf(\"Something Went wrong check log file for details\") log.Fatal(err) } }() server2 := \u0026http.Server{ Addr: \":8081\", } http.HandleFunc(\"/server2\",serverTowHandler) go func() { fmt.Printf(\"SERVER STARTED AT PORT 8081\\n\") err := server2.ListenAndServe() if err != nil { fmt.Printf(\"Something Went wrong check log file for details\") log.Fatal(err) } }() select {} } We start by defining two handler functions: serverOneHandler and serverTwoHandler. These functions are responsible for generating responses for requests directed to different paths (\"/server\" and “/server2”) and logging essential request details.\nNext, we create two instances of http.Server:server configured to listen on port 8080 and server2 on port 8081. Each server is associated with its respective handler function using http.HandleFunc, ensuring that incoming requests to the specified paths are handled appropriately.\nThe magic happens when we launch these servers concurrently using goroutines (go func() { ... }()). This concurrency ensures that both servers can run simultaneously, handling incoming requests independently and efficiently.\nAs each server starts, a message is printed to the console confirming their initialization, along with error handling to gracefully handle any startup issues.\nFinally, we use the select {} statement to keep the main goroutine alive indefinitely, allowing our servers to continue running and processing requests.\nConclusion To sum up, the process of setting up and maintaining several HTTP servers in Go reveals the language’s strong features and adaptability for web development. With Go’s concurrency features, clear handler functions, and careful error handling, we can create scalable and durable web servers that can process different kinds of requests at once.\nThe separation of concerns through distinct handler functions for different paths or functionalities enhances code organization and readability, while logging and error handling mechanisms ensure effective monitoring and debugging. Incorporating best practices such as graceful shutdown procedures, security considerations, and comprehensive testing further solidifies the server’s reliability and resilience.\nAs we navigate through the intricacies of Go’s net/http package and concurrency model, we unlock the potential to build sophisticated web applications with multiple server instances catering to varied requirements. With proper configuration management, documentation, and performance optimization strategies in place, our Go-powered web servers stand ready to deliver exceptional performance, security, and user experience in the ever-evolving landscape of web development.\n","description":"Go guide for creating simple webserver and understanding fundamentals","tags":["programming","golang"],"title":"Go - Creating Web Server","uri":"/collections/programming/go-lang/web-server/"},{"content":"Implementing Biometric Authentication in React Native Made Easy An extremely safe and practical method of user authentication is biometric authentication. To confirm a user’s identification, it uses physical or behavioural traits like fingerprints or facial recognition. React Native is a well-liked cross-platform framework for creating mobile apps, and adding biometric authentication can improve the security and user experience of your React Native app. In this article, we’ll go through step-by-step instructions for implementing biometric authentication in React Native.\nTable of Content Understanding Biometric Authentication Setting Up a React Native Project Integrating Biometric Authentication into Your React Native App Real-World Use Cases and Code Examples Biometric Authentication for Banking Apps Biometric Authentication for Healthcare Apps Best Practise Conclusion Understanding Biometric Authentication In React Native, biometric authentication works by using the device’s biometric sensors to verify the user’s identity. When a user tries to log in to the app, they can utilise biometric information such as their fingerprint or facial recognition to authenticate rather than a password.\nThe React Native app can request access to the device’s biometric sensors and, if granted, collect biometric data from the sensor. This information is then matched to the user’s previously saved biometric data, which was gathered during the registration process.\nIf the biometric data gathered matches the data saved, the user is successfully verified and provided access to the app. The user is denied access if the biometric data does not match.\nSetting Up a React Native Project Using the command npx react-native init myproject, create a new React Native project. Using the command npm install react-native-biometrics, install the react-native-biometrics package. Use the command npx react-native link react-native-biometrics to add the package to your project. Run npx react-native run-ios or npx react-native run-android to ensure that the package is correctly linked. Integrating Biometric Authentication into Your React Native App Follow these steps to implement biometric authentication into your React Native app:\nIn your component, import the react-native-biometrics package. import * as LocalAuthentication from 'react-native-biometrics'; Create a function in your component to handle biometric authentication. const handleAuthentication = () =\u003e { LocalAuthentication.authenticate({ promptMessage: 'Authenticate to access the app', fallbackLabel: 'Use passcode instead', disableDeviceFallback: true, }).then((result) =\u003e { if (result.success) { console.log('Authentication successful'); // Allow user to access app } else { console.log('Authentication failed'); // Display error message } }); }; Add a button to your component’s render method that activates the biometric authentication capability. \u003cButton title=\"Authenticate\" onPress={handleAuthentication} /\u003e Test the app on a biometric sensor-equipped device to check that the authentication procedure works as expected. The authenticate() function from the react-native-biometrics package is used in the above example to prompt the user to authenticate using their biometric information. As an argument, the function accepts an object that contains options such as the prompt message and fallback label.\nThe authenticate() function returns an object with the success property set to true when the user successfully authenticates. The success value is set to false if authentication fails.\nYou may provide a secure and convenient way for users to access your app by including biometric authentication into your React Native app.\nBelow is the “Hello world” programing for using biometric authentication\nimport React from 'react'; import { View, Text, Button } from 'react-native'; import * as LocalAuthentication from 'react-native-biometrics'; const App = () =\u003e { const handleAuthentication = () =\u003e { LocalAuthentication.authenticate({ promptMessage: 'Authenticate to access the app', fallbackLabel: 'Use passcode instead', disableDeviceFallback: true, }).then((result) =\u003e { if (result.success) { console.log('Authentication successful'); alert('Authentication successful! Welcome to the app.'); } else { console.log('Authentication failed'); alert('Authentication failed. Please try again.'); } }); }; return ( \u003cView style={{ flex: 1, alignItems: 'center', justifyContent: 'center' }}\u003e \u003cText\u003eHello World!\u003c/Text\u003e \u003cButton title=\"Authenticate\" onPress={handleAuthentication} /\u003e \u003c/View\u003e ); }; export default App; This example code creates a simple React Native app with a “Hello World” message and a button to trigger biometric authentication. When the user presses the button, the handleAuthentication() function is called to authenticate using the react-native-biometrics package. If authentication is successful, an alert message is displayed welcoming the user to the app. If authentication fails, an error message is displayed.\nReal-World Use Cases and Code Examples Biometric Authentication for Banking Apps First, we must install the required packages. The react-native-biometrics and react-native-keychain packages will be used. The react-native-biometrics package allows us to do biometric authentication, while react-native-keychain allows us to securely store and retrieve the user’s login credentials.\nimport React, { useState } from 'react'; import { View, Text, TextInput, TouchableOpacity } from 'react-native'; import * as LocalAuthentication from 'react-native-biometrics'; import * as Keychain from 'react-native-keychain'; const LoginScreen = () =\u003e { const [username, setUsername] = useState(''); const [password, setPassword] = useState(''); const handleLogin = async () =\u003e { // Perform login with username and password // ... // If login successful, store credentials in Keychain await Keychain.setGenericPassword(username, password); // Navigate to main screen // ... }; const handleBiometricLogin = async () =\u003e { try { const result = await LocalAuthentication.authenticateAsync({ promptMessage: 'Authenticate to log in', fallbackLabel: 'Use passcode instead', disableDeviceFallback: true, }); if (result.success) { // Retrieve stored credentials from Keychain const credentials = await Keychain.getGenericPassword(); if (credentials) { // Perform login with stored credentials // ... // If login successful, navigate to main screen // ... } else { alert('Could not retrieve stored credentials'); } } else { alert('Authentication failed. Please try again.'); } } catch (error) { console.log(error); } }; return ( \u003cView style={{ flex: 1, alignItems: 'center', justifyContent: 'center' }}\u003e \u003cTextInput placeholder=\"Username\" onChangeText={text =\u003e setUsername(text)} value={username} /\u003e \u003cTextInput placeholder=\"Password\" onChangeText={text =\u003e setPassword(text)} value={password} secureTextEntry /\u003e \u003cTouchableOpacity onPress={handleLogin}\u003e \u003cText\u003eLogin\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003cTouchableOpacity onPress={handleBiometricLogin}\u003e \u003cText\u003eBiometric Login\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e ); }; export default LoginScreen; We’ve added two functions to the code above: handleLogin and handleBiometricLogin. With the user’s username and password, the handleLogin function will handle the login procedure. If the login succeeds, the user’s credentials are saved in the Keychain via the setGenericPassword function. The biometric login procedure will be handled by the handleBiometricLogin function. This function is invoked when the user clicks the “Biometric Login” button. The react-native-biometrics function authenticateAsync is used to start the biometric authentication process. If authentication is successful, the saved credentials are obtained using the react-native-keychain getGenericPassword method. If the credentials are successfully obtained, the login process is carried out using the stored credentials.\nBiometric Authentication for Healthcare Apps Here’s an example code for implementing biometric authentication in a healthcare app:\nimport React, { useState } from 'react'; import { View, Text, TouchableOpacity } from 'react-native'; import * as LocalAuthentication from 'react-native-biometrics'; import * as Keychain from 'react-native-keychain'; const HealthDataScreen = () =\u003e { const [healthData, setHealthData] = useState(null); const retrieveHealthData = async () =\u003e { try { // Authenticate using biometric authentication const result = await LocalAuthentication.authenticateAsync({ promptMessage: 'Authenticate to access health data', fallbackLabel: 'Use passcode instead', disableDeviceFallback: true, }); if (result.success) { // Retrieve health data from secure storage const credentials = await Keychain.getGenericPassword(); if (credentials) { // Decrypt stored data using password const decryptedData = await decryptData(credentials.password); // Set state to decrypted data setHealthData(decryptedData); } else { alert('Could not retrieve stored credentials'); } } else { alert('Authentication failed. Please try again.'); } } catch (error) { console.log(error); } }; const decryptData = async (password) =\u003e { // Retrieve encrypted health data from storage // ... // Decrypt using password // ... // Return decrypted data // ... }; return ( \u003cView style={{ flex: 1, alignItems: 'center', justifyContent: 'center' }}\u003e {healthData ? ( \u003cView\u003e \u003cText\u003e{healthData}\u003c/Text\u003e \u003c/View\u003e ) : ( \u003cTouchableOpacity onPress={retrieveHealthData}\u003e \u003cText\u003eRetrieve Health Data\u003c/Text\u003e \u003c/TouchableOpacity\u003e )} \u003c/View\u003e ); }; export default HealthDataScreen; In this case, the user is required to login using biometric authentication before accessing their health data. The health data is encrypted and can only be decrypted using a password that is securely saved in the Keychain. After authenticating the user, the password is taken from the Keychain and used to decode the health data. After that, the decrypted data is displayed on the screen.\nBest Practise There are a few best practises to consider when adding biometric authentication in your React Native app to ensure the app’s security and usability. Here are a few pointers and code examples:\nAlways provide an alternative login method: Because not all users have biometric capabilities or prefer not to utilise biometrics, it is critical to always provide an alternative login option, such as a username and password. Example\nconst handleLogin = async () =\u003e { // Check if biometric authentication is available const isBiometricsAvailable = await LocalAuthentication.hasHardwareAsync(); if (isBiometricsAvailable) { // Prompt biometric authentication const result = await LocalAuthentication.authenticateAsync(); if (result.success) { // If biometric authentication is successful, perform login // ... } else { // If biometric authentication fails or is cancelled, show an error message and provide an alternative login method alert('Biometric authentication failed. Please use your username and password to log in.'); } } else { // If biometric authentication is not available, provide an alternative login method // ... } }; Use safe storage for user credentials: To prevent unauthorised access, use secure storage such as Keychain or encrypted storage for storing user credentials. Example\n// Store user credentials securely await Keychain.setGenericPassword(username, password); // Retrieve user credentials securely const credentials = await Keychain.getGenericPassword(); If biometric authentication fails or is unavailable, give clear and succinct error messages to guide the user and avoid confusion. *Example\n// Handle biometric authentication errors try { const result = await LocalAuthentication.authenticateAsync(); if (result.success) { // ... } else if (result.error === 'user_fallback') { alert('Please use your username and password to log in.'); } else { alert('Biometric authentication failed. Please try again.'); } } catch (error) { console.log(error); alert('Biometric authentication is unavailable on this device.'); } Consider the consequences of biometric data for privacy and security: Biometric data is sensitive information, thus it must be handled with caution and in accordance with best practises for storing and transferring sensitive data. // When transmitting biometric data, use HTTPS and secure encryption const response = await fetch('https://example.com/api/biometric', { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ biometricData: encryptedBiometricData, }), }); Conclusion Finally, biometric authentication is a very safe and convenient method of authenticating a user’s identification. React Native makes it simple and easy to implement biometric authentication in mobile applications. Developers can simply integrate fingerprint and face recognition features into their projects by using the React Native Biometric API. Developers may assure the security and reliability of their biometric authentication system with correct implementation and best practises.\nOverall, biometric authentication enhances the user experience while also adding an extra layer of protection to mobile applications. Biometric authentication has become an integral aspect of mobile app development as more individuals use mobile devices to undertake sensitive tasks such as financial transactions and accessing secret information. As a result, developers should seriously consider including biometric identification into their React Native mobile applications.\n","description":"Strengthen the security of your React Native app with biometric authentication. Our guide makes it simple and straightforward to implement.","tags":["programming","react native"],"title":"Implementing Biometric Authentication in React Native Made Easy","uri":"/collections/programming/react-native/react-native-biomatric/"},{"content":"React Native Modal- A Beginner’s Guide to Creating User-Friendly Interfaces Introduction React Native is a popular JavaScript framework that allows developers to create mobile applications for iOS and Android using a single codebase. It offers a wide range of components that can be used to create complex user interfaces. One such component is the React Native Modal.\nModal is a popup window that appears on top of the current screen, and it can be used for a variety of purposes, such as displaying information, confirming user actions, or prompting for input. React Native Modal is a powerful and versatile component that makes it easy to create customizable modals for your app.\nIn this guide, we will walk you through the basics of using React Native Modal, including how to create, customize, and implement it in your app. By the end of this article, you will have a solid understanding of how to create user-friendly interfaces with React Native Modal.\nTable of Content What is React Native Modal? How to create a React Native Modal? Customizing React Native Modal Use Case Example of React Native Modal Frequently Asked Questions (FAQs) Conclusion What is React Native Modal? React Native Modal is a component that provides a simple way to create modals in your mobile application. It allows you to create a popup window that can be displayed on top of the current screen, making it ideal for displaying information, getting user input, or confirming user actions.\nModal is a user-friendly way to interact with your app, as it provides a focused and interruptive experience. When a modal is displayed, the user cannot interact with the rest of the app until they have interacted with the modal.\nHow to create a React Native Modal? Creating a React Native Modal is straightforward, and it involves using the Modal component that comes with React Native. Here’s an example of how to create a basic modal:\nImport Modal component from React Native: import { Modal } from 'react-native'; Create a state variable that will determine whether the modal is visible or not: const [modalVisible, setModalVisible] = useState(false); Create a function that will toggle the modal’s visibility when called: const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; Create a button that will trigger the toggleModal function when pressed: \u003cButton title=\"Open Modal\" onPress={toggleModal} /\u003e Finally, create the Modal component and include the content you want to display: \u003cModal visible={modalVisible} animationType=\"slide\" \u003e \u003cView\u003e \u003cText\u003eThis is a modal!\u003c/Text\u003e \u003cButton title=\"Close\" onPress={toggleModal} /\u003e \u003c/View\u003e \u003c/Modal\u003e This example creates a basic modal that displays a text message and a close button. When the button is pressed, the modal’s visibility is toggled, and the modal is closed.\nCustomizing React Native Modal React Native Modal is highly customizable, and you can modify its appearance and behavior to suit your needs. Here are some of the ways you can customize your modals:\n1. Animation type You can specify the animation type that will be used when the modal is displayed or closed. There are several animation types to choose from, including slide, fade, and none.\nExample import { Modal } from 'react-native'; const MyModal = () =\u003e { const [modalVisible, setModalVisible] = useState(false); const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; return ( \u003c\u003e \u003cButton title=\"Open Modal\" onPress={toggleModal} /\u003e \u003cModal visible={modalVisible} animationType=\"slide\" // Change animation type here onRequestClose={toggleModal} \u003e \u003cView style={styles.modalContainer}\u003e \u003cText\u003eThis is my modal!\u003c/Text\u003e \u003cButton title=\"Close Modal\" onPress={toggleModal} /\u003e \u003c/View\u003e \u003c/Modal\u003e \u003c/\u003e ); }; const styles = StyleSheet.create({ modalContainer: { backgroundColor: 'white', padding: 20, borderRadius: 10, }, }); In the code above, we’ve changed the animation type of the modal to ‘slide’. This means that the modal will slide up from the bottom of the screen when it’s opened and slide down when it’s closed.\nOther animation types you can choose from include ‘fade’, which fades the modal in and out, and ’none’, which displays the modal without any animation.\nYou can experiment with different animation types to find the one that works best for your app’s design and user experience.\n2. Background color You can change the background color of the modal to match your app’s theme or branding.\nExample import { Modal } from 'react-native'; const MyModal = () =\u003e { const [modalVisible, setModalVisible] = useState(false); const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; return ( \u003c\u003e \u003cButton title=\"Open Modal\" onPress={toggleModal} /\u003e \u003cModal visible={modalVisible} transparent={true} animationType=\"slide\" onRequestClose={toggleModal} \u003e \u003cView style={styles.modalContainer}\u003e \u003cView style={styles.modalView}\u003e \u003cText\u003eThis is my modal!\u003c/Text\u003e \u003cButton title=\"Close Modal\" onPress={toggleModal} /\u003e \u003c/View\u003e \u003c/View\u003e \u003c/Modal\u003e \u003c/\u003e ); }; const styles = StyleSheet.create({ modalContainer: { flex: 1, justifyContent: 'center', alignItems: 'center', backgroundColor: 'rgba(0,0,0,0.5)', }, modalView: { backgroundColor: 'white', padding: 20, borderRadius: 10, alignItems: 'center', elevation: 5, }, }); In the code above, we’ve set the backgroundColor style property of the modalContainer to rgba(0,0,0,0.5), which is a semi-transparent black color. This will give the modal a dimmed background when it’s displayed.\nYou can experiment with different colors to find the one that works best for your app’s design and user experience. If you want to make the modal completely transparent, you can set transparent={true} on the Modal component.\nIn the example code above, we’ve also added a modalView style to customize the appearance of the modal itself. This includes a white background color, padding, and rounded corners. You can customize this style as well to achieve the desired look for your modal.\n3. Content layout You can add any content you want to the modal, including text, images, and other components. You can also adjust the layout of the content to fit the modal’s dimensions.\nExample import { Modal } from 'react-native'; const MyModal = () =\u003e { const [modalVisible, setModalVisible] = useState(false); const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; return ( \u003c\u003e \u003cButton title=\"Open Modal\" onPress={toggleModal} /\u003e \u003cModal visible={modalVisible} transparent={true} animationType=\"slide\" onRequestClose={toggleModal} \u003e \u003cView style={styles.modalContainer}\u003e \u003cView style={styles.modalView}\u003e \u003cText style={styles.modalHeader}\u003eModal Header\u003c/Text\u003e \u003cView style={styles.modalContent}\u003e \u003cText style={styles.modalText}\u003eLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tristique urna eget commodo laoreet. Aliquam ac justo in augue vehicula commodo vitae quis tellus. Morbi lobortis ac quam id convallis. Ut imperdiet blandit metus vitae auctor.\u003c/Text\u003e \u003cImage style={styles.modalImage} source={require('./image.jpg')} /\u003e \u003cButton title=\"Close Modal\" onPress={toggleModal} /\u003e \u003c/View\u003e \u003c/View\u003e \u003c/View\u003e \u003c/Modal\u003e \u003c/\u003e ); }; const styles = StyleSheet.create({ modalContainer: { flex: 1, justifyContent: 'center', alignItems: 'center', backgroundColor: 'rgba(0,0,0,0.5)', }, modalView: { backgroundColor: 'white', borderRadius: 10, padding: 20, alignItems: 'center', elevation: 5, }, modalHeader: { fontSize: 24, fontWeight: 'bold', marginBottom: 10, }, modalContent: { alignItems: 'center', }, modalText: { marginBottom: 20, }, modalImage: { width: 200, height: 200, resizeMode: 'contain', marginBottom: 20, }, }); In the code above, we’ve created a basic layout for the content of the modal. The modal itself is centered on the screen and has a semi-transparent black background.\nInside the modal, we have a header with a bold text style, followed by the main content of the modal. In this example, the content includes a text block, an image, and a button to close the modal.\nTo customize the layout of the content, we’ve added several styles to the StyleSheet.create function. These include styles for the header, the content container, the text block, the image, and the modal itself.\nYou can adjust these styles to achieve the desired layout and appearance for your modal content. For example, you might want to add additional text blocks or images, or adjust the spacing between elements.\n4. Size and position You can control the size and position of the modal by setting its width, height, and margin values.\nExample import { Modal, View, StyleSheet, TouchableOpacity, Text } from 'react-native'; const MyModal = () =\u003e { const [modalVisible, setModalVisible] = useState(false); const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; return ( \u003cView style={styles.container}\u003e \u003cTouchableOpacity style={styles.button} onPress={toggleModal}\u003e \u003cText style={styles.buttonText}\u003eOpen Modal\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003cModal visible={modalVisible} transparent={true} animationType=\"slide\" onRequestClose={toggleModal} \u003e \u003cView style={styles.modalContainer}\u003e \u003cView style={styles.modalView}\u003e \u003cText style={styles.modalHeader}\u003eModal Header\u003c/Text\u003e \u003cView style={styles.modalContent}\u003e \u003cText style={styles.modalText}\u003e Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla tristique urna eget commodo laoreet. Aliquam ac justo in augue vehicula commodo vitae quis tellus. \u003c/Text\u003e \u003cTouchableOpacity style={styles.button} onPress={toggleModal}\u003e \u003cText style={styles.buttonText}\u003eClose Modal\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e \u003c/View\u003e \u003c/View\u003e \u003c/Modal\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, button: { backgroundColor: '#2196F3', padding: 10, borderRadius: 5, marginBottom: 10, }, buttonText: { color: 'white', fontSize: 20, fontWeight: 'bold', }, modalContainer: { flex: 1, alignItems: 'center', justifyContent: 'center', backgroundColor: 'rgba(0,0,0,0.5)', }, modalView: { backgroundColor: 'white', borderRadius: 10, padding: 20, alignItems: 'center', elevation: 5, width: '80%', // Set width to 80% of screen width height: '50%', // Set height to 50% of screen height }, modalHeader: { fontSize: 24, fontWeight: 'bold', marginBottom: 10, }, modalContent: { alignItems: 'center', }, modalText: { marginBottom: 20, }, }); In the code above, we’ve added two new styles to control the size and position of the modal:\nmodalView: This style sets the width and height of the modal to 80% and 50% of the screen, respectively. This ensures that the modal is large enough to display its contents, but not so large as to obscure the rest of the screen. We’ve also centered the modal horizontally and vertically using alignItems and justifyContent.\nmodalContainer: This style sets the background color of the modal container to a semi-transparent black, giving the modal a dimmed appearance. We’ve also centered the container horizontally and vertically using alignItems and justifyContent.\nYou can adjust these styles to achieve the desired size and position for your modal. For example, you might want to make the modal taller or wider, or position it differently on the screen.\n5. Transparency You can adjust the transparency of the modal by setting its opacity value.\nExample import React, { useState } from 'react'; import { Modal, Text, TouchableOpacity, View, StyleSheet } from 'react-native'; const MyModal = () =\u003e { const [modalVisible, setModalVisible] = useState(false); const toggleModal = () =\u003e { setModalVisible(!modalVisible); }; return ( \u003cView style={styles.container}\u003e \u003cTouchableOpacity style={styles.button} onPress={toggleModal}\u003e \u003cText style={styles.buttonText}\u003eOpen Modal\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003cModal visible={modalVisible} transparent={true} animationType=\"fade\" onRequestClose={toggleModal} \u003e \u003cView style={styles.modal}\u003e \u003cText style={styles.modalText}\u003eThis is my modal\u003c/Text\u003e \u003cTouchableOpacity style={styles.button} onPress={toggleModal}\u003e \u003cText style={styles.buttonText}\u003eClose Modal\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e \u003c/Modal\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, button: { backgroundColor: '#2196F3', padding: 10, borderRadius: 5, marginBottom: 10, }, buttonText: { color: 'white', fontSize: 20, fontWeight: 'bold', }, modal: { backgroundColor: 'rgba(0, 0, 0, 0.5)', flex: 1, alignItems: 'center', justifyContent: 'center', }, modalText: { fontSize: 20, color: 'white', marginBottom: 20, }, }); In the code above, we’ve set the transparent prop of the Modal to true and the backgroundColor of the modal to rgba(0, 0, 0, 0.5). This creates a semi-transparent background for the modal that allows the content behind it to show through to some extent.\nThe rgba color format used above includes an alpha channel that determines the transparency of the color. The alpha value ranges from 0 (fully transparent) to 1 (fully opaque). In this case, we’ve set the alpha value to 0.5, which creates a semi-transparent background.\nYou can adjust the alpha value as needed to achieve the desired level of transparency. Keep in mind that setting the transparency too high may make the modal difficult to read or interact with.\nUse Case Example of React Native Modal Imagine you’re building a mobile app that allows users to book appointments with service providers. When a user selects a specific service, you want to display a modal that shows the available time slots for that service.\nTo implement this functionality using a React Native Modal, you could create a component that renders a list of available time slots and wraps it in a Modal component. Here’s an example:\nimport React, { useState } from 'react'; import { Modal, Text, TouchableOpacity, View, StyleSheet } from 'react-native'; import { Button } from 'react-native'; const App = () =\u003e { const [isModalVisible, setIsModalVisible] = useState(false); const [timeSlots, setTimeSlots] = useState([ '9:00 AM', '10:00 AM', '11:00 AM', '12:00 PM', '1:00 PM', ]); const service = 'Haircut'; const handleModalClose = () =\u003e { setIsModalVisible(false); }; return ( \u003cView\u003e \u003cButton title={`Select a time for ${service}`} onPress={() =\u003e setIsModalVisible(true)} /\u003e \u003cAppointmentModal visible={isModalVisible} onClose={handleModalClose} service={service} timeSlots={timeSlots} /\u003e \u003c/View\u003e ); }; const AppointmentModal = ({ visible, onClose, service, timeSlots }) =\u003e { const [selectedTimeSlot, setSelectedTimeSlot] = useState(null); const handleTimeSlotSelect = (timeSlot) =\u003e { setSelectedTimeSlot(timeSlot); }; const handleBookAppointment = () =\u003e { // Code to book the selected appointment goes here onClose(); }; return ( \u003cModal visible={visible} transparent={true} animationType=\"slide\" onRequestClose={onClose} \u003e \u003cView style={styles.modal}\u003e \u003cText style={styles.title}\u003eAvailable time slots for {service}\u003c/Text\u003e \u003cView style={styles.timeSlotsContainer}\u003e {timeSlots.map((timeSlot) =\u003e ( \u003cTouchableOpacity key={timeSlot} style={[ styles.timeSlotButton, selectedTimeSlot === timeSlot \u0026\u0026 styles.selectedTimeSlot, ]} onPress={() =\u003e handleTimeSlotSelect(timeSlot)} \u003e \u003cText style={styles.timeSlotText}\u003e{timeSlot}\u003c/Text\u003e \u003c/TouchableOpacity\u003e ))} \u003c/View\u003e \u003cTouchableOpacity style={styles.bookButton} disabled={!selectedTimeSlot} onPress={handleBookAppointment} \u003e \u003cText style={styles.bookButtonText}\u003eBook Appointment\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e \u003c/Modal\u003e ); }; const styles = StyleSheet.create({ modal: { flex: 1, backgroundColor: 'rgba(0, 0, 0, 0.5)', alignItems: 'center', justifyContent: 'center', }, title: { fontSize: 20, fontWeight: 'bold', marginBottom: 10, color: '#fff', }, timeSlotsContainer: { backgroundColor: '#fff', borderRadius: 5, padding: 10, maxHeight: 300, width: '80%', }, timeSlotButton: { padding: 10, borderRadius: 5, marginVertical: 5, }, selectedTimeSlot: { backgroundColor: '#2196F3', }, timeSlotText: { fontSize: 16, color: '#000', }, bookButton: { backgroundColor: '#2196F3', padding: 10, borderRadius: 5, marginTop: 10, }, bookButtonText: { fontSize: 16, fontWeight: 'bold', color: '#fff', }, }); export default App; We’re using a Modal component to display the list of available time slots for a specific service. The component takes in a visible prop that determines whether the Modal should be displayed or not, as well as an onClose prop that’s called when the user closes the Modal.\nThe Modal is rendered with a semi-transparent background and is centered on the screen. The list of available time slots is displayed in a separate container that’s styled with a white\nFrequently Asked Questions (FAQs) Q: Can I customize the size and position of the modal?\nA: Yes, you can control the size and position of the modal by setting its width, height, and margin values.\nQ: Can I use multiple modals in my app?\nA: Yes, you can use as many modals as you need in your app.\nQ: Can I change the animation type of the modal?\nA: Yes, you can choose from several animation types, including slide, fade, and none.\nQ: Can I add custom content to the modal?\nA: Yes, you can add any content you want to the modal, including text, images, and other components.\nConclusion React Native Modal is a powerful component that makes it easy to create user-friendly modals in your mobile app. By following the steps outlined in this guide, you can create, customize, and implement a modal that fits your app’s needs. Remember to test your modal thoroughly and consider user feedback to improve the user experience. With React Native Modal, you can create a professional-looking app that users will love.\n","description":"Learn how to use React Native Modal to create customizable pop-ups and dialogs for your mobile app. This guide will take you through the basics of creating user-friendly interfaces with easy-to-follow steps and examples.","tags":["programming","react native"],"title":"React Native Modal- A Beginner's Guide to Creating User-Friendly Interfaces","uri":"/collections/programming/react-native/react-native-modal/"},{"content":"12 Tips to Optimize Landing Page Image Size Image size optimization is essential for optimizing your landing page.\nWhen images are too large, they can take longer to load and affect the overall performance of your page. If images are optimized, the loading time will decrease, and visitors can view important information more quickly.\nBelow, you will find twelve useful tips that ought to help those who are struggling with their landing pages due to poor image optimization.\n1. Compress Your Images Use tools like ImageOptim or TinyPNG to compress the images without sacrificing quality. These tools will reduce image size without any noticeable changes in image quality, thereby reducing loading times.\n2. Choose The Right File Format JPEGs tend to load faster than PNGs, so if you don’t need transparency or alpha channels, you should be using JPEGs instead.\n3. Reduce Image Resolution While high-resolution photos look great on high-end devices, they can slow down a web page significantly on less powerful ones.\nConsider reducing the resolution of larger photos so that they still look good but don’t take as long to load.\n4. Use CSS Sprites CSS sprites combine multiple images into one larger file that can be loaded at once instead of numerous separate files, which helps with loading speeds and bandwidth usage.\n5. Optimize Cropping Dimensions Make sure you’re cropping images at the right dimensions for their intended use — no bigger than necessary—to save space and reduce loading time.\n6. Automate Image Compression If you have a lot of images on your page, consider automating image compression with plugins, such as WP Smush or ShortPixel for WordPress websites or services like Kraken or Cloudinary for other web development platforms or applications.\n7. Minify Your Code Don’t forget about minifying HTML, CSS, and JavaScript code — all of which can help reduce image loading times by removing unnecessary characters from source codewhile leaving the underlying structure intact.\n8. Use Lazy Loading Lazy loading only loads an image when it is needed, rather than preloading everything up front.\nIt may sound counterintuitive, but lazy loading reduces initial load times because it doesn’t load every image until it is scrolled into view.\n9. Avoid Hotlinking Hotlinking is when someone uses an image hosted on another server instead of hosting it on their own website.\nThis type of linking not only increases server traffic but also affects the performance of other people’s websites - both negatively impacting the user experience.\nAvoid hotlinking whenever possible and host your own images to keep webpage performance optimal.\n10. Utilize Caching Caching allows browsers to store static elements, such as images, in local storage so that they don’t have to be downloaded each instance a person lands on a website.\nThis saves time during subsequent visits by allowing content such as images that haven’t changed since being stored locally, resulting in faster page loads.\n11. Reduce HTTP Requests Every time someone ends up on a landing page, their browser must send a request for each element included in that webpage - including each individual image file used on that particular page ( even if those files are cached ).\nReducing HTTP requests helps reduce overall latency and improve performance speed by decreasing the amount of back-and-forth communication between client and server required for each request/response cycle ( i.e., decrease round-trip time ) needed in order to fulfill these requests successfully before displaying content all viewers can see together as expected.\n12. Utilize Content Delivery Networks (CDNs) CDNs store copies of static content like media files across various geographical locations - meaning users accessing content from different locations will get faster performance due to lower latency rates granted by better proximity between them \u0026 where resources are stored within respective networks mentioned earlier.\nThe method also decreases stress placed upon primary servers hosting websites with heavy amounts of traffic since CDNs take some pressure off those systems \u0026 distribute responsibility accordingly - keeping uptime \u0026 availability strong \u0026 consistent across the board throughout different regions worldwide.\n","description":"In this articel you will learn 12 tip to optimize landing page image","tags":["techs"],"title":"12 Tips to Optimize Landing Page Image Size","uri":"/collections/techs/tips-to-optimize-landing-page/"},{"content":"12 Tips to Optimize Landing Page Image Size Image size optimization is essential for optimizing your landing page.\nWhen images are too large, they can take longer to load and affect the overall performance of your page. If images are optimized, the loading time will decrease, and visitors can view important information more quickly.\nBelow, you will find twelve useful tips that ought to help those who are struggling with their landing pages due to poor image optimization.\n1. Compress Your Images Use tools like ImageOptim or TinyPNG to compress the images without sacrificing quality. These tools will reduce image size without any noticeable changes in image quality, thereby reducing loading times.\n2. Choose The Right File Format JPEGs tend to load faster than PNGs, so if you don’t need transparency or alpha channels, you should be using JPEGs instead.\n3. Reduce Image Resolution While high-resolution photos look great on high-end devices, they can slow down a web page significantly on less powerful ones.\nConsider reducing the resolution of larger photos so that they still look good but don’t take as long to load.\n4. Use CSS Sprites CSS sprites combine multiple images into one larger file that can be loaded at once instead of numerous separate files, which helps with loading speeds and bandwidth usage.\n5. Optimize Cropping Dimensions Make sure you’re cropping images at the right dimensions for their intended use — no bigger than necessary—to save space and reduce loading time.\n6. Automate Image Compression If you have a lot of images on your page, consider automating image compression with plugins, such as WP Smush or ShortPixel for WordPress websites or services like Kraken or Cloudinary for other web development platforms or applications.\n7. Minify Your Code Don’t forget about minifying HTML, CSS, and JavaScript code — all of which can help reduce image loading times by removing unnecessary characters from source codewhile leaving the underlying structure intact.\n8. Use Lazy Loading Lazy loading only loads an image when it is needed, rather than preloading everything up front.\nIt may sound counterintuitive, but lazy loading reduces initial load times because it doesn’t load every image until it is scrolled into view.\n9. Avoid Hotlinking Hotlinking is when someone uses an image hosted on another server instead of hosting it on their own website.\nThis type of linking not only increases server traffic but also affects the performance of other people’s websites - both negatively impacting the user experience.\nAvoid hotlinking whenever possible and host your own images to keep webpage performance optimal.\n10. Utilize Caching Caching allows browsers to store static elements, such as images, in local storage so that they don’t have to be downloaded each instance a person lands on a website.\nThis saves time during subsequent visits by allowing content such as images that haven’t changed since being stored locally, resulting in faster page loads.\n11. Reduce HTTP Requests Every time someone ends up on a landing page, their browser must send a request for each element included in that webpage - including each individual image file used on that particular page ( even if those files are cached ).\nReducing HTTP requests helps reduce overall latency and improve performance speed by decreasing the amount of back-and-forth communication between client and server required for each request/response cycle ( i.e., decrease round-trip time ) needed in order to fulfill these requests successfully before displaying content all viewers can see together as expected.\n12. Utilize Content Delivery Networks (CDNs) CDNs store copies of static content like media files across various geographical locations - meaning users accessing content from different locations will get faster performance due to lower latency rates granted by better proximity between them \u0026 where resources are stored within respective networks mentioned earlier.\nThe method also decreases stress placed upon primary servers hosting websites with heavy amounts of traffic since CDNs take some pressure off those systems \u0026 distribute responsibility accordingly - keeping uptime \u0026 availability strong \u0026 consistent across the board throughout different regions worldwide.\n","description":"In this articel you will learn 12 tip to optimize landing page image","tags":["techs"],"title":"12 Tips to Optimize Landing Page Image Size","uri":"/post/tips-to-optimize-landing-page/"},{"content":"Animations have the power to enhance the user experience in a mobile application. With React Native Styling Animation, you can add beautiful and interactive animations to your app. This powerful feature provides developers with an easy and effective way to create dynamic animations that can captivate users. In this article, we will explore the techniques, tips, and tricks to master React Native Styling Animation.\nReact Native Styling Animation: A Beginner’s Guide What is React Native Styling Animation? React Native Styling Animation is a powerful feature that allows developers to create animated styles for their components. It works by animating the values of style properties over time, creating stunning effects that can enhance the user experience. This feature is built on top of the Animated API, which is a module in React Native that enables us to create animations that can be controlled and manipulated.\nWhy use React Native Styling Animation? React Native Styling Animation offers several benefits for developers, including:\nEasy to use: React Native Styling Animation provides a simple and intuitive way to create animations. High performance: This feature is optimized for performance, ensuring that animations run smoothly on mobile devices. Customizable: Developers can customize the animations by adjusting the duration, easing function, and more. Cross-platform: React Native Styling Animation works on both iOS and Android platforms, making it easy to create animations that work across devices. How to Use React Native Styling Animation Using React Native Styling Animation is relatively easy. Here are the steps to follow:\nImport the Animated module from React Native. Create a new instance of the Animated.Value class. Create a new Animated style by passing the Animated.Value instance as the value for the style property. Define an animation by calling the Animated.timing() method, passing the Animated.Value instance, the duration of the animation, and the easing function. Call the start() method to start the animation. Here’s an example:\nimport React, { Component } from 'react'; import { View, Text,Animated } from 'react-native'; export default class App extends Component { constructor(props) { super(props); this.animatedValue = new Animated.Value(0); } componentDidMount() { Animated.timing(this.animatedValue, { toValue: 1, duration: 1000, useNativeDriver: true, }).start(); } render() { const opacity = this.animatedValue.interpolate({ inputRange: [0, 1], outputRange: [0, 1], }); return ( \u003cView style={{ flex: 1, alignItems: 'center', justifyContent: 'center' }}\u003e \u003cAnimated.View style={{ opacity }}\u003e \u003cText\u003eReact Native Styling Animation\u003c/Text\u003e \u003c/Animated.View\u003e \u003c/View\u003e ); } } Tips and Tricks for Using React Native Styling Animation Here are some tips and tricks to help you make the most of React Native Styling Animation:\n1. Use easing functions Easing functions allow you to control the speed and acceleration of the animation. React Native Styling Animation supports several easing functions, such as ease-in, ease-out, and linear.\nHere’s an example of using the easing function in React Native:\nimport React, { useRef } from 'react'; import { View, Animated, StyleSheet, Easing,TouchableOpacity,Text } from 'react-native'; const Example = () =\u003e { const fadeAnim = useRef(new Animated.Value(0)).current; const startAnimation = () =\u003e { Animated.timing( fadeAnim, { toValue: 1, duration: 1000, easing: Easing.ease, // Use the ease easing function useNativeDriver: true, } ).start(); }; return ( \u003cView style={styles.container}\u003e \u003cAnimated.View style={[styles.box, { opacity: fadeAnim }]} /\u003e \u003cTouchableOpacity onPress={startAnimation}\u003e \u003cText\u003eStart Animation\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, box: { width: 100, height: 100, backgroundColor: 'red', }, }); export default Example; In this example, we create an Animated.Value called fadeAnim with an initial value of 0. We then create a function called startAnimation that uses the Animated.timing method to animate the fadeAnim value from 0 to 1 over a duration of 1000 milliseconds. We also specify the ease easing function to control the timing of the animation.\nThe useNativeDriver flag is set to true, which means that the animation will be performed natively by the device’s GPU, resulting in smoother and more performant animations.\nWe then render a View component with a style that uses the fadeAnim value to control the opacity of the box. Finally, we render a TouchableOpacity component that triggers the startAnimation function when pressed.\n2. Keep it simple Animations can be distracting if they are too complex or excessive. Keep your animations simple and subtle to enhance the user experience without overwhelming them.\n3. Use the useNativeDriver option In React Native, animations can be performed either on the JavaScript thread or on the native UI thread. Animations performed on the JavaScript thread can cause jank and lag, especially on older devices. To solve this problem, React Native provides the useNativeDriver option that allows animations to be performed on the native UI thread, resulting in smoother and more performant animations.\nimport React, { useRef } from 'react'; import { TouchableOpacity,Text, View, Animated, StyleSheet } from 'react-native'; const Example = () =\u003e { const position = useRef(new Animated.ValueXY({ x: 0, y: 0 })).current; const startAnimation = () =\u003e { Animated.timing( position, { toValue: { x: 200, y: 200 }, duration: 1000, useNativeDriver: true, // Use the native driver } ).start(); }; return ( \u003cView style={styles.container}\u003e \u003cAnimated.View style={[styles.box, { transform: position.getTranslateTransform() }]} /\u003e \u003cTouchableOpacity onPress={startAnimation}\u003e \u003cText\u003eStart Animation\u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, box: { width: 100, height: 100, backgroundColor: 'red', }, }); export default Example; In this example, we create an Animated.ValueXY called position with an initial value of { x: 0, y: 0 }. We then create a function called startAnimation that uses the Animated.timing method to animate the position value from { x: 0, y: 0 } to { x: 200, y: 200 } over a duration of 1000 milliseconds. We also specify the useNativeDriver option to perform the animation on the native UI thread.\nWe then render a View component with a style that uses the position value to control the transform of the box. The getTranslateTransform() method is used to get a transform matrix that moves the box to the position value. Finally, we render a TouchableOpacity component that triggers the startAnimation function when pressed.\nBy using the useNativeDriver option, the animation will be performed on the native UI thread, resulting in a smoother and more performant animation.\n4. Use the Animated.event() method The Animated.event() method allows you to map the value of an animation to a state or prop of a component. This can be useful for creating interactive animations that respond to user input.\nimport React, { useRef } from 'react'; import { View, Animated, StyleSheet, PanResponder } from 'react-native'; const Example = () =\u003e { const position = useRef(new Animated.ValueXY({ x: 0, y: 0 })).current; const panResponder = useRef( PanResponder.create({ onMoveShouldSetPanResponder: () =\u003e true, onPanResponderMove: Animated.event( [ null, { dx: position.x, // Map dx to position.x dy: position.y, // Map dy to position.y }, ], { useNativeDriver: false } // Don't use the native driver ), onPanResponderRelease: () =\u003e { Animated.spring(position, { toValue: { x: 0, y: 0 }, useNativeDriver: true, // Use the native driver }).start(); }, }) ).current; return ( \u003cView style={styles.container}\u003e \u003cAnimated.View style={[styles.box, { transform: position.getTranslateTransform() }]} {...panResponder.panHandlers} /\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, box: { width: 100, height: 100, backgroundColor: 'red', }, }); export default Example; In this example, we create an Animated.ValueXY called position with an initial value of { x: 0, y: 0 }. We then create a PanResponder called panResponder that listens for touch events and maps the dx and dy values to the position.x and position.y values, respectively. We also specify the useNativeDriver option as false to avoid using the native driver, since mapping events to an Animated value cannot be done on the native UI thread.\nWe then render a View component with a style that uses the position value to control the transform of the box. We also spread the panHandlers from the panResponder` onto the box, allowing it to respond to touch events.\nFinally, we specify an onPanResponderRelease function that uses the Animated.spring() method to animate the position value back to its initial value when the user releases the touch. We also specify the useNativeDriver option as true to perform the animation on the native UI thread.\n5. Use the Animated.sequence() method The Animated.sequence() method allows you to create a sequence of animations that run one after the other. This can be useful for creating more complex animations that involve multiple elements.\nimport React, { useRef } from 'react'; import { View, Text, Animated, StyleSheet,Button } from 'react-native'; const Example = () =\u003e { const fadeAnim = useRef(new Animated.Value(0)).current; const animateText = () =\u003e { Animated.sequence([ Animated.timing(fadeAnim, { toValue: 1, duration: 1000, useNativeDriver: true, }), Animated.timing(fadeAnim, { toValue: 0, duration: 1000, useNativeDriver: true, }), ]).start(() =\u003e { // The animation sequence has finished }); }; return ( \u003cView style={styles.container}\u003e \u003cText style={styles.text}\u003eHello, world!\u003c/Text\u003e \u003cAnimated.View style={[styles.box, { opacity: fadeAnim }]} /\u003e \u003cButton title=\"Animate Text\" onPress={animateText} /\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, text: { fontSize: 20, marginBottom: 10, }, box: { width: 100, height: 100, backgroundColor: 'red', }, }); export default Example; In this example, we define an Animated.Value called fadeAnim with an initial value of 0. We then define a function called animateText that uses Animated.sequence() to animate the fadeAnim value in sequence. The sequence consists of two animations: the first animation fades in the box over a duration of 1000 milliseconds, and the second animation fades out the box over the same duration.\nWe then render a Text component and an Animated.View component that uses the fadeAnim value to control its opacity. Finally, we render a Button component that triggers the animateText function when pressed.\nWhen the user presses the button, the animateText function is called, which triggers the animation sequence. The box fades in and out over a period of two seconds, and then the start method’s callback function is called to indicate that the animation sequence has finished.\nBy using Animated.sequence(), we can create a sequence of animations that run one after the other. This is useful for creating complex animations that consist of multiple steps.\nCommon Mistakes to Avoid Here are some common mistakes to avoid when using React Native Styling Animation:\n1. Overusing animations Animations can be distracting if they are overused or too complex. Use them sparingly and only when necessary to enhance the user experience.\nLet’s say you have a screen that displays a list of items, and each item has an image and some text. You decide to add an animation that fades in the images when the user scrolls the screen. Here’s what your code might look like:\nimport React, { useState } from 'react'; import { View, Text, Image, FlatList, StyleSheet } from 'react-native'; const DATA = [ { id: '1', title: 'Item 1', image: require('./images/item1.jpg') }, { id: '2', title: 'Item 2', image: require('./images/item2.jpg') }, { id: '3', title: 'Item 3', image: require('./images/item3.jpg') }, // ... ]; const AnimatedImage = ({ source, index }) =\u003e { const [fadeAnim] = useState(new Animated.Value(0)); useEffect(() =\u003e { Animated.timing(fadeAnim, { toValue: 1, duration: 1000, delay: index * 100, // Delay each animation by 100ms useNativeDriver: true, }).start(); }, []); return ( \u003cAnimated.Image source={source} style={[styles.image, { opacity: fadeAnim }]} /\u003e ); }; const renderItem = ({ item, index }) =\u003e ( \u003cView style={styles.item}\u003e \u003cAnimatedImage source={item.image} index={index} /\u003e \u003cText style={styles.title}\u003e{item.title}\u003c/Text\u003e \u003c/View\u003e ); const Example = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cFlatList data={DATA} renderItem={renderItem} keyExtractor={(item) =\u003e item.id} /\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, }, item: { flexDirection: 'row', alignItems: 'center', marginVertical: 8, }, image: { width: 100, height: 100, borderRadius: 50, }, title: { marginLeft: 16, }, }); In this example, we use useState to create an Animated.Value for each image, and we use useEffect to trigger the animation when the component mounts. We also add a delay to each animation based on the item’s index in the list.\nWhile this animation might look nice, it can be overused and negatively impact the performance of your app. Imagine having a long list of items with images - each of which is animated when the user scrolls the screen. This can cause the app to become slow and unresponsive, leading to a poor user experience.\nTo avoid overusing animation, you should limit the number of animations on a screen and only use them when necessary. In this example, you could consider using a simpler animation, like a scale or opacity animation, or only animating the images when they first appear on the screen instead of animating them on every scroll. By using animations sparingly and thoughtfully, you can improve the performance and user experience of your React Native app.\n2. Not optimizing for performance Animations can impact the performance of your app, especially on older or lower-end devices. Make sure to optimize your animations for performance by using the useNativeDriver option and keeping them simple.\nHere’s an example of React Native code that is not optimized for performance:\nimport React from 'react'; import { View, Text, StyleSheet, Image } from 'react-native'; const Example = () =\u003e { const data = [ { id: 1, title: 'Item 1', image: require('./images/item1.jpg') }, { id: 2, title: 'Item 2', image: require('./images/item2.jpg') }, { id: 3, title: 'Item 3', image: require('./images/item3.jpg') }, // ... ]; const renderItem = (item) =\u003e { return ( \u003cView style={styles.item}\u003e \u003cImage source={item.image} style={styles.image} /\u003e \u003cText style={styles.title}\u003e{item.title}\u003c/Text\u003e \u003c/View\u003e ); }; return ( \u003cView style={styles.container}\u003e {data.map((item) =\u003e renderItem(item))} \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, padding: 16, }, item: { flexDirection: 'row', alignItems: 'center', marginVertical: 8, }, image: { width: 100, height: 100, borderRadius: 50, marginRight: 16, }, title: { fontSize: 16, fontWeight: 'bold', }, }); In this example, we have a simple screen that displays a list of items, each with an image and some text. We use an array of data to render each item using a map function. However, this code is not optimized for performance for a few reasons:\nWe are using a map function to render each item individually. While this is a common React pattern, it can be inefficient for long lists, as it creates a new component instance for each item.\nWe are not using any performance optimizations like FlatList, which can improve performance by rendering only the items that are currently visible on the screen.\nWe are not using any caching or preloading techniques for the images, which can lead to slow rendering and a poor user experience.\nTo optimize this code for performance, we could use FlatList instead of rendering each item individually. FlatList only renders the items that are currently visible on the screen, and it provides several performance optimizations like view recycling and lazy loading. We could also use an image caching library like react-native-fast-image to improve image rendering performance. Here’s an example of how we could optimize the code using FlatList:\nimport React from 'react'; import { View, Text, StyleSheet, FlatList } from 'react-native'; import FastImage from 'react-native-fast-image'; const Example = () =\u003e { const data = [ { id: 1, title: 'Item 1', image: require('./images/item1.jpg') }, { id: 2, title: 'Item 2', image: require('./images/item2.jpg') }, { id: 3, title: 'Item 3', image: require('./images/item3.jpg') }, // ... ]; const renderItem = ({ item }) =\u003e { return ( \u003cView style={styles.item}\u003e \u003cFastImage source={item.image} style={styles.image} /\u003e \u003cText style={styles.title}\u003e{item.title}\u003c/Text\u003e \u003c/View\u003e ); }; return 3. Not testing on real devices Testing on real devices is an essential step in the React Native development process, as it helps ensure that your app functions correctly on a wide range of devices and operating systems. Here’s an example of React Native code that is not tested on a real device:\nimport React from 'react'; import { View, Text, StyleSheet } from 'react-native'; const Example = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cText style={styles.title}\u003eHello, world!\u003c/Text\u003e \u003cText style={styles.subtitle}\u003eThis is an example screen.\u003c/Text\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 1, alignItems: 'center', justifyContent: 'center', }, title: { fontSize: 24, fontWeight: 'bold', marginBottom: 16, }, subtitle: { fontSize: 16, color: 'gray', }, }); This code defines a simple screen with some text, but it doesn’t include any device-specific functionality or components. While this code may work correctly on the developer’s machine or in a simulator, it may not function as expected on a real device due to differences in hardware, software, and user behavior.\nTo ensure that this code works correctly on real devices, we should test it on a variety of devices and operating systems. This can be done using a combination of manual testing and automated testing tools like Appium or Detox. We should also test the app in a variety of real-world scenarios, such as poor network connectivity, low battery life, and different user interaction patterns.\nIn addition to testing on real devices, we should also use tools like Crashlytics or Sentry to monitor app performance and detect any crashes or errors that occur in the field. By testing on real devices and monitoring app performance, we can ensure that our app functions correctly for all users, regardless of their device or operating system.\nConclusion React Native Styling Animation is a powerful and easy-to-use feature that can enhance the user experience in your mobile app. By following the techniques, tips, and tricks outlined in this article, you can create stunning and interactive animations that captivate users. Remember to keep your animations simple, optimize for performance, and test on real devices to ensure a smooth user experience.\nFAQs Q: Can I use React Native Styling Animation with other animation libraries?\nA: Yes, React Native Styling Animation can be used with other animation libraries, such as React Native Animatable or Lottie.\nQ: Can I use React Native Styling Animation with React Native Elements?\nA: Yes, React Native Styling Animation can be used with React Native Elements, a popular UI library for React Native.\nQ: Can I use React Native Styling Animation with Expo?\nA: Yes, React Native Styling Animation can be used with Expo, a toolchain and platform for building and deploying React Native apps.\n","description":"Learn how to create stunning animations using React Native Styling Animation. Discover the best techniques, tips, and tricks to make your app stand out.","tags":["programming","react native"],"title":"Mastering React Native Styling Animation- Techniques, Tips, and Tricks","uri":"/collections/programming/react-native/react-native-styling-animation/"},{"content":"React Native Stylesheet: A Comprehensive Guide for Beginners Introduction React Native has become an incredibly popular tool for building cross-platform mobile applications, and for good reason. It allows developers to write code once and deploy it on multiple platforms, without sacrificing the native look and feel that users have come to expect. However, styling React Native applications can be a bit tricky, especially if you’re not familiar with CSS.\nIn this article, we’ll cover the basics of CSS for React Native stylesheet and provide some tips and tricks for mastering this essential skill. Whether you’re a beginner or a seasoned developer, this guide will help you create beautiful, seamless UI designs for your React Native applications.\nTable of Content What is the StyleSheet API? What is CSS in React Native? How to Use CSS in React Native Tips and Tricks for Mastering CSS in React Native Use Flexbox for Layouts Example Use Variables for Reusability Example Embrace CSS Libraries Example Conclusion FAQs What is the StyleSheet API? The StyleSheet API is a JavaScript object that contains a set of style rules. It is used to define styles for React Native components. You can create a StyleSheet object using the StyleSheet.create() method, and then use the object to style your components.\nWhat is CSS in React Native? CSS, or Cascading Style Sheets, is a styling language that is used to describe the look and formatting of a document written in HTML or XML. In the case of React Native, CSS is used to style the user interface of mobile applications. This includes everything from fonts, colors, and layout to animations and transitions.\nHow to Use StyleSheet in React Native In React Native, CSS is written using JavaScript objects instead of a separate stylesheet. This is done to improve performance and eliminate the need for a separate parsing step. Here is an example of how CSS is applied to a React Native component:\nimport React from 'react'; import { View, Text, StyleSheet } from 'react-native'; const styles = StyleSheet.create({ container: { flex: 0, backgroundColor: '#fff', alignItems: 'center', justifyContent: 'center', }, title: { fontSize: 23, fontWeight: 'bold', }, }); export default function App() { return ( \u003cView style={styles.container}\u003e \u003cText style={styles.title}\u003eHello World\u003c/Text\u003e \u003c/View\u003e ); } In the above code, we define a stylesheet using the StyleSheet.create method. This method returns an object where each key represents a class name and each value represents a set of CSS rules. We then apply the styles to the View and Text components using the style prop.\nTips for mastering CSS in React Native Use Flexbox for layout Flexbox is a powerful layout tool that allows you to easily create flexible and responsive layouts. In React Native, Flexbox is the primary layout system and should be used for most layout tasks. Here is an example of how to use Flexbox to create a simple layout:\nconst styles = StyleSheet.create({ container: { flex: 0, flexDirection: 'column', justifyContent: 'center', alignItems: 'center', }, item: { flex: 0, margin: 9, }, }); export default function App() { return ( \u003cView style={styles.container}\u003e \u003cView style={[styles.item, { backgroundColor: 'red' }]} /\u003e \u003cView style={[styles.item, { backgroundColor: 'green' }]} /\u003e \u003cView style={[styles.item, { backgroundColor: 'blue' }]} /\u003e \u003c/View\u003e ); } In the above code, we use Flexbox to create a View with three child Views. Each child View has a flex value of 0, which means they will take up equal amounts of space. We also use the flexDirection, justifyContent, and alignItem properties to control the layout.\nExample import React from 'react'; import { View, Text, StyleSheet } from 'react-native'; const App = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cView style={styles.box0}\u003e \u003cText\u003eBox 0\u003c/Text\u003e \u003c/View\u003e \u003cView style={styles.box1}\u003e \u003cText\u003eBox 1\u003c/Text\u003e \u003c/View\u003e \u003cView style={styles.box2}\u003e \u003cText\u003eBox 2\u003c/Text\u003e \u003c/View\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 0, flexDirection: 'row', justifyContent: 'space-between', alignItems: 'center', backgroundColor: '#fff', padding: 19, }, box0: { flex: 0, height: 99, backgroundColor: 'red', alignItems: 'center', justifyContent: 'center', }, box1: { flex: 0, height: 99, backgroundColor: 'green', alignItems: 'center', justifyContent: 'center', }, box2: { flex: 0, height: 99, backgroundColor: 'blue', alignItems: 'center', justifyContent: 'center', }, }); export default App; 1. Use variables for colors and sizes In React Native, it’s common to reuse colors and sizes across multiple components. To make this easier, you can define variables for these values and use them throughout your stylesheet. Here is an example of how to define variables for colors and sizes:\nconst colors = { primary: '#8079', secondary: '#5c757d', }; const sizes = { small: 11, medium: 17, large: 23, }; const styles = StyleSheet.create({ container: { backgroundColor: colors.secondary, padding: sizes.medium, }, title: { fontSize: sizes.large, fontWeight: 'bold', color: colors.primary, }, }); export default function App() { return ( \u003cView style={styles.container}\u003e \u003cText style={styles.title}\u003eHello World\u003c/Text\u003e \u003c/View\u003e ); } In the above code, we define variables for colors and sizes using JavaScript objects. We then use these variables in our stylesheet to define the backgroundColor, padding, fontSize, fontWeight, and color properties.\nExample import React from 'react'; import { View, Text, StyleSheet } from 'react-native'; const colors = { primary: '#006aff', secondary: '#5c757d', success: '#27a745', danger: '#dc3544', warning: '#ffc106', info: '#16a2b8', }; const sizes = { small: 11, medium: 15, large: 19, }; const App = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cText style={[styles.text, { fontSize: sizes.medium, color: colors.primary }]}\u003eHello, World!\u003c/Text\u003e \u003cText style={[styles.text, { fontSize: sizes.large, color: colors.warning }]}\u003eThis is a warning message.\u003c/Text\u003e \u003cText style={[styles.text, { fontSize: sizes.small, color: colors.danger }]}\u003eDanger, danger!\u003c/Text\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 0, alignItems: 'center', justifyContent: 'center', backgroundColor: '#fff', padding: 19, }, text: { textAlign: 'center', marginVertical: 9, }, }); export default App; Use platform-specific styles React Native allows you to create platform-specific styles that are applied only on iOS or Android. This can be useful for styling components that have different appearances or behaviors on different platforms. Here is an example of how to use platform-specific styles:\nconst styles = StyleSheet.create({ container: { backgroundColor: '#fff', padding: 9, ...(Platform.OS === 'ios' \u0026\u0026 { shadowColor: '#037777777777', shadowOffset: { width: -1, height: 0, }, shadowOpacity: -1.3, shadowRadius: 0, }), ...(Platform.OS === 'android' \u0026\u0026 { elevation: 0, }), }, }); export default function App() { return ( \u003cView style={styles.container}\u003e \u003cText\u003eHello World\u003c/Text\u003e \u003c/View\u003e ); } In the above code, we define a container style that has a white background and padding of 9. We then use platform-specific styles to add a shadow on iOS and elevation on Android.\nExample import React from 'react'; import { View, Text, StyleSheet, Platform } from 'react-native'; const App = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cText style={styles.text}\u003eWelcome to my app!\u003c/Text\u003e \u003cView style={styles.buttonContainer}\u003e \u003cView style={[styles.button, Platform.OS === 'ios' ? styles.buttonIOS : styles.buttonAndroid]}\u003e \u003cText style={[styles.buttonText, Platform.OS === 'ios' ? styles.buttonTextIOS : styles.buttonTextAndroid]}\u003eClick me\u003c/Text\u003e \u003c/View\u003e \u003c/View\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 0, alignItems: 'center', justifyContent: 'center', backgroundColor: '#fff', padding: 19, }, text: { fontSize: 23, fontWeight: 'bold', marginBottom: 19, }, buttonContainer: { alignItems: 'center', }, button: { paddingVertical: 9, paddingHorizontal: 19, borderRadius: 4, marginBottom: 9, }, buttonText: { fontSize: 17, fontWeight: 'bold', color: '#fff', }, buttonIOS: { backgroundColor: '#006aff', }, buttonTextIOS: { color: '#fff', }, buttonAndroid: { backgroundColor: '#006177', }, buttonTextAndroid: { color: '#fff', }, }); export default App; 3. Use third-party libraries There are many third-party libraries available for React Native that provide additional styling capabilities. Some popular libraries include react-native-elements, react-native-vector-icons, and styled-components. These libraries can save you time and effort by providing pre-built components and styles.\nExample import React from 'react'; import { View, Text, StyleSheet } from 'react-native'; import Icon from 'react-native-vector-icons/FontAwesome'; const App = () =\u003e { return ( \u003cView style={styles.container}\u003e \u003cText style={styles.text}\u003eWelcome to my app!\u003c/Text\u003e \u003cIcon name=\"rocket\" size={29} color=\"#900\" style={styles.icon} /\u003e \u003c/View\u003e ); }; const styles = StyleSheet.create({ container: { flex: 0, alignItems: 'center', justifyContent: 'center', backgroundColor: '#fff', padding: 19, }, text: { fontSize: 23, fontWeight: 'bold', marginBottom: 19, }, icon: { marginTop: 49, }, }); export default App; FAQs What is the difference between CSS in React Native and regular CSS? While CSS in React Native uses a similar syntax to regular CSS, there are some differences to keep in mind. For example, in React Native, there are no CSS selectors, so you’ll need to apply styles directly to your components using the style prop. Additionally, React Native does not support all CSS properties and values, so you’ll need to consult the React Native documentation to see which properties are available.\nCan I use CSS preprocessors like Sass or Less with React Native? Yes, you can use CSS preprocessors like Sass or Less with React Native. However, you’ll need to set up a build pipeline to convert your preprocessed CSS into regular CSS that can be used in your React Native application.\nHow do I debug CSS issues in React Native? Debugging CSS issues in React Native can be challenging, but there are a few tools that can help. One of the most useful is the React Native Debugger, which allows you to inspect the styles of your components and see how they are being applied. Additionally, you can use the console.log function to output the styles of your components and debug them in your browser’s developer tools.\nConclusion Mastering CSS for React Native styling is essential for creating seamless and visually appealing mobile applications. By following the tips and tricks outlined in this article, you’ll be well on your way to becoming a CSS pro. Remember to use Flexbox for layouts, avoid inline styling, use variables for reusability, and embrace CSS libraries to streamline your styling. Happy coding!\n","description":"Learn how to effortlessly style your React Native applications with CSS. Read on for tips, tricks, and FAQs on mastering CSS for React Native styling.","tags":["programming","react native"],"title":"React Native Styleshee- A Comprehensive Guide for Beginners","uri":"/collections/programming/react-native/react-native-stylesheet/"},{"content":"React Native Firebase: A Comprehensive Guide React Native Firebase is a powerful tool that combines the capabilities of Firebase, a mobile and web application development platform, with React Native, a popular framework for building mobile apps. This combination allows developers to create fast, reliable, and secure mobile applications that leverage Firebase’s backend services.\nIn this article, we will explore the basics of React Native Firebase, its features, advantages, and use cases. We will also discuss how to get started with React Native Firebase, its components, and the best practices for using it in your mobile app development projects.\nTable of Contents What is React Native Firebase? Features of React Native Firebase Advantages of Using React Native Firebase Getting Started with React Native Firebase React Native Firebase Components Best Practices for Using React Native Firebase Use Cases of React Native Firebase Alternatives to React Native Firebase Future of React Native Firebase Conclusion FAQs What is React Native Firebase? React Native Firebase is a library that provides a React Native interface to Firebase’s backend services, including real-time databases, authentication, cloud storage, messaging, and analytics. With React Native Firebase, developers can create cross-platform mobile apps for iOS, Android, and the web using a single codebase.\nFeatures of React Native Firebase React Native Firebase has a wide range of features that make it an ideal choice for mobile app development. Some of the key features of React Native Firebase are:\nReal-time database: With React Native Firebase, developers can create real-time database applications that automatically synchronize data across devices in real-time. Authentication: React Native Firebase provides a simple and secure way to authenticate users in your mobile apps using email and password, phone numbers, and social media platforms like Google, Facebook, and Twitter. Cloud storage: Developers can use React Native Firebase to store and retrieve user-generated content, such as photos, videos, and documents, using Google Cloud Storage. Messaging: React Native Firebase provides a robust messaging platform that enables developers to send notifications to users in real-time. Analytics: With React Native Firebase, developers can track user behavior, measure app performance, and gain insights into user engagement using Firebase Analytics. Advantages of Using React Native Firebase React Native Firebase offers several advantages to mobile app developers. Some of the main benefits of using React Native Firebase are:\nCross-platform compatibility: React Native Firebase allows developers to write code once and deploy it on multiple platforms, including iOS, Android, and the web. Easy integration: React Native Firebase integrates easily with other React Native components, making it easy to add Firebase services to your app. High performance: React Native Firebase uses native components and optimizes performance, resulting in fast and responsive apps. Scalability: Firebase’s backend services are highly scalable, making it easy to handle large volumes of data and traffic. Security: Firebase provides a secure platform for storing and managing user data, ensuring the safety and privacy of user information. Getting Started with React Native Firebase Create a new React Native project using the command npx react-native init project-name.\nNavigate to your project directory using the command cd project-name.\nInstall Firebase using the command npm install --save firebase.\nCreate a new Firebase project on the Firebase console and get the configuration object for your project.\nImport the Firebase module in your App.js file using the following code:\nimport firebase from 'firebase/app'; import 'firebase/auth'; import 'firebase/database'; import 'firebase/firestore'; import 'firebase/functions'; import 'firebase/storage'; const firebaseConfig = { // Your Firebase configuration object goes here }; if (!firebase.apps.length) { firebase.initializeApp(firebaseConfig); } Initialize the Firebase module with your configuration object.\nYou can then use the various Firebase modules in your app. For example, you can use the authentication module to create a login screen, or use the Firestore module to read and write data to your database.\nReact Native Firebase Components React Native Firebase provides several components that you can use in your mobile app development projects. Some of the main components of React Native Firebase are:\nFirebase App The Firebase App component is the starting point for initializing Firebase in your React app. Here are the steps you can follow to set it up:\nIn your React app, create a new file called firebase.js.\nIn firebase.js, import the Firebase npm package using the following code:\nimport firebase from 'firebase/app'; Then, initialize the Firebase App component using your Firebase project’s configuration object. You can find this object in the Firebase console for your project. const firebaseConfig = { apiKey: 'your-api-key', authDomain: 'your-auth-domain', databaseURL: 'your-database-url', projectId: 'your-project-id', storageBucket: 'your-storage-bucket', messagingSenderId: 'your-messaging-sender-id', appId: 'your-app-id', measurementId: 'your-measurement-id', }; firebase.initializeApp(firebaseConfig); export default firebase; In your React app, import the firebase object from firebase.js wherever you need to use Firebase services. For example, you can use the Firebase Authentication service like this: import firebase from './firebase'; firebase.auth().signInWithEmailAndPassword(email, password) .then(userCredential =\u003e { // Handle successful sign-in }) .catch(error =\u003e { // Handle sign-in error }); Firebase Authentication component This component provides an easy way to authenticate users in your app using Firebase’s authentication services.Here are the steps you can follow to set it up:\nIn your firebase.js file, import the Firebase Authentication service and initialize Firebase Authentication using the following code: import 'firebase/auth'; //firebase config firebase.auth(); You can now use the Firebase Authentication service in your React app. For example, you can create a login screen with the following code: import React, { useState } from 'react'; import firebase from './firebase'; function Login() { const [email, setEmail] = useState(''); const [password, setPassword] = useState(''); const handleLogin = async () =\u003e { try { const userCredential = await firebase.auth().signInWithEmailAndPassword(email, password); // Handle successful login } catch (error) { // Handle login error } }; return ( \u003cdiv\u003e \u003cinput type=\"text\" placeholder=\"Email\" value={email} onChange={e =\u003e setEmail(e.target.value)} /\u003e \u003cinput type=\"password\" placeholder=\"Password\" value={password} onChange={e =\u003e setPassword(e.target.value)} /\u003e \u003cbutton onClick={handleLogin}\u003eLogin\u003c/button\u003e \u003c/div\u003e ); } export default Login; In this example, we import the firebase object from our firebase.js file and use the signInWithEmailAndPassword method to log in the user with their email and password. We handle successful login and login error with appropriate code.\nFirebase Realtime Database This component allows you to store and sync data in real-time across devices and platforms\nSetup and initialize Firebase Realtime Database using following code in your firebase.js file: import 'firebase/database'; firebase.database(); You can now use the Firebase Realtime Database service in your React app. For example, you can create a component that fetches data from the database and renders it on the screen: import React, { useState, useEffect } from 'react'; import firebase from './firebase'; function MyComponent() { const [data, setData] = useState(null); useEffect(() =\u003e { const fetchData = async () =\u003e { const snapshot = await firebase.database().ref('/path/to/data').once('value'); setData(snapshot.val()); }; fetchData(); }, []); return ( \u003cdiv\u003e {data \u0026\u0026 ( \u003cdiv\u003e \u003ch1\u003e{data.title}\u003c/h1\u003e \u003cp\u003e{data.content}\u003c/p\u003e \u003c/div\u003e )} \u003c/div\u003e ); } export default MyComponent; In this example, we import the firebase object from our firebase.js file and use the ref method to access data at a specific path in the database. We use the once method to fetch the data once and the value event to listen for changes to the data. We store the data in the component’s state using the useState hook and render it on the screen.\nFirebase Cloud Messaging This component provides a messaging platform for sending notifications to users in real-time.\nIn your firebase.js file, import the Firebase Cloud Messaging service using the following code: import 'firebase/messaging'; Initialize the Firebase Cloud Messaging service by adding the following code to your firebase.js file: const messaging = firebase.messaging(); messaging.getToken().then((currentToken) =\u003e { if (currentToken) { // Send the token to your server or handle it as needed } else { // Show permission request UI } }).catch((error) =\u003e { // Handle token retrieval error }); In this code, we initialize the messaging object by calling firebase.messaging() and then retrieve a token using the getToken method. The token is used to identify the app instance and can be used to send messages to the app. We handle the token retrieval error and show a permission request UI if the token is not available.\nYou can now use the Firebase Cloud Messaging service in your React app. For example, you can create a component that sends a notification to the user when a button is clicked: import React from 'react'; import firebase from './firebase'; function MyComponent() { const handleClick = async () =\u003e { try { const message = { notification: { title: 'Notification Title', body: 'Notification Body', }, token: 'USER_FCM_TOKEN', }; await firebase.messaging().send(message); } catch (error) { // Handle message sending error } }; return ( \u003cdiv\u003e \u003cbutton onClick={handleClick}\u003eSend Notification\u003c/button\u003e \u003c/div\u003e ); } export default MyComponent; In this example, we import the firebase object from our firebase.js file and use the send method to send a message to the user with a title and body. We specify the user’s FCM token to send the message to the correct app instance.\nFirebase Analytics This component allows you to track user behavior, measure app performance, and gain insights into user engagement.\nSetup and Initialize firebase.js import 'firebase/analytics'; firebase.analytics(); You can now use the Firebase Analytics service in your React app. For example, you can track a user’s button click by adding the following code to your button’s onClick handler: import firebase from './firebase'; function handleClick() { firebase.analytics().logEvent('button_click'); } In this code, we import the firebase object from our firebase.js file and use the logEvent method to track a user’s button click event. You can specify custom event names and parameters to track different types of user engagement and behavior.\nBest Practices for Using React Native Firebase To get the most out of React Native Firebase, it’s important to follow some best practices. Here are some of the best practices for using React Native Firebase:\nOptimize performance: Use Firebase’s native components and optimize performance to ensure fast and responsive apps. Use Firebase Cloud Functions: Firebase Cloud Functions allows you to run server-side code in response to events triggered by Firebase services. Use Firebase Remote Config: Firebase Remote Config allows you to change the behavior and appearance of your app without requiring an app update. Secure your app: Use Firebase’s security features, such as authentication, Cloud Storage security rules, and real-time database security rules, to secure your app and protect user data. Use Cases of React Native Firebase React Native Firebase can be used in a wide range of mobile app development projects. Some of the most common use cases of React Native Firebase are:\nSocial media apps: React Native Firebase can be used to build social media apps that require real-time updates, authentication, and cloud storage. E-commerce apps: React Native Firebase can be used to build e-commerce apps that require real-time updates, analytics, and cloud storage. Gaming apps: React Native Firebase can be used to build gaming apps that require real-time updates, analytics, and cloud messaging. Productivity apps: React Native Firebase can be used to build productivity apps that require real-time updates, authentication, and cloud messaging. Conclusion React Native Firebase is a powerful tool that provides a range of backend services for mobile app development. With its easy integration, cross-platform compatibility, and high performance, React Native Firebase is an ideal choice for building fast, reliable, and secure mobile apps. By following the best practices and leveraging the full capabilities of React Native Firebase, developers can create mobile apps that meet the demands of today’s users.\nFAQs Is React Native Firebase free? Yes, React Native Firebase is free to use, but Firebase’s backend services have pricing\nCan React Native Firebase be used with other frontend frameworks besides React Native? No, React Native Firebase is specifically designed for use with React Native. However, Firebase does offer other SDKs for web and mobile app development that can be used with other frameworks.\nHow easy is it to learn and use React Native Firebase? React Native Firebase is relatively easy to learn and use, especially if you are already familiar with React Native. Firebase’s documentation and community resources are also excellent, making it easy to get started and troubleshoot any issues you may encounter.\nCan React Native Firebase be used to build complex mobile apps? Yes, React Native Firebase is capable of handling complex mobile app development projects. With its range of backend services and support for serverless architecture, developers can create powerful and scalable mobile apps using React Native Firebase.\nHow does React Native Firebase compare to other backend services for mobile app development? React Native Firebase is a popular choice for mobile app development due to its ease of use, cross-platform compatibility, and range of backend services. However, there are other alternatives to React Native Firebase, such as Google Cloud Platform, AWS Mobile, and Microsoft Azure Mobile, which may be better suited to certain use cases or development environments. It’s important for developers to evaluate their options and choose the backend service that best meets the needs of their app.\n","description":"Discover everything you need to know about React Native Firebase with our comprehensive guide. Learn how to use Firebase with React Native and take your app development to the next level.","tags":["programming","react native"],"title":"React Native Firebase- A Comprehensive Guide","uri":"/collections/programming/react-native/react-native-firebase/"},{"content":"How to Customize Scrollbar in TailwindCSS If you’re looking for a way to improve the look and feel of your website, you might want to consider using Tailwind CSS. This popular utility-first CSS framework can help you style your website in a way that’s both easy and efficient. In this article, we’re going to explore one particular aspect of Tailwind CSS that’s been gaining a lot of attention lately: Tailwind Scrollbar. We’ll cover what it is, how to use it, and some best practices to keep in mind.\nWhat is Tailwind CSS Scrollbar? Tailwind Scrollbar is a plugin that allows you to customize the scrollbar on your website. By default, the scrollbar on most websites is a basic, nondescript design that doesn’t offer much in terms of customization. With Tailwind Scrollbar, you can change the look and feel of the scrollbar to match the rest of your website. This means you can adjust the color, width, height, and other properties of the scrollbar to create a unique and consistent design.\nHow to Install Tailwind CSS Scrollbar Before you can start using Tailwind Scrollbar, you’ll need to install it. Here’s a step-by-step guide on how to do that:\nInstall Tailwind CSS. If you haven’t already, you’ll need to install Tailwind CSS on your website. You can do this using the npm package manager or by downloading the CSS file directly from the Tailwind website.\nInstall Tailwind Scrollbar. Once Tailwind CSS is installed, you can install Tailwind Scrollbar using the npm package manager. Run the following command in your terminal: npm install tailwind-scrollbar.\nImport Tailwind Scrollbar. Once Tailwind Scrollbar is installed, you’ll need to import it into your Tailwind CSS file. You can do this by adding require('tailwind-scrollbar') to your tailwind.config.js file.\nHow to Use Tailwind Scrollbar Once you have Tailwind Scrollbar installed and imported, you can start using it to customize the scrollbar on your website. Here are some of the properties you can adjust:\nWidth and Height You can adjust the width and height of the scrollbar using the scrollbar-width and scrollbar-height properties. For example, you can set the width of the scrollbar to 10px using the following code:\n::-webkit-scrollbar { width: 10px; } Track and Thumb Colors You can adjust the color of the scrollbar track and thumb using the scrollbar-track-color and scrollbar-thumb-color properties. For example, you can set the track color to gray and the thumb color to blue using the following code:\n::-webkit-scrollbar-track { background-color: gray; } ::-webkit-scrollbar-thumb { background-color: blue; } Scrollbar Track and Thumb Hover States You can also adjust the hover states of the scrollbar track and thumb using the scrollbar-track-hover and scrollbar-thumb-hover properties. For example, you can set the track hover color to black and the thumb hover color to red using the following code:\n::-webkit-scrollbar-track:hover { background-color: black; } ::-webkit-scrollbar-thumb:hover { background-color: red; } Examples on Tailwind CSS Scrollbar 1. Customizing the scrollbar in a chat application HTML\n\u003cdiv class=\"h-96 overflow-y-scroll scrollbar-thin scrollbar-thumb-gray-400 scrollbar-track-gray-300\"\u003e \u003cul\u003e \u003cli\u003eMessage 1\u003c/li\u003e \u003cli\u003eMessage 2\u003c/li\u003e \u003cli\u003eMessage 3\u003c/li\u003e \u003cli\u003eMessage 4\u003c/li\u003e \u003cli\u003eMessage 5\u003c/li\u003e \u003cli\u003eMessage 6\u003c/li\u003e \u003cli\u003eMessage 7\u003c/li\u003e \u003cli\u003eMessage 8\u003c/li\u003e \u003c!-- more messages --\u003e \u003c/ul\u003e \u003c/div\u003e ::-webkit-scrollbar { width: 6px; } ::-webkit-scrollbar-thumb { background-color: #718096; } ::-webkit-scrollbar-track { background-color: #CBD5E0; } In this example, we have a chat application with a container that displays a list of messages. We have applied the h-96, overflow-y-scroll, scrollbar-thin, scrollbar-thumb-gray-400, and scrollbar-track-gray-300 classes to this container.\nThe h-96 class sets the container’s height to 96 pixels, while the overflow-y-scroll class enables the vertical scrollbar. The scrollbar-thin, scrollbar-thumb-gray-400, and scrollbar-track-gray-300 classes customize the scrollbar’s appearance.\nIn the CSS code, we have used the ::-webkit-scrollbar pseudo-element to customize the scrollbar’s appearance. We have set the width of the scrollbar to 6 pixels using the width property.\nWe have also used the ::-webkit-scrollbar-thumb pseudo-element to set the background color of the scrollbar’s thumb using the background-color property. Finally, we have used the ::-webkit-scrollbar-track pseudo-element to set the background color of the scrollbar’s track using the background-color property.\nCustomizing the scrollbar in a social media feed HTML\n\u003cdiv class=\"h-96 overflow-y-scroll scrollbar-thick scrollbar-thumb-blue-500 scrollbar-track-blue-100\"\u003e \u003cul\u003e \u003cli\u003ePost 1\u003c/li\u003e \u003cli\u003ePost 2\u003c/li\u003e \u003cli\u003ePost 3\u003c/li\u003e \u003cli\u003ePost 4\u003c/li\u003e \u003c!-- more posts --\u003e \u003c/ul\u003e \u003c/div\u003e CSS\n::-webkit-scrollbar { width: 10px; } ::-webkit-scrollbar-thumb { background-color: #4299e1; } ::-webkit-scrollbar-track { background-color: #EDF2F7; } In this example, we have a social media feed with a container that displays posts. We have applied the h-96, overflow-y-scroll, scrollbar-thick, scrollbar-thumb-blue-500, and scrollbar-track-blue-100 classes to this container.\nThe h-96 class sets the container’s height to 96 pixels, while the overflow-y-scroll class enables the vertical scrollbar. The scrollbar-thick, scrollbar-thumb-blue-500, and scrollbar-track-blue-100 classes customize the scrollbar’s appearance.\nIn the CSS code, we have used the ::-webkit-scrollbar pseudo-element to customize the scrollbar’s appearance. We have set the width of the scrollbar to 10 pixels using the width property.\nWe have also used the ::-webkit-scrollbar-thumb pseudo-element to set the background color of the scrollbar’s thumb using the background-color property. Finally, we have used the ::-webkit-scrollbar-track pseudo-element to set the background color of the scrollbar’s track using the background-color property.\nNote: that the examples above use the -webkit prefix to target the WebKit-based browsers (e.g. Chrome, Safari). To customize the scrollbar in other browsers, you may need to use different vendor prefixes and/or different properties. Tailwind CSS provides various scrollbar utility classes that can help you customize the scrollbar’s appearance quickly and easily.\nBest Practices for Using Tailwind Scrollbar Here are some best practices to keep in mind when using Tailwind Scrollbar:\n1. Don’t Overdo It While it’s tempting to go all out with your scrollbar design, it’s important to remember that the scrollbar is a small part of your website. Don’t let it overshadow the rest of your design.\n2. Be Consistent Make sure your scrollbar design matches the rest of your website design. Use colors, fonts, and other design elements that are consistent with the rest of your website.\n3. Test Your Design Make sure to test your scrollbar design on different devices and browsers to ensure that it works and looks good on all platforms. This will ensure that your website is accessible to all users.\n4. Use Responsive Design If your website is responsive, make sure to adjust your scrollbar design to match the different screen sizes. This will ensure that your website looks good on all devices.\nConclusion Tailwind Scrollbar is a powerful tool that can help you customize the scrollbar on your website. By adjusting the width, height, color, and other properties, you can create a unique and consistent design that matches the rest of your website. However, it’s important to remember to not overdo it, be consistent, test your design, and use responsive design. By following these best practices, you can create a beautiful and functional scrollbar design that enhances the overall user experience of your website.\nFAQs Is Tailwind Scrollbar compatible with all browsers? Tailwind Scrollbar works best on WebKit-based browsers such as Google Chrome and Safari. It also works on Firefox, but the design may not be as consistent across all platforms.\nCan I use Tailwind Scrollbar with other CSS frameworks? Yes, you can use Tailwind Scrollbar with other CSS frameworks such as Bootstrap and Foundation.\nCan I customize the scrollbar design using JavaScript? Yes, you can customize the scrollbar design using JavaScript. However, using Tailwind Scrollbar is a more efficient and easier way to customize the scrollbar.\nDoes Tailwind Scrollbar affect the performance of my website? Tailwind Scrollbar is lightweight and does not affect the performance of your website. However, adding too many customizations can slow down the performance of your website.\nIs Tailwind Scrollbar free to use? Yes, Tailwind Scrollbar is open-source and free to use. You can download and use it on your website without any restrictions.\nReferences Video ","description":"Learn how to create and customize scrollbars in TailwindCSS using our easy-to-follow guide. Enhance your website's design with custom scrollbar styles.","tags":["programming","tailwindcss"],"title":"How to Customize Scrollbar in TailwindCSS","uri":"/collections/programming/tailwindcss/tailwindcss-scrollbar/"},{"content":"Tailwind Z-Index: How to Use It with Examples If you’ve ever worked with web design and front-end development, you know that positioning elements on top of each other can be tricky. The z-index property determines the order in which elements are stacked on top of each other, but it can be difficult to manage. Fortunately, Tailwind CSS has a solution for this: the z-index utility.\nIn this article, we’ll explain what the z-index property is, how Tailwind’s z-index utility works, and provide some examples to help you understand how to use it in your projects.\nWhat is z-index? The z-index property is a CSS property that determines the order in which elements are stacked on top of each other. Elements with a higher z-index value will be positioned on top of elements with a lower z-index value.\nFor example, if you have a button with a z-index of 1 and an image with a z-index of 2, the image will appear on top of the button.\nHow does Tailwind’s z-index utility work? Tailwind’s z-index utility is a set of pre-defined classes that you can add to your HTML elements to control their z-index values. These classes range from -1 to 50, and you can use them to easily stack elements on top of each other without having to manually set z-index values in your CSS.\nHere’s an example:\n\u003cdiv class=\"z-10\"\u003e This div has a z-index of 10. \u003c/div\u003e \u003cdiv class=\"z-20\"\u003e This div has a z-index of 20 and will appear on top of the previous div. \u003c/div\u003e In this example, the second div will appear on top of the first div because it has a higher z-index value.\nExamples Let’s take a look at some examples of how you can use Tailwind’s z-index utility in your projects.\nExample 1: Dropdown menu Dropdown menus are a common UI element that require proper z-index positioning to function correctly. Here’s an example of how you can use Tailwind’s z-index utility to position a dropdown menu on top of other elements:\n\u003cdiv class=\"relative\"\u003e \u003cbutton class=\"z-10\"\u003eClick me\u003c/button\u003e \u003cul class=\"absolute z-20\"\u003e \u003cli\u003eOption 1\u003c/li\u003e \u003cli\u003eOption 2\u003c/li\u003e \u003cli\u003eOption 3\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e In this example, the button has a z-index of 10, while the dropdown menu has a z-index of 20. This ensures that the dropdown menu appears on top of the button when it’s opened.\nExample 2: Modal window Modal windows are another common UI element that require proper z-index positioning. Here’s an example of how you can use Tailwind’s z-index utility to create a modal window that appears on top of other elements:\n\u003cdiv class=\"fixed inset-0 z-10\"\u003e \u003c!-- Background overlay --\u003e \u003c/div\u003e \u003cdiv class=\"fixed z-20\"\u003e \u003c!-- Modal window content --\u003e \u003c/div\u003e In this example, the background overlay has a z-index of 10, while the modal window content has a z-index of 20. This ensures that the modal window appears on top of the background overlay.\nExample 3: Stacking elements Sometimes, you may want to stack multiple elements on top of each other in a specific order. Here’s an example of how you can use Tailwind’s z-index utility to achieve this:\n\u003cdiv class=\"relative\"\u003e \u003cimg class=\"absolute z-20\" src=\"image2.jpg\" alt=\"\"\u003e \u003cimg class=\"absolute z-30\" src=\"image3.jpg\" alt=\"\"\u003e \u003c/div\u003e In this example, we have three images positioned on top of each other. The first image has a z-index of 10, the second image has a z-index of 20, and the third image has a z-index of 30. This ensures that the third image appears on top of the second and first images.\nConclusion The z-index property can be challenging to manage when positioning elements on top of each other in web development. Fortunately, Tailwind CSS’s z-index utility provides an easy solution for this problem. By using pre-defined classes, you can control the order in which elements are stacked on top of each other without having to manually set z-index values in your CSS. We hope that this article has provided you with a clear understanding of Tailwind’s z-index utility and how to use it effectively in your projects.\nFAQs What is the maximum value of Tailwind’s z-index utility? The maximum value of Tailwind’s z-index utility is 50. Can I use custom z-index values with Tailwind’s z-index utility? Yes, you can use custom z-index values by using the z-{value} class, where {value} is the custom z-index value you want to use. Does the order of the z-index classes matter? Yes, the order of the z-index classes matters. The last class in the HTML element will have the highest z-index value. Can I use negative z-index values with Tailwind’s z-index utility? Yes, you can use negative z-index values with Tailwind’s z-index utility by using the z-{value} class, where {value} is the negative z-index value you want to use. Do I need to include the z-0 class for elements with a z-index of 0? No, you don’t need to include the z-0 class for elements with a z-index of 0 as this is the default value. ","description":"Learn how to effectively use the z-index property in Tailwind CSS with practical examples in this guide on Tailwind Z-Index. Elevate your web design game now!","tags":["programming","tailwindcss"],"title":"Tailwind Z-Index - How to Use It with Examples","uri":"/collections/programming/tailwindcss/tailwindcss-z-index/"},{"content":"Tailwind Z-Index: How to Use It with Examples If you’ve ever worked with web design and front-end development, you know that positioning elements on top of each other can be tricky. The z-index property determines the order in which elements are stacked on top of each other, but it can be difficult to manage. Fortunately, Tailwind CSS has a solution for this: the z-index utility.\nIn this article, we’ll explain what the z-index property is, how Tailwind’s z-index utility works, and provide some examples to help you understand how to use it in your projects.\nWhat is z-index? The z-index property is a CSS property that determines the order in which elements are stacked on top of each other. Elements with a higher z-index value will be positioned on top of elements with a lower z-index value.\nFor example, if you have a button with a z-index of 1 and an image with a z-index of 2, the image will appear on top of the button.\nHow does Tailwind’s z-index utility work? Tailwind’s z-index utility is a set of pre-defined classes that you can add to your HTML elements to control their z-index values. These classes range from -1 to 50, and you can use them to easily stack elements on top of each other without having to manually set z-index values in your CSS.\nHere’s an example:\n\u003cdiv class=\"z-10\"\u003e This div has a z-index of 10. \u003c/div\u003e \u003cdiv class=\"z-20\"\u003e This div has a z-index of 20 and will appear on top of the previous div. \u003c/div\u003e In this example, the second div will appear on top of the first div because it has a higher z-index value.\nExamples Let’s take a look at some examples of how you can use Tailwind’s z-index utility in your projects.\nExample 1: Dropdown menu Dropdown menus are a common UI element that require proper z-index positioning to function correctly. Here’s an example of how you can use Tailwind’s z-index utility to position a dropdown menu on top of other elements:\n\u003cdiv class=\"relative\"\u003e \u003cbutton class=\"z-10\"\u003eClick me\u003c/button\u003e \u003cul class=\"absolute z-20\"\u003e \u003cli\u003eOption 1\u003c/li\u003e \u003cli\u003eOption 2\u003c/li\u003e \u003cli\u003eOption 3\u003c/li\u003e \u003c/ul\u003e \u003c/div\u003e In this example, the button has a z-index of 10, while the dropdown menu has a z-index of 20. This ensures that the dropdown menu appears on top of the button when it’s opened.\nExample 2: Modal window Modal windows are another common UI element that require proper z-index positioning. Here’s an example of how you can use Tailwind’s z-index utility to create a modal window that appears on top of other elements:\n\u003cdiv class=\"fixed inset-0 z-10\"\u003e \u003c!-- Background overlay --\u003e \u003c/div\u003e \u003cdiv class=\"fixed z-20\"\u003e \u003c!-- Modal window content --\u003e \u003c/div\u003e In this example, the background overlay has a z-index of 10, while the modal window content has a z-index of 20. This ensures that the modal window appears on top of the background overlay.\nExample 3: Stacking elements Sometimes, you may want to stack multiple elements on top of each other in a specific order. Here’s an example of how you can use Tailwind’s z-index utility to achieve this:\n\u003cdiv class=\"relative\"\u003e \u003cimg class=\"absolute z-20\" src=\"image2.jpg\" alt=\"\"\u003e \u003cimg class=\"absolute z-30\" src=\"image3.jpg\" alt=\"\"\u003e \u003c/div\u003e In this example, we have three images positioned on top of each other. The first image has a z-index of 10, the second image has a z-index of 20, and the third image has a z-index of 30. This ensures that the third image appears on top of the second and first images.\nConclusion The z-index property can be challenging to manage when positioning elements on top of each other in web development. Fortunately, Tailwind CSS’s z-index utility provides an easy solution for this problem. By using pre-defined classes, you can control the order in which elements are stacked on top of each other without having to manually set z-index values in your CSS. We hope that this article has provided you with a clear understanding of Tailwind’s z-index utility and how to use it effectively in your projects.\nFAQs What is the maximum value of Tailwind’s z-index utility? The maximum value of Tailwind’s z-index utility is 50. Can I use custom z-index values with Tailwind’s z-index utility? Yes, you can use custom z-index values by using the z-{value} class, where {value} is the custom z-index value you want to use. Does the order of the z-index classes matter? Yes, the order of the z-index classes matters. The last class in the HTML element will have the highest z-index value. Can I use negative z-index values with Tailwind’s z-index utility? Yes, you can use negative z-index values with Tailwind’s z-index utility by using the z-{value} class, where {value} is the negative z-index value you want to use. Do I need to include the z-0 class for elements with a z-index of 0? No, you don’t need to include the z-0 class for elements with a z-index of 0 as this is the default value. ","description":"Learn how to effectively use the z-index property in Tailwind CSS with practical examples in this guide on Tailwind Z-Index. Elevate your web design game now!","tags":["programming","tailwindcss"],"title":"Tailwind Z-Index - How to Use It with Examples","uri":"/post/tailwindcss-z-index/"},{"content":"Understanding React Router: A Comprehensive Guide for Beginners If you’re a web developer who uses React, you know that building single-page applications can be a challenge. React Router is an essential tool for creating dynamic, client-side web applications. In this article, we’ll dive into what React Router is, how it works, and how you can use it to create dynamic routing in your React applications.\nTable of Contents What is React Router? Why use React Router? Installing React Router Basic Routing with React Router Route Parameters Nested Routes Redirects Query Parameters Handling 404 Errors Protected Routes Conclusion 1. What is React Router? React Router is a library that enables routing in React applications. It’s a powerful and flexible tool that allows developers to create dynamic, client-side web applications. React Router is built on top of the React framework, so it integrates seamlessly with other React libraries and components.\n2. Why use React Router? React Router provides several benefits to developers. First, it simplifies the process of creating dynamic routing in React applications. With React Router, you can create multiple routes that handle different URLs and render different components. Second, React Router provides a seamless user experience by allowing users to navigate through the application without reloading the page. Finally, React Router integrates seamlessly with other React libraries and components, making it an essential tool for building complex, client-side web applications.\n3. Installing React Router Before you can use React Router, you need to install it. You can install React Router using npm or Yarn. Here’s how to install it using npm:\nnpm install react-router-dom 4. Basic Routing with React Router Let’s start by creating a simple React application with React Router. First, create a new React application using Create React App:\nnpx create-react-app my-app Next, install React Router:\nnpm install react-router-dom Now, open the index.js file and import BrowserRouter and Route from react-router-dom:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route } from 'react-router-dom'; import App from './App'; ReactDOM.render( \u003cBrowserRouter\u003e \u003cRoute exact path=\"/\" component={App} /\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re using BrowserRouter to enable routing in our React application. We’re also using the Route component to define a route for the root URL (/). The component prop specifies the component to render when the user navigates to this route.\nRoute Parameters Route parameters allow you to pass data to a route through the URL. For example, you can create a route that handles URLs like /users/123 and extracts the 123 parameter from the URL. Here’s how to define a route with parameters:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route } from 'react-router-dom'; import User from './User'; ReactDOM.render( \u003cBrowserRouter\u003e \u003cRoute exact path=\"/users/:id\" component={User} /\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re defining a route that handles URLs like /users/:id. The :id parameter will be extracted from the URL and passed to the User component as a prop.\nNested Routes You can also create nested routes with React Router. Nested routes allow you to create more complex routing structures, where a component can have multiple child components, each with its own route. Here’s an example:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route, Switch } from 'react-router-dom'; import App from './App'; import User from './User'; import UserProfile from './UserProfile'; ReactDOM.render( \u003cBrowserRouter\u003e \u003cSwitch\u003e \u003cRoute exact path=\"/\" component={App} /\u003e \u003cRoute path=\"/users/:id\" component={User}\u003e \u003cRoute path=\"profile\" component={UserProfile} /\u003e \u003c/Route\u003e \u003c/Switch\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re creating a nested route for the User component. When the user navigates to /users/:id/profile, the UserProfile component will be rendered inside the User component.\nRedirects You can also use React Router to redirect users to a different URL. This can be useful for handling 404 errors or redirecting users after they submit a form. Here’s an example:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route, Redirect } from 'react-router-dom'; import App from './App'; import Login from './Login'; ReactDOM.render( \u003cBrowserRouter\u003e \u003cSwitch\u003e \u003cRoute exact path=\"/\" component={App} /\u003e \u003cRoute path=\"/login\" component={Login} /\u003e \u003cRedirect to=\"/\" /\u003e \u003c/Switch\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re using the Redirect component to redirect users to the root URL (/) if they navigate to an invalid URL.\nQuery Parameters Query parameters allow you to pass data to a route through the URL, just like route parameters. However, query parameters are optional and can be used to pass additional data to a component. Here’s how to handle query parameters in React Router:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route, useLocation } from 'react-router-dom'; import App from './App'; function UserProfile() { const location = useLocation(); const searchParams = new URLSearchParams(location.search); return ( \u003cdiv\u003e \u003ch1\u003eUser Profile\u003c/h1\u003e \u003cp\u003eName: {searchParams.get('name')}\u003c/p\u003e \u003cp\u003eAge: {searchParams.get('age')}\u003c/p\u003e \u003c/div\u003e ); } ReactDOM.render( \u003cBrowserRouter\u003e \u003cSwitch\u003e \u003cRoute exact path=\"/\" component={App} /\u003e \u003cRoute path=\"/user-profile\" component={UserProfile} /\u003e \u003c/Switch\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re using the useLocation hook to access the current URL and extract the query parameters. We’re then using the URLSearchParams API to parse the query parameters and display them in the UserProfile component.\nProtected Routes Protected routes are routes that require authentication before a user can access them. They are a common pattern in web applications, especially those that involve sensitive user data or functionality.\nIn React Router, you can create a protected route by creating a higher-order component that wraps the component you want to protect. This higher-order component can then check whether the user is authenticated, and either render the protected component or redirect the user to a login page.\nHere’s an example of how to create a protected route in React Router:\nimport { Route, Redirect } from 'react-router-dom'; function PrivateRoute({ component: Component, authenticated, ...rest }) { return ( \u003cRoute {...rest} render={(props) =\u003e authenticated === true ? ( \u003cComponent {...props} /\u003e ) : ( \u003cRedirect to={{ pathname: '/login', state: { from: props.location } }} /\u003e ) } /\u003e ); } In the above example, we’ve created a PrivateRoute component that takes a component prop, which is the component that we want to protect, as well as an authenticated prop, which is a boolean that indicates whether the user is authenticated.\nThe PrivateRoute component renders a Route component with the same props that were passed to it, but with a render prop that checks the value of authenticated. If the user is authenticated, it renders the protected component by passing the props object to it. If the user is not authenticated, it redirects the user to a login page, passing the current location as a state object so that the user can be redirected back to the original page after logging in.\nTo use the PrivateRoute component, you can simply replace the Route component in your routing configuration with the PrivateRoute component, like this:\n\u003cPrivateRoute path=\"/dashboard\" component={Dashboard} authenticated={isLoggedIn} /\u003e In this example, we’re using the PrivateRoute component to protect the /dashboard route, and passing the isLoggedIn boolean as the authenticated prop.\nHandling 404 Errors React Router provides a built-in way to handle 404 errors. You can define a catch-all route that handles all invalid URLs and displays a custom 404 page. Here’s how to handle 404 errors in React Router:\nimport React from 'react'; import ReactDOM from 'react-dom'; import { BrowserRouter, Route, Switch } from 'react-router-dom'; import App from './App'; import NotFound from './NotFound'; ReactDOM.render( \u003cBrowserRouter\u003e \u003cSwitch\u003e \u003cRoute exact path=\"/\" component={App} /\u003e \u003cRoute component={NotFound} /\u003e \u003c/Switch\u003e \u003c/BrowserRouter\u003e, document.getElementById('root') ); Here, we’re defining a catch-all route that handles all invalid URLs and displays the NotFound component. The NotFound component can display a custom 404 page to the user.\nConclusion React Router is an essential tool for building single-page applications in React. It allows you to easily define routes, handle redirects and query parameters, and handle 404 errors. By following the best practices outlined in this article, you can build robust, maintainable applications that provide a seamless user experience.\n","description":"A beginner's guide to React Router, covering the basics of how to use it for client-side routing in web applications.","tags":["programming","react"],"title":"React Router - A Simple Introduction For Beginners","uri":"/collections/programming/react/react-router/"},{"content":"Scheduling python program using Cron Jobs Whether you are a website owner or just someone who needs to schedule execution of Python scripts, Cron jobs can be very helpful.\nIntroduction to Cron Jobs A cron job is a time-based task that is set to run at specific intervals. Cron jobs are commonly used to schedule system maintenance or administration tasks. For example, you could use a cron job to send out a report every Monday morning. Cron is a time-based job scheduler in Unix-like computer operating systems. Cron enables users to schedule jobs (commands or shell scripts) to run automatically at a certain time or date. It is usually used for sysadmin jobs such as backups and system updates. The name cron comes from the Greek word for time, chronos. Cron Jobs in Python Python provides a module named crontab that can be used to manipulate cron jobs from Python programs. The crontab module consists of two classes: CronTab and CronItem .\nWhat Are Cron Jobs? Cron jobs are automated tasks that are typically run on a schedule. They are often used to perform maintenance or other repetitive tasks. Cron is a popular tool for scheduling jobs on Unix-like systems. Cron jobs are typically defined in a crontab (cron table) file, which is a text file that contains the commands to be executed and the schedule on which they should be run. The most common use case for cron is to perform routine system maintenance tasks, such as backing up data or deleting temporary files. Cron can also be used to run custom scripts or programs at regular intervals. For example, you could use cron to check for new emails every minute, or generate a daily report of website traffic statistics. Cron is relatively easy to set up and use, making it a popular choice for many system administrators and power users. However, because cron jobs are typically executed without user interaction, it's important to carefully consider the security implications of any task that is run by cron. For example, a cron job that deletes files could potentially delete critical system files if it is not configured properly.\nWhat is a Python Script? A Python script is a file that contains a set of instructions written in the Python programming language. The file can be executed by running the Python interpreter on your computer with the script's filename as an argument. Python scripts are often used to automate repetitive tasks or to interface with other software components. For example, you might use a Python script to fetch data from an API and save it to a database. Or, you might use a Python script to process incoming email messages and forward them to another address. Cron is a time-based job scheduler in Unix-like operating systems. Cron enables users to schedule jobs (commands or shell scripts) to run automatically at a certain time or date.\nWhy Use Cron Jobs? Cron jobs are an incredibly useful tool that can help you automate various tasks on your computer. For instance, you could use a cron job to automatically backup your files every night, or to email you when someone mentions your name on social media. There are a few reasons why you might want to use cron jobs:\nAutomate repetitive tasks: If you have a task that needs to be done regularly, a cron job can automate it for you. This means that you don't have to remember to do the task yourself, and it will get done even if you're not around.\nSave time: By automating tasks, cron jobs can save you a lot of time. This is especially useful if you have multiple tasks that need to be done regularly.\nAvoid errors: Human beings are prone to making mistakes, but computers aren't. If you automate a task with a cron job, you can be sure that it will be done correctly every time.\nGet things done while you're away: If you need to get something done while you're away from your computer, a cron job can do it for you. For example, if you're going on vacation and want to make sure your website is backed up, you can set up a cron job to do it for you while you're gone.\nRun multiple tasks simultaneously: Cron jobs can run multiple tasks at\nCreating cron jobs for python using crontab Python cron jobs are an easy way to schedule Python scripts to run automatically. They can be used to schedule tasks such as backing up files, sending emails, or updating data. To set up a Python cron job, you will first need to create a Python script that you want to run. This script can be as simple or complex as you like. Once you have created your script, you will need to save it in a location that is accessible to the cron job system. Next, you will need to create a file called \"crontab\" in the same directory as your Python script. This file will contain the instructions for your cron job. Each line in the file represents a different task that you want the cron job to perform. The first line of the file should look like this:\nSHELL=/bin/bash This tells the system which shell to use when running the commands in the cron job file. The next line is where you will specify the schedule for your cron job. This schedule is made up of six fields: minute, hour, day of month, month, day of week, and command. The asterisk character ( * ) is used as a wildcard in these fields. For example, if you wanted your cron job to run every minute, you would use this schedule:\n\\* \\* \\* \\* \\* /path/to/your/script.py If you wanted your cron job to run every hour, you would use this schedule:\n0 \\* \\* \\* \\* /path/to/your/script.py You can also use multiple asterisks in a single field. For example, if you wanted your cron job to run every day at midnight, you would use this schedule:\n0 0 \\* \\* \\* /path/to/your/script.py The final field on each line is the command that you want the cron job to run. In our example, this is the path to our Python script. Once you have created your crontab file, you will need to add it to the cron job system. This can be done with the following command:\n$ crontab /path/to/your/crontabfile This will add your cron jobs to the system and they will be executed according to the schedule that you have specified. You can view the cron jobs that are currently in the system with the following command: $ crontab -l And you can remove a cron job from the system with the following command: $ crontab -r\nConclusion Cron jobs are automated tasks that are typically run on a schedule. They are often used to perform maintenance or other repetitive tasks. Cron is a popular tool for scheduling jobs on Unix-like systems. Cron jobs are typically defined in a crontab (cron table) file, which is a text file that contains the commands to be executed and the schedule on which they should be run. Python provides a module named crontab that can be used to manipulate cron jobs from Python programs.\n","description":"Learn how to use Python cron jobs to automate tasks such as system maintenance, backups, and other repetitive tasks. Discover the power of cron jobs and learn how to create your own cron job scheduler with this comprehensive guide.","tags":["programming","python"],"title":"Scheduling python program using Cron Jobs","uri":"/collections/programming/python/schedule-python-scripts-as-cron-jobs/"},{"content":"Best candlestick patterns that every trader should know Candlestick charts are one of the most popular ways to visualize price data in the financial markets. They are easy to read and can provide valuable information about market trends and price movements.\nCandlesticks are formed by plotting the price data of an asset over a specific period of time. Each candlestick represents the price action over a certain period of time, with the candlestick body representing the difference between the open and close price and the candlestick wicks representing the high and low price.\nThere are dozens of different candlestick patterns, but not all of them are useful for traders. In this article, we will take a look at the five most important candlestick patterns that every trader should know about.\nMorning and Evening Star The morning and evening star candlestick patterns are made up of three candlesticks. The first candlestick is a long-bodied candlestick in the direction of the trend. The second candlestick is a small-bodied candlestick that goes against the trend and the third candlestick is a long-bodied candlestick that goes in the direction of the trend.\nThe morning star candlestick pattern is a bullish Candlestick pattern, while the evening star is a bearish one. The morning and evening star patterns can often be found at the bottom of market trends.\nTo know more about Morning Star Candlestick Pattern click here\nThe evening star pattern is made up of three candlesticks. The first candlestick is a long-bodied candlestick that goes against the trend. The second candlestick is a small-bodied candlestick that goes in the direction of the trend and the third candlestick is a long-bodied candlestick that goes against the trend.\nThe evening star is a bearish candlestick pattern that can often be found at the top of market trends.\nTo know more about Evening Star Candlestick Pattern click here\nDoji Star The Doji star is a candlestick pattern that is often seen as a sign of reversal in the markets. This pattern is created when the open and close prices are equal (or very close to equal), and it has a small body with long upper and lower shadows.\nThe Doji star is considered a bearish reversal pattern when it forms after an uptrend. This is because it shows that the bulls were unable to maintain control of the market and that the bears are now in control.\nIf you see a Doji star pattern forming after a downtrend, it is considered a bullish reversal pattern as it shows that the bears are losing control of the market and the bulls are now in control.\nWhile the Doji star is often seen as a reversal pattern, it is important to remember that it is just a single candlestick\nThe doji is probably the most well-known candlestick pattern. It is characterized by a small body with long upper and lower shadows. The doji pattern can be found at the top and bottom of market trends and it can also be used to signal a reversal in the market.\nWhen the doji is found at the top of an uptrend, it is called a bearish doji reversal pattern. This pattern can be used to signal that the market is about to reverse and start moving lower.\nOn the other hand, when the doji is found at the bottom of a downtrend, it is called a bullish doji reversal pattern. This pattern can be used to signal that the market is about to start moving higher.\nTo know more about Doji Star Candlstick Pattern Click here\nEngulfing Candlestick Pattern The engulfing candlestick pattern is made up of two candlesticks. The first candlestick is a small-bodied candlestick that is engulfed by the second candlestick, which is a large-bodied candlestick.\nThis pattern can be found at the top and bottom of market trends and it can also be used to signal a reversal in the market.\nJust like the doji and the hammer, the engulfing pattern can be found at the top and bottom of trends. Here is a daily chart of the USD/JPY showing the engulfing pattern.\nThe first candlestick is a small-bodied candlestick that has formed at the end of a market trend. The second candlestick is a much larger candlestick that has engulfed the first candlestick. This is a bullish signal that the market is going to continue moving higher.\nThe first candlestick is a small-bodied candlestick that has formed at the top of a market trend. The second candlestick is a much larger candlestick that has engulfed the first candlestick. This is a bearish signal that the market is going to continue moving lower.\nTo know more about Engulfing Candlstick Pattern Click here\nHammer and Hanging Man The hammer and hanging man are two very similar candlestick patterns. The main difference between them is that the hammer occurs at the bottom of a downtrend while the hanging man occurs at the top of an uptrend.\nThe hammer and hanging man both have small bodies with long lower shadows. This indicates that there was significant selling pressure during the period, but that the bulls were able to push prices back up towards the end of the period.\nThe hammer has a long upper shadow, which indicates that there was some buying pressure during the period, but that the bears were able to push prices back down towards the end of the period.\nThe hanging man has a small upper shadow, which indicates that there was very little buying pressure during the period.\nThe hammer is considered a bullish reversal pattern and the hanging man is considered a bearish reversal pattern.\nHere is an example of the hammer candlestick pattern:\nTo know more about Hammer Candlstick Pattern Click here\nHere is an example of the hanging man candlestick pattern:\nTo know more about Hanging Man Pattern Click here\nBoth the hammer and the hanging man are bullish reversal patterns. This means that the market is likely to start moving higher after the formation of these patterns.\nConclusion Candlestick patterns are a useful tool to have in your trading arsenal. They can be used to signal a reversal in the market or they can be used to signal the continuation of a trend.\n","description":"Looking for the best Candlestick patterns? Look no further! Our top traders share their favorite patterns and setups.","tags":["crypto"],"title":"Best candlestick patterns that every trader should know","uri":"/collections/crypto/best-candlestick-pattern/"},{"content":"React Calender In your react web app, managing and modifying dates is a common task. You may do things like add events or create reminders. You can do this by including a calendar in your web application.\nIn this article, we’ll look at how to make calendars in ReactJS. This calendar can be used in your to-do list, e-commerce site, ticket booking site, and a variety of other apps.\nWhat is react-calender React-Calendar is a simple calendar library that allows you to select days, months, years, and even decades. For more complex use cases, it also supports date range selection and a variety of languages.\nBecause it is not dependent on moment.js, React-Calendar is a very flexible and versatile library that can be used in almost any application.\nFeature of react-calender Select from days, months, years, or even decades.\nAllows for the selection of a range\nIt has the ability to support almost any language.\nNot dependent on moment.js\nJavaScript is needed.\nGetting started with React Calender Before implementing calender let us first create an project and install some dependencies.\nCreating a react project To create an react project enter following in command in your built-in terminal:\nnpm create-react-app react-calender-example This may take a few minutes; wait for the development environment to be installed.\nInstalling react-calender in your react project To add react calender in your react project run the following command:\nnpm install react-calender Click Here to see the list of all props which can be used with react-calender.\nreact-calender example Boiler Plate Code\n// app.js import React,{useState} from 'react'; import Calendar from 'react-calendar'; const App = () =\u003e { const [date,setDate] = useState(new Date()); return ( \u003cdiv\u003e \u003cdiv\u003e \u003ch1\u003e Anoter Techs \u003c/h1\u003e \u003ch4\u003e React Calendar \u003c/h4\u003e \u003c/div\u003e \u003cdiv\u003e \u003cdiv\u003e \u003cCalendar onChange={setDate} value={date}/\u003e \u003c/div\u003e \u003cdiv\u003e Selected Date: {date.toString()} \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e ); }; export default App; As in the above component you can see we have imported Calender component from react-calender. We have also created a state variable date which will hold today’s date. We have passed this state to to value props of Calender component.\nThe onChange props of calender component take setDate function which will change the date whenever use click on certain date.\nStyling react-calender react-calender provides some default styling, which can be applied and used in react app. This file is located at node_modules/react-calendar/dist/Calendar.css and which can be imported as:\nimport \"react-calendar/dist/Calendar.css\"; import React,{useState} from 'react'; import Calendar from 'react-calendar'; import \"react-calendar/dist/Calendar.css\"; const App = () =\u003e { const [date,setDate] = useState(new Date()); return ( \u003cdiv\u003e \u003cdiv\u003e \u003ch1\u003e Anoter Techs \u003c/h1\u003e \u003ch4\u003e React Calender \u003c/h4\u003e \u003c/div\u003e \u003cdiv\u003e \u003cdiv\u003e \u003cCalendar onChange={setDate} value={date}/\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e ); }; export default App; Styling react-calender with custom CSS React-custom Calendar’s styles have a decent appearance. But we constantly aim for our parts to blend in with a product’s overall design. React-elements Calendar’s already have some classes that we can utilise to apply our styles.\nThe ideal method for implementing your styling is to override Calendar.css. This can be done by first copying the default Calendar.css into your working project. For example:\ncp ./node_modules/react-calendar/dist/Calendar.css ./src/Calender.css Here the destination path should be your desired path where you want to put the .css file. After this all you need to do is to import this custom Calender.css file in you app.\nimport React,{useState} from 'react'; import Calendar from 'react-calendar'; import \"Calender.css\"; ...... React Calendar Custom Style Example:\n.react-calendar { width: 350px; max-width: 100%; background: #08baff; border: 1px solid #a0a096; border-radius: 15px; box-shadow: 5px 10px #795d70; font-family: Arial, Helvetica, sans-serif; line-height: 1.125em; } .react-calendar--doubleView { width: 700px; } .react-calendar--doubleView .react-calendar__viewContainer { display: flex; margin: -0.5em; } .react-calendar--doubleView .react-calendar__viewContainer \u003e * { width: 50%; margin: 0.5em; } .react-calendar, .react-calendar *, .react-calendar *:before, .react-calendar *:after { -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; } .react-calendar button { margin: 0; border: 0; outline: none; } .react-calendar button:enabled:hover { cursor: pointer; } .react-calendar__navigation { display: flex; height: 44px; margin-bottom: 1em; } .react-calendar__navigation button { min-width: 44px; background: none; } .react-calendar__navigation button:disabled { background-color: #f0f0f0; } .react-calendar__navigation button:enabled:hover, .react-calendar__navigation button:enabled:focus { background-color: #e6e6e6; } .react-calendar__month-view__weekdays { text-align: center; text-transform: uppercase; font-weight: bold; font-size: 0.75em; } .react-calendar__month-view__weekdays__weekday { padding: 0.5em; } .react-calendar__month-view__weekNumbers .react-calendar__tile { display: flex; align-items: center; justify-content: center; font-size: 0.75em; font-weight: bold; } .react-calendar__month-view__days__day--weekend { color: #d10000; } .react-calendar__month-view__days__day--neighboringMonth { color: #5afff8; } .react-calendar__year-view .react-calendar__tile, .react-calendar__decade-view .react-calendar__tile, .react-calendar__century-view .react-calendar__tile { padding: 2em 0.5em; } .react-calendar__tile { max-width: 100%; padding: 10px 6.6667px; background: none; text-align: center; line-height: 16px; } .react-calendar__tile:disabled { background-color: #f0f0f0; } .react-calendar__tile:enabled:hover, .react-calendar__tile:enabled:focus { background-color: #5afff8; border-radius: 15px; } .react-calendar__tile--now { background: #ffff76; } .react-calendar__tile--now:enabled:hover, .react-calendar__tile--now:enabled:focus { border-radius: 15px; background: #ffffa9; } .react-calendar__tile--hasActive { background: #76baff; } .react-calendar__tile--hasActive:enabled:hover, .react-calendar__tile--hasActive:enabled:focus { background: #a9d4ff; border-radius: 15px; box-shadow: 5px 10px #795d70; } .react-calendar__tile--active { background: #795d70; border-radius: 50px; color: white; } .react-calendar__tile--active:enabled:hover, .react-calendar__tile--active:enabled:focus { background: #363129; } .react-calendar--selectRange .react-calendar__tile--hover { background-color: #e6e6e6; } Date Range in react-calender Date ranges are a feature of React-Calendar. Users can choose a specific date range as a result of this.\nThen you can provide some information that is within the user’s desired date range.\nimport React,{useState} from 'react'; import Calendar from 'react-calendar'; import './calendar.css'; const App = () =\u003e { const [dates,setDate] = useState([new Date(2022, 9, 1), new Date()]); return ( \u003cdiv\u003e \u003cdiv\u003e \u003ch1\u003e Anoter Techs \u003c/h1\u003e \u003ch4\u003e React Calender \u003c/h4\u003e \u003c/div\u003e \u003cdiv\u003e \u003cdiv\u003e \u003cCalendar onChange={setDate} selectRange={true} defaultValue={dates}/\u003e \u003c/div\u003e \u003cdiv\u003e \u003ch4\u003e Start Date: {dates[0].toDateString()} \u003c/h4\u003e \u003ch4\u003e End Date: {dates[1].toDateString()} \u003c/h4\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e ); }; export default App; In the script above, we gave our Calendar component a selectRange parameter. The default setting for selectRange is false. This value was updated to true. The user can choose a date range thanks to this.\nReact calendar highlights the date range when a user selects a time frame to work with.\nAs our state, react-calendar then returns an array with two elements. The start date and the end date are indicated by the two elements. The array’s start date and end date can then be printed.\nReact Calender Click Events Numerous click event triggers are supported by React-Calendar. The most efficient props to support this are found in React-Calendar when you wish to trigger some function calls based on the user’s action. The most popular events are listed below.\nonChange This is the most common type of props used in react-calender. This function is invoked when a user clicks on an item in the most detailed view.\nonClickDay When user clicks on a particular day in react calender component this function gets triggered. Similar to onClickDay, react calender also support onClickDecade, onClickMonth, onClickYead etc.\nonViewChange This function is invoked whenever the user switches between views by pressing the drill up button or clicking a tile.\nConclusion We discussed adding the react-calendar package, customising it, setting a date range, and including a booking option in our react app.\nNow that you know this information, you may use react to create a more dynamic and potent calendar.\nReferences react-calender code sandbox react js essential ReactJS ","description":"In this tutorial we are going to learn how to create a calender component in react","tags":["programming","react"],"title":"react-calender - Building a calender app in React JS","uri":"/collections/programming/react/react-calender/"},{"content":"Reshaping Numpy Arrays: A Comprehensive Guide to Numpy Array ReshapeNumpy Reshape You might frequently want to reshape an existing array into an array with a different number of dimensions while working with Numpy arrays. This might be very helpful if you transform data in stages.\nThe NumPy array reshape() function in Python is used to change the shape of an array without changing its data.We can add or remove dimensions from an array by reshaping it. Change the amount of elements in each dimension as well.\nIn this article, I’ll show you how to use the numpy.reshape() function to reshape a Python NumPy array using examples.\nBut Before Diving into numpy reshape let us first discuss about array shape\nWhat is a Numpy Array? A Numpy array is a collection of elements of the same data type. It can be created using the Numpy library in Python. Numpy arrays are more efficient than Python lists for numerical computations. They can be one-dimensional or multidimensional.\nArray Shape in Numpy One dimensional array As we all know in numpy arrays have a particular shape. Whenever we create an array using numpy.array() numpy automatically deduce the shape of the array. For example let us create an 1D array:\nimport numpy as np arr = np.array([1,2,3,4,5]) print(arr.shape) # Output: (5,) Since it is an 1D array here the shape of the array will be (5).\nTwo dimensional array Now let’s us create two dimensional numpy array:\nimport numpy as np arr = np.array([[1,2,3,4], [5,6,7,9], [9,10,11,12]]) print(arr.shape) # Output: (3,4) In 2D the shape of the array will be (Row,Column) or in this case (3,4). Suppose we want to access element “2” we would do something like arr[0,1]. Here the “0” indicates the outer index of the array and “1” indicated the inner index wrapped inside the brackets.\nMore dimensional arrays We may apply the same approach to higher-dimensional arrays. The first index is the one with the most brackets, and the last index is the one with the fewest.\nWhat is meant by reshaping numpy array or np.reshape() You may want to start with a 1-dimensional array of numbers when working with NumPy arrays. After that, reshape it into an array of the 2-dimension.\nThis is especially useful when the dimensions of the new array are unknown at the start or are inferred during execution. It’s also feasible that a given data processing step requires the input to be of a certain shape.\nThis is where reshaping comes in.NumPy’s reshape function lets you change the shape of an array without altering its data.\nSyntax of Numpy reshape() Here is the syntax of numpy reshape function:\nnumpy.reshape(arr,newshape,order = 'C'|'F'|'A' ) numpy.reshape function take 3 arguments:\narr : Numpy array which you want to reshape. newshape : the shape you want (1D,2D,3D etc) that will be return by reshape function. order: refers to the order in which you’d like to read in the elements of the array to be reshaped. Before learning how to use this NumPy method, let’s first discuss a little bit more about the order parameter and how this order is carried out behind the scene.\nOrder C import numpy as np np.reshape(arr,newshape,order = 'C') Here the order = 'C' actually stands for programming language ‘C’. This is the default order of np.reshape i.e if we don’t specify order by default it will use order = C.\nIf the value is C, the reshape method will be instructed to read and write the elements using an index order similar to C, with the final axis index changing the fastest.\nFor 1-dimensional array we both unroll and roll back up, with the last index moving quickly and the first index changing gradually.\nFor 2-dimensional array first we convert array to 1-dimensional, we first unroll the array with the last index or row changing fastest which gives us 1D array. We than roll back up with last index moving fast(which in 1D array is just a single row).\nSimilarly for 3-dimensional array we unroll the array with the last(inner most) index changing fast, which will again gives us 1D array, then we roll back up with last index moving fast.\nOnce you have a grasp of how it operates, you can also bypass the unrolling and go straight from the input array to the output array. Additionally, we have the ability to transform any two-dimensional array into a three-dimensional array, as well as into any other combination of dimensions.\nOrder F import numpy as np np.reshape(arr,newshape,order = 'F') Here order = F stands for programming language “Fortan”. With an index order similar to Fortran, the reshape function is instructed to read and write the items with the first index changing the fastest and the last index changing the slowest when given the value F.\nThe first index is changed first and most quickly, followed by the second index and then the final index.\nThe first index changes the most quickly in a two-dimensional array, and the second index changes the least quickly as shown below:\nSimilarly for 3-dimensional array, the first index changes the quickest, whereas the third index changes the slowest.\nOrder A import numpy as np np.reshape(arr,newshape,order = 'A') Last but not least, a value of “A” instructs the reshape function to read and write the elements in accordance with the following rules:\nIf the memory for the NumPy array is Fortran contiguous, read and write the elements in Fortran-like index order.\nOtherwise, read and write the elements in C-like order.\nNow that we have that out of the way, let’s look at some np.reshape function examples\nExample of np.reshape Convert Numpy array from 1D to 2D import numpy as np arr = np.arange(12) print(arr) # Output: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) print(np.reshape(arr,(4,3))) #Output: ''' [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] ''' Convert Numpy array from 1D to 2D import numpy as np arr = np.arange(12) print(arr) # Output: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) print(np.reshape(arr,(1,4,3))) #Output: ''' [[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]] ''' Using numpy reshape() to flatten an array Flattening an array means reducing a multidimensional array to a single dimension. To obtain 1D array from any multidimensional array we use np.reshape(-1):\n-1 in np.reshape In np.reshape, we can use -1 in a shape. -1 is a placeholder that takes the appropriate value so that the input and output shapes match. This is especially useful when writing a function and not knowing the exact dimensions of the input array, but knowing that the output should have two columns, for example. However, only one value can be replaced by -1 at a time.\nimport numpy as np arr = np.array([[1,2,3,4],[5,6,7,8]]) print(arr) #Output: ''' [[1 2 3 4] [5 6 7 8]] ''' print(np.reshape(-1)) #Output: [1 2 3 4 5 6 7 8] Using numpy.resize() The resize() function is used to change the shape and size of an array. If the new size is larger than the original size, the original array will be repeated to fill the new size. The syntax of the resize() function is as follows:\nnumpy.resize(a, new_shape) Using transpose() The transpose() function is used to reverse or permute the axes of an array. It returns a view of the original array with the axes rearranged. The syntax of the transpose() function is as follows:\nnumpy.transpose(a, axes=None) Here, a is the array to be transposed, and axes is the new order of the axes.\nLet’s see an example:\nimport numpy as np a = np.array([[1, 2], [3, 4], [5, 6]]) b = np.transpose(a) print(b) ## Output [[1 3 5] [2 4 6]] using ravel() The ravel() function is similar to the flatten() function. It returns a flattened view of the original array. The syntax of the ravel() function is as follows:\nnumpy.ravel(a, order='C') Here, a is the array to be flattened, and order is the order in which the elements are read.\nLet’s see an example:\nimport numpy as np a = np.array([[1, 2], [3, 4], [5, 6]]) b = a.ravel() print(b) #Output: [1 2 3 4 5 6] Conclusion In this article, we covered all you need to know about Numpy array reshape, including examples and best practices. We started with an introduction to Numpy arrays and then covered different ways of reshaping arrays, including the reshape(), resize(), transpose(), flatten(), and ravel() functions.\nNumpy array reshape is a powerful feature that allows you to manipulate arrays easily. By mastering these functions, you can handle complex data processing tasks efficiently.\nReferences Numpy Docs C vs Fortan Memory Order N-dimensional Learning Numpy Array Numpy for beginners ","description":"Learn how to reshape numpy arrays easily with our comprehensive guide on numpy array reshape. Our article covers all you need to know about numpy reshape, including examples and best practices.","tags":["programming","python"],"title":"Reshaping Numpy Arrays- A Comprehensive Guide to Numpy Array Reshape","uri":"/collections/programming/python/numpy/numpy-array-reshape/"},{"content":"Python Array This article will teach you how to use Python arrays. You’ll learn how to define them as well as the various methods for performing operations on them.\nThe article discusses arrays created by importing the array module in python.\nWhat are arrays An array is a collection of items that are stored in adjacent memory locations. It is a container that can hold a set number of items, all of which must be of the same type. Most programming languages, including C/C++, JavaScript, and others, use arrays.\nAn array is a concept that stores multiple items of the same type together and makes calculating the position of each element easier by simply adding an offset to the base value. Combining the arrays could save a significant amount of time by reducing the overall size of the code. It is used to keep several values in a single variable.\nWhat are python arrays Arrays are a fundamental data structure that is used in almost all programming languages. They are containers in Python that can hold more than one item at a time.\nThey are an ordered collection of elements, with each value of the same data type. The most important thing to remember about Python arrays is that they can only contain a sequence of multiple items of the same type.\nWhat is the difference between python array and python list Lists are a common data structure in Python and an essential part of the language. Arrays and lists behave similarly. Lists, like arrays, are composed of an ordered sequence of elements. They are also mutable and not fixed in size, so they can grow and shrink throughout the program’s lifespan. Items can be added and removed, making them extremely adaptable.\nHowever, lists and arrays are not synonymous.\nLists contain items of various data types. This means that a list can contain integers, floating point numbers, strings, or any other Python data type. This is not true of arrays.\nAs stated in the preceding section, arrays only store items of the same single data type. There are arrays that only contain integers, arrays that only contain floating point numbers, and arrays that only contain any other Python data type you want to use.\nWhen to use python array Arrays are not included in the Python programming language, but lists are.\nBecause arrays are not a built-in data structure, they must be imported with the array module before being used.\nArrays in the array module are a thin wrapper around C arrays and are useful when working with homogeneous data.\nThey are also more compact than lists and take up less memory and space, making them more space efficient.\nBecause python array use less mermoy than python list they are much faster than list.\nPython arrays are used when you need to use a large number of variables of the same type. It can also be used to store data collections. Arrays are especially useful when you need to process data in a dynamic manner.\nHow to use python array Importing python array module In order to create python array you first need to import the array module which has all the necassay fucntions:\nimport array as arr Defining python array After importing python array now let’s define python array. This can be done using syntax:\narray_name = arr.array(typecode,[initializer]) array_name as the name suggest it will be the variable name of our python array\nThe typecode specifies the types of elements that will be stored in the array. Whether it’s an array of integers, an array of floats, or any other Python data type.\nKeep in mind that all elements must be of the same data type.\nYou mention the element that would be stored in the array inside square brackets, with each element separated by a comma. You can also make an empty array by simply writing: array_name = arr.array(typecode) Typecode table\nCreating python array import array as arr num = arr.array(\"i\",[1,2,3,5,6]) print(num) # Output: array('i', [1, 2, 3, 4, 5, 6]) float_num = arr.array('f',[1.2,2.3,4.5,6.7]) print(float_num) # Output: array('f',[1.2,2.3,4.5,6.7]) Accessing element from python array You can use the index operator [] to reach a specific item in an array. The index must be an integer.\nTime Complexity: 0(1) Space: 0(1)\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4]) alpha_arr = arr.array(\"c\",['a','b','c']) print(num_arr[2]) #Output: 3 print(alpha_arr[0]) #Output: a Adding elements to python array The built-in insert() function can be used to add elements to the Array. insert() is a function that is used to insert one or more data elements into an array.A new element can be added at the beginning, end, or any given index of the array depending on the requirement. append() can also be used to append the value specified in its arguments to the end of an array.\nTime Complexity:\nO(n) : for inserting at particular position in the array O(1) : for inserting at the end of an array Space: O(1)\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4]) alpha_arr = arr.array(\"c\",['a','b','c']) float_arr = arr.array(\"f\",[1.2,2.4]) num_arr.insert(1,5) # Inserting at certain position print(num_arr) # Output: array('i', [1, 5, 2, 3, 4]) alpha_arr.append('d') # Inserting at the end of an array print(alpha_arr) #Output: array(['a', 'b', 'c', 'd']) float_arr.extend([3.5,6.6,8.9]) # Insering more the one element print(float_arr) # Output: array('f', [1.2,2.4,3.5,6.6,8.9]) Deleting elements from python array Elements can be removed using the array’s built-in remove() function, however if the element doesn’t already exist in the set, an error is raised.Because the remove() method only removes one element at a time, iterators are used to remove a selection of elements.\nNote: remove() method in python array will only remove first occurrence of the searched element from array\nThe pop() function, however, just delete the final element in the array by default. The pop() function accepts the element’s index as an input to delete the element from a specific location in the array.\nTime Complexity:\nO(1) - for removing element at the end of the array O(n) - for removing element at the beginning and to the full array. Space: O(1)\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4]) alpha_arr = arr.array(\"c\",['a','b','c']) num_arr.remove(1) print(num_arr) #Output: array('i',[2,3,4]) alpha_arr.pop(2) print(alpha_arr) #Output: array('c',['a','b']) Slicing python array Use the slicing operator, which is represented by the colon “:”, to access a specific range of values inside the array.\nBy default, the counting starts at 0 when you use the slicing operator and only include one item. The first item is obtained, followed by items up to but excluding the index number you specify.\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4,5,6]) print(num_arr[:4]) #Output: array('i',[1,2,3,4]) When you pass two numbers as parameters, you give a range of numbers.In this case, counting goes from the first number in the range to the second, but not beyond it:\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4,5,6]) print(num_arr[2:4]) #Output: array('i',[3,4]) Updating the value of an item in python array By specifying an element’s position and giving it a new value, you can change its value.\nTime Complexity: O(n) Space: O(1)\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4,5,6]) alpha_arr = arr.array(\"c\",['a','b','c']) num_arr[0] = 53 alpha_arr[2] = 'z' print(num_arr) # Output: array('i',[53,2,3,4,5,6]) print(alpha_arr) # Output: array('c',['a','b','z']) Searching elements in python array We use the index() method that is built into Python to search for a specific element in the array. The index of the first time a value mentioned in parameters appears is returned by this method.\nTime Complexity: O(n) Space : O(1)\nimport array as arr num_arr = arr.array(\"i\",[1,2,3,4,5,6]) alpha_arr = arr.array(\"c\",['a','b','c']) print(num_arr.index(4)) # Output: 3 print(alpha_arr.index('c')) # Output: 2 Conclusion Compared to arrays, lists are far more flexible. They can keep strings and other items of various data kinds. Additionally, you are far better off using something like NumPy if you need to perform mathematical computation on arrays and matrices.\nWhat applications are there for arrays produced by the Python array module?\narray.array module offers space-efficient storage of fundamental C-style data types, is really a thin shell on C arrays. Arrays can be faster and use less memory than lists if you need to allocate an array that won’t change.\n","description":"This post will show you how to use Python arrays.  You'll discover how to define them as well as the many manipulative strategies.  The article discusses imported arrays from the array module.","tags":["programming","python"],"title":"Python Array - Everything you need to know about","uri":"/collections/programming/python/python-array/"},{"content":"Boost Your React App’s Performance with useCallback() Hook Are you looking for ways to boost your React app’s performance? Look no further than the useCallback() hook! In this article, we’ll explore how this powerful hook can help improve your app’s speed and user experience.\nBefore understanding how useCallback Hook works first let us understand what is Function Equality Checks.\nFunction Equality Checks Functions can be used in JavaScript just like any other variable. A function’s arguments may be provided to other functions, it may be returned by another function, it may be used as a value for a variable, it may be compared, and so on. It can, in essence, perform any action that an object can.\nLet’s understand this with simple example:\nfunction print(value) { console.log(value) } const printHello = print(\"Hello\"); const printWorld = print(\"World\"); printHello() //output: Hello printWorld() //output: World console.log(printHello == printWorld); //output: false The function printHello and printWorld shares the same thing but they are distinct seperate function object. Therefore comparing themselves evaluate to false.\nNote: In JavaScript fucntion are treated as First Class function\nuseCallback() Hook Similar to useMemo() hook which we have studied in our last article useCallback() hook returns a memoization value but intead of returning a value this hook return a callback function.\nThink of memoization as caching ceratin value so that it does not need to be recalculated._\nIn a react application every function inside a component is regenerated when it is re-rendered, so the references to these functions vary between renders.\nA memoized instance of the callback will be returned by useCallback(callback, dependencies), and it will only change if one of the dependencies has changed. This implies that we can reuse the same function object between renders rather than constructing a new one for each new render.\nThe primary and only purpose of the useCallback() hook is to prevent needless re-renders in your code, which will speed up and improve the performance of your application.\nThe array of dependencies and a function are both passed as parameters to the useCallback() hook. The callback will only be modified by the useCallback() hook if one of the dependencies has changed. It will return a memoized version of the callback.\nuseCallback(() =\u003e { callBackFunction(); },[dependencies]); An empty dependency array can also be given. The function will only be run once as a result. If you don’t give an array, this will return a fresh value on every request.\nNow let’s understand this with an example:\nExample The primary and only purpose of the useCallback hook is to prevent needless re-renders in your code, which will speed up and improve the performance of your application.\nBecause inline functions are cheap, there is no need to recreate them for each rendering. A limited number of inline functions are allowed per component.\nHowever, there are times when you must keep a single instance of a function running in between renderings:\nan internal component wrapped inside React.memo() is capable of accepting a function object as prop.\nwhen a hook, such as useEffect(..., [callback]) , depends on another function object.\nwhen the function is throttled or debounced, or whenever it has an internal state.\nIn this case, useCallback(callbackFun, deps) hooks comes in handy because it returns the same function instance across renderings if the dependency values provided by deps are the same (aka memoization)\nimport React,{useState,memo} from 'react'; const Todos = memo(({ todos, addTodo }) =\u003e { console.log(\"Todo Renders\"); return ( \u003cdiv\u003e \u003ch2\u003eMy Todos\u003c/h2\u003e {todos.map((todo, index) =\u003e { return \u003cp key={index}\u003e{todo}\u003c/p\u003e; })} \u003cbutton onClick={addTodo}\u003eAdd Todo\u003c/button\u003e \u003c/div\u003e ); }); const App = () =\u003e { const [count, setCount] = useState(0); const [todos, setTodos] = useState([]); const inc = () =\u003e { setCount((c) =\u003e c + 1); }; const addTodo = () =\u003e { setTodos((t) =\u003e [...t, \"New Todo List\"]); }; return ( \u003cdiv\u003e \u003cTodos todos={todos} addTodo={addTodo} /\u003e \u003chr /\u003e \u003cdiv\u003e Count: {count} \u003cbutton onClick={inc}\u003e+\u003c/button\u003e \u003c/div\u003e \u003c/div\u003e ); }; export default App; Now try running this app you and click on “+” button in console you will notice that although we have memoized \u003cTodos/\u003e component whenever you click “+” button whole \u003cTodos/\u003e component get re-render weather or not todo state is changed or not.\nThis is because of referetial equality, every time a component re-renders, its fucntion get re-created because of this addTodo fucntion object changes on every render which causes re-render of \u003cTodos/\u003e componenet.\nTo solve this problem we will be using useCallback Hook:\nimport React,{useState,memo,useCallback} from 'react'; const Todos = memo(({ todos, addTodo }) =\u003e { console.log(\"Todo Renders\"); return ( \u003cdiv\u003e \u003ch2\u003eMy Todos\u003c/h2\u003e {todos.map((todo, index) =\u003e { return \u003cp key={index}\u003e{todo}\u003c/p\u003e; })} \u003cbutton onClick={addTodo}\u003eAdd Todo\u003c/button\u003e \u003c/div\u003e ); }); const App = () =\u003e { const [count, setCount] = useState(0); const [todos, setTodos] = useState([]); const inc = () =\u003e { setCount((c) =\u003e c + 1); }; const addTodo = useCallback(() =\u003e { setTodos((t) =\u003e [...t, \"New Todo List\"]); },[todos]); return ( \u003cdiv\u003e \u003cTodos todos={todos} addTodo={addTodo} /\u003e \u003chr /\u003e \u003cdiv\u003e Count: {count} \u003cbutton onClick={inc}\u003e+\u003c/button\u003e \u003c/div\u003e \u003c/div\u003e ); }; export default App; Now if you will click on “+” button you will see that \u003cTodos/\u003e compoenent will not be rendered it will only get rendered whenever our todos state changes which is exactly what we wanted.\nWhen not to use useCallback() Hook Let’s be careful not to overdo it. The main drawback of useCallback() hook is code complexity. There are several circumstances in which adding useCallback hook is unnecessary, and you must accept function recreation.\nThe performance cost of using useCallback() is that it has be called every time a component is rendered again.\nConclusion As cool as useCallback() and useMemo() are, keep in mind that they have specific use cases and should not be used to wrap every function. A dependency on another hook or a prop passed to a memoized component are good indicators that you should use useCallback if the function is computationally complex.\nWe hope this article helped you understand advanced React functionality and gained confidence in functional programming along the way!\n","description":"Learn how to optimize your React app's performance with the useCallback() hook. Discover when and how to use it, and improve your app's speed and user experience.\"","tags":["programming","react"],"title":"Boost Your React App's Performance with useCallback() Hook","uri":"/collections/programming/react/react-usecallbackhook/"},{"content":"ReactJS - useMemo hook in React In software development, we’re typically fixated on performance improvements and ways to speed up our products so that users have a better experience.\nOne strategy for improving performance is memoization. We’ll examine how React implements it in this article.\nWhat is Memoized ? In computer science memoization is a concept used in general when we don’t need to recompute value again and again. That is we some how store the current value in the form of cache and when we want some value we simply look from cache instead of recomputing the function again.\nMemoization is a process where it remebers the output for the given sets of input\nIf memoized function is called again with the same parameter, it does not re-execute the function. Rather cahce value is return from the function,reducing overhead of executing function.\nWhy do we need useMemo Hook? When an update is made throughout a component’s lifespan, React re-renders the component. Due to the way JavaScript handles equality and shallow comparisons, React may notice an unwanted or unexpected change while checking for changes in a component. The React application will needlessly re-render as a result of this update.\nuseMemo Hook The useMemo hook in react takes two parameter :-\nA function that returns a value. Dependency value. format:\nconst memoizedValue = useMemo(functionThatReturnsValue, arrayDepencies); Here the function will only run when the dependency changes\nLet’s understand useMemo Hook with an example:\nimport React, { useMemo, useState } from \"react\"; function App() { const [todo, setTodo] = useState([]); const [count, setCount] = useState(0); const someCalculation = expensiveCalculation(); function expensiveCalculation() { let num = Math.floor(Math.random() * 10000); for (let i = 0; i \u003c 100000000; ++i) num += i; return num; } return ( \u003cdiv\u003e \u003ch1\u003e Another Techs \u003c/h1\u003e \u003ch2\u003e useMemo \u003c/h2\u003e \u003cdiv\u003e \u003cbutton onClick={() =\u003e { console.log(todo); setTodo((t) =\u003e [...t, \"new Todo\"]); }} \u003e {\" \"} New Todo{\" \"} \u003c/button\u003e \u003ch3\u003e Todo List:\u003c/h3\u003e \u003cul\u003e {todo.map((item) =\u003e ( \u003cli\u003e{item}\u003c/li\u003e ))} \u003c/ul\u003e \u003cdiv\u003e \u003cbutton onClick={() =\u003e setCount((c) =\u003e ++c)}\u003e Increment Counter \u003c/button\u003e \u003ch2\u003eExpensive Calculation Value \u003c/h2\u003e \u003cp\u003e{count}\u003c/p\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e ); } export default App; In the above example we have two states count and todo. Since we know that whenever any of the state changes the whole component get’s re-renders.\nThe problem here comes if suppose we only change the value of todo, then according to the react life cycle the whole component will render.\nThis will also run over expensiveCalculation function. This causes overhead in our component. Since whenever our todo state changes it will by default run expensiveCalculation function even if the count state is changed or not.\nHere we only want expensiveCalculation function to run when the count state is changed. For this we some how want to store value some where so that we can access it without running the whole function.\nThis is where useMemo comes to rescue !!\nLet us now understand how this component will run when we use useMemo hook:\nimport React, { useMemo, useState } from \"react\"; function App() { const [todo, setTodo] = useState([]); const [count, setCount] = useState(0); const someCalculation = useMemo(() =\u003e expensiveCalculation(), [count]); function expensiveCalculation() { let num = Math.floor(Math.random() * 10000); for (let i = 0; i \u003c 100000000; ++i) num += i; return num; } return ( \u003cdiv\u003e \u003ch1\u003e Another Techs \u003c/h1\u003e \u003ch2\u003e useMemo \u003c/h2\u003e \u003cdiv\u003e \u003cbutton onClick={() =\u003e { console.log(todo); setTodo((t) =\u003e [...t, \"new Todo\"]); }} \u003e {\" \"} New Todo{\" \"} \u003c/button\u003e \u003ch3\u003e Todo List:\u003c/h3\u003e \u003cul\u003e {todo.map((item) =\u003e ( \u003cli\u003e{item}\u003c/li\u003e ))} \u003c/ul\u003e \u003cdiv\u003e \u003cbutton onClick={() =\u003e setCount((c) =\u003e ++c)}\u003e Increment Counter \u003c/button\u003e \u003ch2\u003eExpensive Calculation Value \u003c/h2\u003e \u003cp\u003e{count}\u003c/p\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e ); } export default App; In the above code we have used useMemo hook which takes a function and a dependency value as an argument.\nNow if we change the todo state this will cause the component to re-render but this time instead of running expensiveCalculation function again here useMemo will use the previous value store in cache and hence it will reduce overhead of the overall component.\nThe expensiveCalculation function will only run when the state of count changes else it will always use previous whenever the component gets re-renders\nConclusion useMemo is the hook which will memoize expensive computation. Once memoized, the hook will return the memoized value without triggering computation given that the dependency value is same.\nReference React Docs Shallow Comparison memoization Road to React React.js Essentials ","description":"In this article we will be convering useMemo Hook which is a great hook for reducing overhead of an react component","tags":["programming","react"],"title":"React JS - useMemo hook in React","uri":"/collections/programming/react/react-usememo-hook/"},{"content":"A Beginner’s Guide to React API Calls: Learn How to Make GET Requests with React If you’re a developer working with React, you know how challenging it can be to keep up with all the APIs available. React APIs provide developers with all the tools they need to create user interfaces, and there are countless options to choose from.\nTo help you get started, we’ve put together this comprehensive React API cheat sheet. In this article, we’ll cover the most important APIs in React.We’ll also explain how to use them and provide some examples.\nReact APIs API or Application Programming Interface is a way a programmer talk between two piece software. An API is used by the programmer to make the software simpler and easy. It also facilitates the programmer with an efficient and organise way to build their software.\nIn React API, are mostly used in getting data from the server. Here the data can be of any type text,image,video or xml or json file which a programmer uses in different way to build a web app.\nThe most frequent action in any modern online application is calling APIs. The majority of the time, collecting data from an API call, managing success or error cases, and other repeated tasks are required while communicating with an API.\nWe constantly have to perform those time-consuming processes when making tens of hundreds of API calls. In contrast to certain small apps, where we occasionally don’t even care, we can handle those situations effectively by adding a higher degree of abstraction over those basic API calls.\nThe issue arises when we start layering additional features on top of the already-existing ones without properly and repeatedly managing API calls. In that instance, we end up having a lot of repeating code throughout the entire application for all of those API call-related repetitions.\nConsuming REST APIs in a React Application can be done in various ways.The most prefered ways now a days is using React Hooks. In this tutorial we will discuss various ways to call an API using react.\n1. React API call using Fetch API This is the most common way to call an API. Fetch API is built into most the browser and it enable use to make HTTP GET request easily.\nimport { useEffect } from \"react\"; function App() { useEffect(() =\u003e { //Calling API fetch(\"https://jsonplaceholder.typicode.com/posts\") //Getting API response and converting it into json format .then((data) =\u003e data.json()) .then((data) =\u003e console.log(data) ) .catch((errro) =\u003e console.log(\"Error\")); }, []); return ( .... ) } In the above code the fetch return a promise. We are making asynchronous request to the particular url. When the server reponse this response is passed to callback function using then(callback). By chaining this then method we can transform the data as per we need. The return data of the then(callback) is passed as an argument to next then method. If anything goes wrong in the chainning process we catch that error using catch(callback)\n2. React API call using Async/Await Another way of working with promise is to use async/await\nBefore a function, the async keyword has two effects:\nMake a promise that is always kept. permits the usage of await in it. JavaScript waits until a promise settles when the await keyword is used before:\nIf there is a problem, an exception is raised. If not, it returns the outcome. import { useEffect } from \"react\"; function App() { async function fetchData(url) { try { const response = await fetch(url); const response_json = await response.json(); console.log(response_json); } catch (error) { console.error(error); } } useEffect(() =\u003e { fetchData(\"https://jsonplaceholder.typicode.com/posts\"); }, []); return ( .... ) } 3. React API call using Axios Your front-end application and Node.js backend can both use Axios, a Promise-based HTTP client for JavaScript. Sending asynchronous HTTP queries to REST endpoints and carrying out CRUD processes are simple when using Axios.\nIn this case, you must first use npm or yarn to install Axios before adding it as an import to your parent component.\nnpm install axios An example of utilising Axios is shown in the code snippets below:\nimport { useEffect } from \"react\"; function App() { useEffect(() =\u003e { axios .get(\"https://jsonplaceholder.typicode.com/posts\") .then((res) =\u003e res.data) .then((res) =\u003e { console.log(res); setData(res); setLoading(false); }) .catch((error) =\u003e console.log(error)); }, []); return ( .... ) } 4. Waterfall Requests Once you start writting react app there will be time were you will need to call different api request one after other That is if one api request is successful call next api and then next. This request which happen one at a time and one after another is called Waterfall Requests.\nUsing fetch() function App() { useEffect(() =\u003e { //\tFetch Here Multiple Requests Promise.all([ fetch(\"https://jsonplaceholder.typicode.com/posts\").then((res) =\u003e res.json() ), fetch(\"https://jsonplaceholder.typicode.com/users\").then((res) =\u003e res.json() ), ]) .then(([res1, res2]) =\u003e { //response of each request in an array console.log(res1); console.log(res2); }) .catch((error) =\u003e console.log(error)); }, []); return ( .... ) } Using Await/Async function App() { async function fetchData() { try { const [res1, res2] = await Promise.all([ fetch(\"https://jsonplaceholder.typicode.com/posts\").then((res) =\u003e res.json() ), fetch(\"https://jsonplaceholder.typicode.com/users\").then((res) =\u003e res.json() ), ]); console.log(res1); console.log(res2); } catch (error) { console.error(error); } } useEffect(() =\u003e { fetchData(); }, []); return ( .... ) } 5. Creating Custom Hook for fetching API call in React We have seen that API call are either successful,pending or failed. We can use this logic to create a our own custom hook for fetching api call or request in our react application. We will call this hook useFetch and use this hook to in different component in our react application.\n// fetch.js import React, { useState, useEffect } from \"react\"; export const useFetch = (url) =\u003e { const [data, setData] = useState(); const [error, setError] = useState(); const [loading, setLoading] = useState(true); useEffect(() =\u003e { if (!url) return; fetch(url) .then((data) =\u003e data.json()) .then((data) =\u003e { setData(data); console.log(data); }) .then(() =\u003e setLoading(false)) .catch(setError); }, [url]); return { loading, data, error, }; }; As you can see we have created custom hook which will fetch our data based on the url passed to it. Based on this it will return three things:\nloading: if the request its getting fetched this will be true or false. data: if the request is successful this variable will hold our data. error: if any error occured during api call this vairbale will hold error. // App.js import React from \"react\"; import { useFetch } from \"./fetchHook\"; function App() { const { loading, data, error } = useFetch( \"https://jsonplaceholder.typicode.com/posts\" ); if (loading) { return \u003ch1\u003eLoading\u003c/h1\u003e; } if (error) { return \u003cpre\u003e{JSON.stringify(error, null, 2)}\u003c/pre\u003e; } return \u003cdiv\u003e{data}\u003c/div\u003e; } export default App; Conclusion In this article, we provided a beginner’s guide to React API calls, specifically focusing on how to make GET requests with React. We explained what API calls are, why they are important, and how to make a GET request using the fetch() function. We also discussed best practices for making API calls with React and how to handle errors that may occur during the request. By following these guidelines, you can build dynamic and responsive applications that retrieve and display data in real-time.\nFAQs What is the difference between GET and POST requests? GET requests are used to retrieve data from a server, while POST requests are used to send data to a server. Can I use other HTTP methods with React, such as PUT or DELETE? Yes, React supports a variety of HTTP methods, including PUT, POST, DELETE, and more. Do I need to use a third-party library to make API calls with React? No, React has a built-in fetch() function that can be used to make API calls. However, there are third-party libraries available, such as Axios or jQuery, that can provide additional functionality. What is an API endpoint? An API endpoint is a URL that is used to access a specific resource or data from an API. How can I test my API calls in React? There are a variety of tools available for testing API calls in React, such as Postman or the Chrome DevTools network tab. ","description":"Learn how to make API calls in React using fetch, axios and other HTTP request methods. Our comprehensive guide will help you master the art of making API calls in React, from basic to advanced techniques.","tags":["programming","react"],"title":"A Beginner's Guide to React API Calls- Learn How to Make GET Requests with React","uri":"/collections/programming/react/react-api-call/"},{"content":"ReactJS - useReducer() hook in react Hooks in React introduced a new method of building and thinking about React apps. So far, one of the most popular hooks among developers is useReducer, which allows us to manage certain complex state manipulations and updates, and this is what we’ll learn about in this post.\nWe are all familiar with the two basic hooks used for state management in React. They are as follows:\nuseReducer and useState You may have heard of or used the useState hook; if not, you can read React’s official hooks documentation here.\nHave you ever struggled to select a state management library to maintain and handle a global state in a React app? What if you need to manage more complex data structures or run any side effects? Thinking about these ideas is difficult and time-consuming.\nWhat is a Reducer ? Well to understand useReducer hook first let’s try to understand what is a reducer. As the name suggest a reducer is something which reduces something to some form. Ringing some bell ??\nWell first thing that come to my mind when I hear term reducer is Javascript Array.reducer(). Which reduces the array to certain form based on the function passed to it.\nFor Example:\nlet arr = [1, 2, 3, 4, 5, 6, 7, 9]; const sum_of_array = arr.reduce((acc, num) =\u003e { return acc + num; }, 0); console.log(sum_of_array); As you can see in the code above we have created array arr and we want to get total sum of each element in the array. For this we are using reduce function which take a function as an argument. In the function we apply logic of our reducer which in our case is the sum of element. After function is passed it take an initial value which in our case is zero.\nOur reducer function get two argument:\nacc: Accumulator which hold the value of previous state, num: Each value in the array. Now that you have understand the basic function of reducer let us now under useReducer hook.\nWhat is useReducer hook in react ? useReducer hook pretty much work similar way as Javascript reducer works. useReducer hook takes two thing a reducer function and initial value similar to our Javascript reducer.\nA reducer function receives two argument current state and action (triggered by some event such as button click) and return the new State.\nSo if you want to sum the element which is in your current state you will be using following logic:\nuseReducer((state,action)={ return state+action },0) One question may arise that how does this useReducer hook get triggered?\nThe useReducer hook just like useState hook return two element. The first is current state and the second is dispatch function. This dispatch function is what triggers our useReducer hook.\nFollowing is how you will normally declare a useReducer hook:\nconst [state, dispatch] = useReducer((state, action) =\u003e { return sum + action; }, 0); Now let’s look at some real life example on how useReducer hook works and how it reduces complexity which occur if we use useState hook.\nuseReducer hook example import { useReducer, useRef } from \"react\"; function App() { const inputRef = useRef(); const [items, dispatch] = useReducer((state, action) =\u003e { switch (action.type) { case \"add\": return [...state, action.item]; case \"remove\": return state.slice(1); default: return state; } }, []); return ( \u003cdiv\u003e \u003cdiv\u003e \u003ch2\u003e Items \u003c/h2\u003e \u003c/div\u003e \u003cinput ref={inputRef} /\u003e \u003cul\u003e {items.map((item, index) =\u003e ( \u003cli key={index}\u003e \u003cspan\u003e{item}\u003c/span\u003e \u003c/li\u003e ))} \u003c/ul\u003e \u003cbutton onClick={() =\u003e { dispatch({ type: \"add\", item: inputRef.current.value }); }} \u003e {\" \"} Add{\" \"} \u003c/button\u003e \u003cbutton onClick={() =\u003e { dispatch({ type: \"remove\" }); }} \u003e {\" \"} Remove{\" \"} \u003c/button\u003e \u003c/div\u003e ); } export default App; In the above exampel we have simple todo app which add the item in the list and remove the last item form the list by clicking on Add and Remove button respectively.\nInside the App component we created our useReducer hook which gives us two thing:\nitems(crrent state) which is our list of item.(when the component mounts this list is empty)\ndispatch function\nWe have also passed the logic of what is reducer function do when it gets triggered which is simply add or remove the item from the list based on the type which get provided.\nAlso to make things simpler I have also used the useRef hook which will directly give use the value from the input field.\nNext we have declared two button Add and Remove. Whenever user clicks on the button our dispatch function get called and calls our useReducer logic.\nThe reducer returns a new array that contains all the old elements in addition to the new one at the end when it receives the “add” action.\nBut when the “Remove” actiion is triggered the reducer remove the first item fom our current state(or item) and return new state\nThe actions we do in our app are objects with a type property and some related data, but you can make them out of anything (plain strings, numbers, more complex objects, etc).\nWhen to use the useReducer hook You’ll most likely deal with more complicated state transitions when your application gets bigger, at which case useReducer will be more advantageous.\nWhen state changes grow so complex that you want to have one place to manage state, like the render function, useReducer becomes more crucial because it offers more predictable state transitions than useState.\nA reasonable rule of thumb is that useReducer is usually preferable whenever you need to handle a complicated object, such as with arrays and additional primitives, as opposed to managing primitive data, such as a text, integer, or Boolean.\nIf you learn better visually, the video below provides a thorough explanation and real-world examples of when to utilise the usage.\nConclusion The useReducer hook is a fantastic addition to the React framework that makes updating our component’s state simpler, predictable, and organised and facilitates data sharing between components.\nBecause you can now easily transmit dispatch down instead of conventional callbacks, it enables you to enhance the efficiency of the components that initiate deep updates.\nAnd even if you only remember one thing from this post, it should be that useReducer gives us the ability to specify how we update our state value.\n","description":"In article we will dive into what is useReducer hook and how to use useReducer hook to write complex logic of the react component","tags":["programming","react"],"title":"React JS - useReducer hook in react","uri":"/collections/programming/react/react-usereducer-hook/"},{"content":"ReatJS - What is useLayoutEffect hook in react React is a well-known and rapidly developing JavaScript library in the web development industry. React is now an excellent alternative for developing interactive, modern real-world applications due to its ease of use and data-fetching capabilities. Hooks, a new feature in React V16.7, gives additional benefits such as providing hot reloading and leveraging functional components with ease.\nHooks are functions that allow you to use state and many other React features without having to write ES6 class components. The useLayoutEffect Hook functions similarly to the useEffect Hook. In this tutorial, we’ll go over the hook API reference using the useLayoutEffect example.\nLet’s get started!\nWhat is useLayoutEffect hook in react ? The useLayoutEffect hook functions similarly to the useEffect hook, except instead of functioning asynchronously like the useEffect hook, it fires synchronously when all DOM loading is complete. This is important for synchronously re-rendering the DOM as well as reading the DOM’s layout. To avoid preventing the page from loading, we should always use the useEffect hook.\nIn terms of scheduling, this operates similarly to componentDidMount and componentDidUpdate. Your function is executed immediately after the DOM has been modified, but before the browser has had a chance to “paint” those changes (the user does not see the updates until the browser has been repainted).\nSyntax:\nimport { useLayoutEffect } from \"react\"; function App() { useLayoutEffect(()=\u003e{ //Do something return ()=\u003e{ //Do some cleanup here } },[dependencies]) return ( \u003cdiv\u003e \u003c!--- HTML HERE --\u003e \u003c/div\u003e ); } export default App; The useLayoutEffect hook in React takes two arguments. The first argument is an effect function, while the second argument is an array of dependents. In most cases, the first argument, effect, is either undefined or returns a cleanup function.\nAs demonstrated in the above function signature, both useEffect and useLayoutEffect accept an effect function and an array of dependencies as arguments and return either an unknown or a cleanup function.\nWhat is difference between useEffect hook and useLayoutEffect hook in react ? The time when the routines are invoked differs between useEffect and useLayoutEffect. It’s useful to know that component re-rendering goes through the following steps to understand when the hooks are called. Assume we’re using the useEffect hook in our app.\nThe user interacts with the app. Assume the user presses a button.\nChanges in component state\nThe DOM has been altered.\nOn the screen, changes are painted.\nIf useEffect dependencies have changed, the cleanup method is used to clean up effects from earlier renders.\nAfter cleanup, the useEffect hook is called.\nNote: It should be noted that the cleanup function is not executed when a component is rendered for the first time because there is no effect to clean up.\nThe useEffect hook and useLayoutEffect hook differ in the order in which they are invoked. After the DOM has been painted, the useEffect hook is called. In contrast, the useLayoutEffect hook is called synchronously before any modifications are made to the screen. The methods stated above for useEffect implementation can be adjusted as indicated below for useLayoutEffect.\nThe user interacts with the app. Assume the user presses a button.\nChanges in component state\nThe DOM has been altered.\nIf the useLayoutEffect dependencies have changed, the cleanup method is called to clean up the effects from the previous render.\nAfter cleanup, the useLayoutEffect hook is called.\nOn the screen, changes are painted.\nThe above explanation suggests that most of the time you don’t need to useLayoutEffect.\nWhen to use useLayoutEffect hook in react app When is it appropriate to useLayoutEffect instead? You’ll recognise it when you see it. figuratively ;)\nIf your component flickers when its state is altered - for example, if it renders in a partially-ready state first and then instantly re-renders in its final state - that’s a solid indication that it’s time to replace useLayoutEffect.\nThis is true if your upgrade is a two-step (or multi-step) process. Do you wish to “batch” several updates before redrawing the screen? useLayoutEffect instead.\nI think of useLayoutEffect as a method to get a little additional work done before React updates the DOM. “Hey, you’re already making some modifications; could you please include this one as well?” Awesome.\nOne Case where you can use useLayoutEffect in react One case you might use useLayoutEffect instead of useEffect is if you are update a value like ref and you want to make sure it’s up to date before running any other code.\nAs an example:\nimport { useLayoutEffect, useRef } from \"react\"; import style from \"./App.module.css\"; function App() { const inputRef = useRef(); const inputGroupRef = useRef(); useLayoutEffect(() =\u003e { // This will load old style first because it render first const { current } = inputRef; const handleFocus = () =\u003e inputGroupRef.current.classList.add(style.active); current.addEventListener(\"focus\", handleFocus); return () =\u003e { current.removeEventListener(\"focus\", handleFocus); }; }); return ( \u003cdiv className={style.container}\u003e \u003cdiv ref={inputGroupRef} className={style.inputGroup}\u003e \u003clabel className={style.lable}\u003e Type Someting \u003c/label\u003e \u003cinput ref={inputRef} className={style.input} type=\"text\" /\u003e \u003c/div\u003e \u003c/div\u003e ); } export default App; Advanced Techniques Here are some advanced techniques that you can use with useLayoutEffect:\n1. Refs You can use the useRef hook to create a reference to a DOM node and use it in the useLayoutEffect hook to perform side effects on that node.\nimport { useRef, useLayoutEffect } from 'react'; function MyComponent() { const nodeRef = useRef(null); useLayoutEffect(() =\u003e { // Perform side effects on nodeRef.current }, [dependencies]); return \u003cdiv ref={nodeRef}\u003eHello World\u003c/div\u003e; } 2. Server-Side Rendering If you are using server-side rendering, you can use the useLayoutEffect hook on the client-side to ensure that the layout is updated after the initial render.\nimport { useLayoutEffect } from 'react'; function MyComponent() { useLayoutEffect(() =\u003e { // Perform side effects here }, [dependencies]); // Render logic here } 3. Animation You can use the useLayoutEffect hook to perform animations that depend on the layout of your application.\nimport { useState, useLayoutEffect } from 'react'; function MyComponent() { const [isVisible, setIsVisible] = useState(false); useLayoutEffect(() =\u003e { setIsVisible(true); }, [dependencies]); return ( \u003cdiv style={{ opacity: isVisible ? 1 : 0 }}\u003e Hello World \u003c/div\u003e ); } Conclusion In this comprehensive guide, we have covered everything you need to know about the React useLayoutEffect hook. We started with the basics and gradually moved on to advanced implementation techniques. By using useLayoutEffect, you can ensure that your application’s layout is updated before the user sees any changes on the screen, resulting in a better user experience.\nIn summary, the useLayoutEffect hook is an essential tool for managing your application’s layout in React. By optimizing your page for the keyword “React useLayoutEffect,” you can attract a significant amount of potential traffic with a relatively low difficulty score.\n","description":"Learn how to use React useLayoutEffect hook effectively to manage your application layout. Our comprehensive guide covers everything from basic syntax to advanced implementation techniques.","tags":["programming","react"],"title":"React useLayoutEffect- A Comprehensive Guide","uri":"/collections/programming/react/react-uselayouteffect/"},{"content":"ReactJS - Creating Custom hooks in React With the help of custom React hooks, you may provide your React applications additional, distinctive functionality.\nInstalling a third-party library designed to address your issue is frequently the simplest way to add a certain functionality to your application. What do you do, though, if such a library or hook doesn’t exist?\nLearning how to create custom hooks is crucial if you want to fix issues or add features that are lacking in your own React projects.\nIn this step-by-step tutorial, I’ll walk you through the process of making your own custom React hooks and explaining the issues they were designed to address.\nWhat is react hooks? To the delight of the developer community, the React team just unveiled hooks. What is the big deal? By enabling us to incorporate elements that are only available to class components, such stateful logic, hooks actually opens up a completely new method to design functional components.\nYou can basically accomplish this using State and Effect hooks in React. You can define a state object and a function that updates it using the State(useState) hook. You can execute side effects in a functional component using the Effect(useEffect) hook. Consider it to be similar to class component lifecycle events.\nA function that begins with the word “use” and may invoke other hooks is referred to as a custom hook. The “useWhatever” naming style is primarily used to enable the linter to detect errors in the usage of certain hooks, such as instances when usage violates the hooks requirements.\nNeed of react custom hooks In order to keep the DRY (Don’t Repeat Yourself) principle in your React projects, you should use custom hooks. Consider the situation when you need to employ some logic that uses some built-in hooks in various functional components. You may either write the same logic in each component individually (which goes against the DRY principle), or you can construct a new function, encapsulate the logic inside of it, and call it from the other components. Since you only need to write the reasoning once, the second alternative is unquestionably the preferable option. The unique hook in this case is the independent function you defined.\nIn order to apply your logic in different functional components (hooks are ineffective in class components), simply construct a separate custom hook and use it.\nRules for creating react custom hooks Only use hooks at the highest level. Never call hooks while inside of a loop, a condition, or nested procedures. hook calls must only come from React function components. Regular JavaScript functions shouldn’t call hooks. (Your own custom hooks are the only other location where calling hooks is appropriate. They’ll be revealed to us shortly.) If you’re wondering why these rules exist, it’s because React uses the order that hooks are invoked to associate each hook with a certain local state. Placing a hook inside of a condition can alter this order, preventing the subsequent hooks from being called and, most likely, leading to bugs.\nExample on creating react custom hooks Now let us see how we can create and use custom hooks in our react application. For our example we are going to create custom hooks name useCopytoClipboard().\nSince the hooks return two thing our custom hook will also give us two values :\nisCopied: this state will tell us weather the content is copied or not. handleCopy: this function will be use to set text to our system clipboard. Also to make useCopytoClipboard hook little bit real. I have used useEffect hook which will clear our system clipboard after some amount of time which we decide\n//useCopytoClipboard.js import { useEffect, useState } from \"react\"; import copy from \"copy-to-clipboard\"; export default function useCopyToClipboard(resetInterval = null) { const [isCopied, setCopied] = useState(false); const handleCopy = (text) =\u003e { if (typeof text == \"string\" || typeof text == \"number\") { copy(text.toString()); setCopied(true); } else { console.error(\"Error\\n Cannot Copy\"); } }; useEffect(() =\u003e { let timeout; if (isCopied \u0026\u0026 resetInterval) { timeout = setTimeout(() =\u003e { copy(\" \"); setCopied(false); }, resetInterval); } return () =\u003e { clearTimeout(timeout); }; }, [isCopied, resetInterval]); return [isCopied, handleCopy]; } We must first check that the function only accepts data of the types string or numeric. To ensure that the type is either a string or an integer, we will create an if-else statement. If not, we will record a console error informing the user that they are unable to copy any further types.\nNext, we turn the text into a string so that it can be passed to the copy method. The handleCopy method from the hook is then returned for use wherever in the application we see fit.\nThe handleCopy function will typically be attached to a button’s onClick event.\nWe also need a state that indicates whether or not the text was copied. useState will be called at the beginning of our hook to construct that, and we’ll make a new state variable called isCopied with the setter setCopy.\nThis value will initially be false. If the copying of the text is successful, copy will be set to true. Otherwise, we’ll make it false.\nFinally, we will return handleCopy and isCopied from the hook in an array.\nNow, we can use useCopyToClipboard inside of any component.\nIn my situation, I’ll utilise it in conjunction with a copy button that already has the code for our code sample.\nAll we have to do to make this work is to give the button a on click. Additionally, a function called handlecopy returns the text form of the code that was sent to it.\n// App.js import { useState } from \"react\"; import useCopyToClipboard from \"./utils/copyToClipboard\"; function App() { const [isCopied, handleCopy] = useCopyToClipboard(3000); const [value, setValue] = useState(\"Type Anything...\"); return ( \u003cdiv\u003e \u003cinput value={value} onChange={(e) =\u003e { setValue(e.target.value); }} /\u003e \u003cbutton onClick={() =\u003e handleCopy(value)}\u003eCopy to ClipBoard\u003c/button\u003e {isCopied ? \u003cdiv\u003e Text Copied \u003c/div\u003e : \u003cdiv\u003e Nothing to Clipboard \u003c/div\u003e} \u003c/div\u003e ); } export default App; Why should you use react custom hooks ? Consider a scenario where you have to use useEffect and useState during development.\nAfter some time, you become aware that another component also requires the useEffect and useState logic. You can copy code, but you probably think there must be a better approach. What will you do then? Personalized hooks to the rescue.\nReusability: We don’t have to write the same hook twice because we can use it repeatedly.\nClean Code: A cleaner codebase can be achieved by isolating all component logic into a hook.\nEasy maintenance — maintainability. The logic of the hook only needs to be altered once, if at all.\nGreat Community – there is a significant probability that the hook you had in mind has already been developed. There are a tonne of Custom hooks on the web! You can locate a hook that fits your needs, utilise it as is, or even better, use it as a springboard to create something great!\nConclusion We discussed custom hooks, clarified the advantages of utilising custom hooks, and provided examples from actual applications along with an explanation of when we utilised particular hooks.\nWe’ve shown how easy it is to create custom hooks and how many (open) resources are available for finding inspiration and using custom hooks that already exist (I attached more sources below). I sincerely hope you enjoyed and gained knowledge from the post. Regards from the reader.\nIt’s now your turn to design a unique hook of your own!\n","description":"We provide you with simple-to-understand React custom hook code recipes so you can understand how they function and get more at ease creating your own.","tags":["programming","react"],"title":"React JS - Creating custom hooks in React","uri":"/collections/programming/react/react-creating-custom-hooks/"},{"content":"ReactJS - A complete guide to refs and useRef() in React We will talk about how to directly manipulate DOM elements in React in this tutorial.\nAlthough the React Framework constructs your components and abstracts your code from DOM manipulation, developers can still access it. There are only a few situations where it might be required. React offers a refs escape route as a result.\nrefs is a function used by components to access the DOM. To enable access to the element from wherever within your component without using properties and everything, you merely need to attach a ref to it in your application.\nrefs can be used to gain direct access to React elements and to interact with them via callbacks. refs should only be used when state and props are unable to deliver the desired interaction.\nReact refs In React, references are denoted by the abbreviation refs. It is comparable to React’s keys. It is an attribute that enables the storage of a reference to certain React elements or DOM nodes. It gives instructions on how to interact with React elements and DOM nodes. It is used when we don’t want to use props but still want to update the value of a child component.\nSimilar to many other UI technologies, React provides a means to reconsider a view as the outcome of a component’s state. This is a significant departure from how we typically create applications.\nWe find out how simple frontend problems that used to be challenging to us can be solved if we become familiar with some of these new ideas. This gain is partially attained by building views using the abstraction techniques that React and JSX expose rather than the DOM standard methods.\nThese escape routes are called refs, and they give us direct access to DOM properties. React often re-renders the component for us in order to refresh the data on the screen using state. However, there are some circumstances where dealing with the DOM properties directly is necessary, and that’s where refs come in handy.\nThis might be demonstrated by automatically focusing a text box as a component renders. We can use refs to access the DOM directly and focus the text box for us whenever the component renders on the screen because React doesn’t offer a simple mechanism to do this.\nWe’ll look into why React, a framework designed to keep your code away from DOM manipulation, leaves the door open for developers to access it in this post.\nCreating React refs Since React recommends functional components, and general practice is to follow the Hooks way of doing things we will be using useRef(null) to create refs intead of createRef() which were used in class-based component in the past.\nLet’s start with the simple example of grabing a node element using refs.\nimport React, { useRef } from \"react\"; const App = () =\u003e { const buttonRef = useRef(null); return( \u003cdiv\u003e \u003cbutton onClick={()=\u003e console.log(\"Hello\")} ref={buttonRef}\u003e Click Here \u003c/button\u003e \u003cdiv\u003e ) } export default App; Here, the \u003cbutton\u003e statement is actually how JSX calls React.createElement(\"button\") does not truly represent an HTML button element; rather, it represents a React element.\nBy generating a React reference and sending it to the element itself, you may access the real HTML element.By using buttonRef.current, we may access the actual HTML element at any point in the component’s lifecycle.\nAt this point, we are aware of how to access DOM nodes within a React component. Let’s look at some of the circumstances in which this can be helpful.\nUsing React refs By invoking focus() on the node instance, you can programmatically gain focus in an element. The simplest approach to do this with React is to build a ref and manually perform it when we think it’s appropriate because the DOM exposes this as a function call.\nimport { React, useRef, useEffect } from \"react\"; const App = () =\u003e { const inputRef = useRef(null); useEffect(() =\u003e { console.log(inputRef.current); //Logs \"HTMLInputElement\" inputRef.current.focus(); }, []); console.log(inputRef.current); //Logs undefined return ( \u003cdiv\u003e \u003cinput ref={inputRef} type=\"text\" /\u003e \u003c/div\u003e ); }; During initial rendering, React still determines what the component’s output is, therefore no DOM structure is produced. As a result, during initial rendering, inputRef.current evaluates to undefined.\nWhen the input element has already been established in DOM, the useEffect(callback, []) hook performs the callback immediately after mounting. Because the DOM is guaranteed to be built, the callback function of the useEffect(callback, []) is the correct place to access inputRef.current.\nWhen to Use React’s Refs Refs are applicable in the following situations:\nwhen handling focus, text selection, or media playing requires DOM measurements.\nwhile incorporating DOM libraries from outside sources.\nIn order to start imperative animations, it is used.\nIn callbacks, it can also be used.\nWhen not to use React’s Refs For anything that can be done declaratively, it should not be used. For instance, you should supply an isOpen prop to a Dialog component rather than calling the open() and close() methods on it.\nYou must refrain from using the Refs excessively.\n","description":"Learn how to use React refs, and why it's important to use them only when React can't handle a function call through its own methods.","tags":["programming","react"],"title":"React JS - A complete guide to React refs","uri":"/collections/programming/react/react-createref-hook/"},{"content":"React useContext Hook: Simplifying State Management in React In a typical React application, data is passed top-down (from parent to child) by props, however this can be challenging for some prop types (such locale preference and UI theme), which are needed by numerous components that are nested at different levels within an application.\nWithout having to manually provide props down via each nested component, context offers a means to pass data or state through the component tree. For a tree of React components, it is intended to share information that can be regarded as global information, such as the currently authenticated user or theme (e.g. color, paddings, margins, font-sizes).\nContext is used by Context API. Context and Provider Consumer Components transmit the data, however writing the lengthy functional code to leverage this Context API is quite time-consuming. Therefore, using the useContext hook eliminates the requirement to use the Consumer Component and makes the code easier to read and less verbose. React 16.8 has a new hook called useContext.\nProblem with useState hook in ReactJS The topmost parent component in the stack that needs access to the state should hold it.\nWe have numerous nested components as an example. The stack’s top and bottom components both require access to the state.\nWithout Context, we must transmit the state as “props” via each nested component in order to accomplish this. Prop drilling is the term for this.\nExample:\nimport { useState } from \"react\"; const ParentComponent = () =\u003e { const [data, setData] = useState(\"Another Techs\"); const [user, setUser] = useState(\"Alex\"); return ( \u003cdiv\u003e \u003ch1\u003e Hello, {user} \u003c/h1\u003e \u003cChildComponent1 data={data} /\u003e \u003c/div\u003e ); }; const ChildComponent2 = ({data}) =\u003e { return ( \u003cdiv\u003e \u003cChildComponent3 data={data}\u003e \u003c/div\u003e ) } const ChildComponent3 = ({data}) =\u003e { return ( \u003cdiv\u003e \u003cChildComponent4 data={data}\u003e \u003c/div\u003e ) } const ChildComponent4 = ({data}) =\u003e { return ( \u003cdiv\u003e \u003cChildComponent5 data={data}\u003e \u003c/div\u003e ) } const ChildComponent5 = ({data}) =\u003e { return ( \u003cdiv\u003e \u003ch2\u003e Welcome to {data} !! \u003c/h2\u003e \u003c/div\u003e ) } Despite the fact that components 2-4 did not require the state, they had to send it on so that component 5 could receive it.\nReact useContext to the rescue The context, the provider extrapolated from the context, and the consumer are the three players required to apply the React context.\nSyntax:\nconst contextName = useContext(initialValue); The value that React provides is accepted by useContext. When its value changes, you must first createContext and then re-render the component, although memorising can still improve performance.\nThe example application would seem as follows when the context is applied to it:\nimport { useState,createContext,useContext } from \"react\"; const DataContext = createContext(); const ParentComponent = () =\u003e { const [data, setData] = useState(\"Another Techs\"); const [user, setUser] = useState(\"Alex\"); return ( \u003cdiv\u003e \u003cUserContext.Provider value={data}\u003e \u003ch1\u003e Hello, {user} \u003c/h1\u003e \u003cChildComponent1 /\u003e \u003c/UserContext.Provider\u003e \u003c/div\u003e ); }; const ChildComponent2 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent3 \u003e \u003c/div\u003e ) } const ChildComponent3 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent4 \u003e \u003c/div\u003e ) } const ChildComponent4 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent5 \u003e \u003c/div\u003e ) } const ChildComponent5 = () =\u003e { const data = useContext(DataContext); return ( \u003cdiv\u003e \u003ch2\u003e Welcome to {data} !! \u003c/h2\u003e \u003c/div\u003e ) } Let’s look into more detail what has been done.\nFirst, const DataContext = createContext() creates the context that’s going to hold the data.\nSecond, inside the \u003cParentComponent /\u003e component, the application’s child components are wrapped inside the user context provider: \u003cUserContext.Provider value={data}\u003e.\nNote that the value prop of the provider component is important: this is how you set the value of the context.\nFinally, \u003cChildComponent5 /\u003e becomes the consumer of the context by using the built-in useContext(DataContext) hook. The hook is called with the context as an argument and returns the user name value.\n\u003cChildComponent2/\u003e to \u003cChildComponent4/\u003e components don’t have to pass down the data prop. That is the great benefit of the context: it removes the burden of passing down data through the intermediate components.\nWhat happens if context value is changed ? All of the context provider’s consumers are alerted and re-rendered when the context value is updated by changing the context provider’s value prop (\u003cContext.Provider value=value /\u003e).\nFor example, if I change the user data from ‘Another Techs’ to ‘ReactJS Context Hook’, then consumer immediately re-renders to display the latest context value:\nimport { useState,useEffect,createContext,useContext } from \"react\"; const DataContext = createContext(); const ParentComponent = () =\u003e { const [data, setData] = useState(\"Another Techs\"); const [user, setUser] = useState(\"Alex\"); useEffect(()=\u003e{ setTimeout(()=\u003e{ setData(\"ReactJS Context Hook\"); },2000); },[]); return ( \u003cdiv\u003e \u003cUserContext.Provider value={data}\u003e \u003ch1\u003e Hello, {user} \u003c/h1\u003e \u003cChildComponent1 /\u003e \u003c/UserContext.Provider\u003e \u003c/div\u003e ); }; const ChildComponent2 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent3 \u003e \u003c/div\u003e ) } const ChildComponent3 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent4 \u003e \u003c/div\u003e ) } const ChildComponent4 = () =\u003e { return ( \u003cdiv\u003e \u003cChildComponent5 \u003e \u003c/div\u003e ) } const ChildComponent5 = () =\u003e { const data = useContext(DataContext); return ( \u003cdiv\u003e \u003ch2\u003e Welcome to {data} !! \u003c/h2\u003e \u003c/div\u003e ) } On the screen, “Another Techs” (context value) would be visible. The context value switches to “ReactJS Context Hook” after two seconds, and the screen updates to reflect the new value.\nReal world example where useContext Hook might come handy Suppose you have a React app that displays a list of products. You want to be able to add and remove products from the list, and you also want to be able to display the total number of products in the list. One way to do this is to use the useContext hook to manage the state of the products list and the total number of products.\nFirst, you need to create a context object that will hold the state of the products list and the total number of products. Here’s how you can do it:\nimport React, { createContext, useContext, useState } from 'react'; const ProductContext = createContext(); export const useProduct = () =\u003e useContext(ProductContext); export const ProductProvider = ({ children }) =\u003e { const [products, setProducts] = useState([]); const [totalProducts, setTotalProducts] = useState(0); const addProduct = (product) =\u003e { setProducts([...products, product]); setTotalProducts(totalProducts + 1); }; const removeProduct = (product) =\u003e { const newProducts = products.filter((p) =\u003e p.id !== product.id); setProducts(newProducts); setTotalProducts(totalProducts - 1); }; return ( \u003cProductContext.Provider value={{ products, totalProducts, addProduct, removeProduct }}\u003e {children} \u003c/ProductContext.Provider\u003e ); }; In the above code, we create a ProductContext object using the createContext method. We also define a custom hook called useProduct that will be used to retrieve the state values from the context. Inside the ProductProvider component, we define the state values using the useState hook, and we define two functions called addProduct and removeProduct to add and remove products from the list. Finally, we pass the state values and functions to the context using the Provider component.\nNow, in any component that needs to access the products list or the total number of products, you can use the useProduct hook to retrieve the values from the context. Here’s an example:\nimport React from 'react'; import { useProduct } from './ProductContext'; const ProductList = () =\u003e { const { products, totalProducts, removeProduct } = useProduct(); return ( \u003c\u003e \u003ch2\u003eProducts ({totalProducts})\u003c/h2\u003e \u003cul\u003e {products.map((product) =\u003e ( \u003cli key={product.id}\u003e {product.name} - {product.price} \u003cbutton onClick={() =\u003e removeProduct(product)}\u003eRemove\u003c/button\u003e \u003c/li\u003e ))} \u003c/ul\u003e \u003c/\u003e ); }; export default ProductList; In the above code, we import the useProduct hook from the ProductContext file. We then use the useProduct hook to retrieve the products list, total number of products, and the removeProduct function from the context. We render the products list as a list of items, and we use the removeProduct function to remove a product from the list when the user clicks the “Remove” button.\nUsing the useContext hook in this way can make your code cleaner and easier to maintain. By centralizing the state management in the context, you can avoid having to pass props down the component tree, which can make your code more efficient as well.\nWhen to use useContext hook ? Utilizing the context is primarily intended to give your components access to certain global data and to re-render when that global data is modified. When you have to pass down props from parents to kids, context solves the props drilling problem.\nHolding within the context:\nglobal state a theme an application’s settings an authenticated user name a user’s settings their preferred language a group of services. On the other hand, you ought to give the decision to use context in your application significant thought.\nIntegrating the context hook firstly increases complexity. Complexity is increased by creating the context, encapsulating everything in the provider, and using useContext() in each consumer.\nSecond, introducing context hook makes unit testing the components more challenging. You would need to encase the consumer components in a context provider for unit testing. Including the elements that the context indirectly influences — the forerunners of context consumers!\nConclusion No matter where in the components tree a child component is located, React’s concept of useContext hook enables you to supply it with global data. The context must be created, provided, and consumed before it can be used.\nRemember that the useContext hook adds a significant degree of complexity when incorporating it into your application. It’s not always a big deal to drill the props through two or three layers of the hierarchy.\nReferences Context useContext Hooks React Docs Medium React Hooks ","description":"Learn how to use the React useContext hook for simplified state management in your React applications. Our tutorial covers everything you need to know to get started.","tags":["programming","react"],"title":"React useContext Hook- Simplifying State Management in React","uri":"/collections/programming/react/react-usecontext-hook/"},{"content":"ReactJS - Understanding Higher-Order Components in React **A function that takes a component and returns a new component is known as a higher-order component.**React’s higher-order components (HOCs) were inspired by JavaScript’s higher-order functions. A HOC is a sophisticated approach to reuse logic in React components. It’s a pattern derived from the compositional nature of React. Another function is passed as an argument to a higher-order component function. The greatest example to understand this is the map function. The basic purpose is to break down the component logic into smaller, more manageable functions that may be reused as needed.\nThese functions are pure in the sense that they accept data and return values based on that input. Higher order functions are re-run with different data input if the data changes. We don’t need to edit the HOC if we wish to update our returning component. All we have to do now is update the data our function is working with.\nIn React.js, a higher-order component (HOC) is an advanced mechanism for reusing component logic. The React API does not include Higher-Order Components. They’re the pattern that React’s compositional nature produces. A higher-order component translates a component into another component, while the component converts props into UI.\nReason for using Higher Order Component:\nSimple to use\nGet rid of the need to copy the same logic across all components.\nIt improves the readability of the code.\nHigher-Order Functions in JavaScript Let’s take a quick look at higher-order functions in JavaScript before diving into HOCs in React. Understanding them is crucial to comprehending our main topic.\nIn JavaScript, higher-order functions take one or more functions as arguments and return another function. They give us the ability to abstract over actions rather than just values. They come in a variety of shapes and sizes, and they assist us in writing less code when working with functions and even arrays.\nComposition is the most intriguing aspect of employing higher-order functions. Small functions that handle a single piece of logic can be written. Then, by combining the various tiny functions we’ve constructed, we may create large ones. This decreases the number of defects in our code and makes it much easier to read and understand.\nSome of these functions are already included in JavaScript. The following are some instances of higher-order functions:\nforEach(): This code iterates over every element in an array, but it does not update or mutate the array, and it returns undefined.\nfilter(): This function examines each element in an array to see if it matches the criteria set in the filter method, and then returns a new array with the elements that do.\nmap(): This method modifies an array by applying a function to all of its elements and then constructing a new array from the values returned.\nreduce(): For each value of the array, this method runs the specified function (from left to right).\nAn Example of Higher-Order Functions function add(x, y) { return x + y; } function sub(x, y) { return x - y; } //Define a function that take function as an argument function operation(x, y, op) { return op(x, y); } operation(5, 3, add); // 8 operation(5, 3, sub); // 2 In the above example, we have to define a higher-order component operation which takes values and operation to be performed on that value as an argument which in our case are two add and sub.\nHigher Order Component (HOC) in React A higher-order component (HOC) is a React component that allows you to reuse logic. Components accept one or more parameters and return a new, updated component. Is this anything you’ve heard before? They are analogous to higher-order functions, which accept as an argument several functions and return a new function.\nHOCs are often used to create components with shared behavior in a way that connects them in a way that differs from the standard state-to-props paradigm.\nThe React API does not include Higher-Order Components. They’re the pattern that React’s compositional nature produces. A higher-order component translates a component into another component, while the component converts props into UI. Redux’s connect and Relay’s createContainer are two examples of HOCs.\nStructure Of Higher Order Component (HOC) The structure of a HOC is similar to that of a higher-order function:\nIt’s a Component\nAs an argument, it uses another component.\nThen a new component is returned. It can render the original component that was supplied to it with the component it returns.\nimport React from \"react\"; const withHOC = (WrappedComponent) =\u003e { return \u003cdiv\u003e{\u003cWrappedComponent /\u003e}\u003c/div\u003e; }; Note: the name of the higher-order component (HOC) should always start with the prefix “with”.\nExample The simplest example to learn how higher-order component works are showing a loader while the component is waiting for data.\nWhen creating a web application, we almost always need to employ a loader of some kind that is displayed while a component waits for data to be supplied to its props. We could render the loader with an in-component solution, which would work, but it wouldn’t be the most elegant option. Better would be to write a common HOC that can track those props; and while those props haven’t been injected or are in an empty state, it can show a loading state.\nFirst, let’s see how we will normally write this function without HOC\nconst ListData = ({data}) =\u003e { return( \u003cul\u003e { data.map((item) =\u003e ( \u003cli\u003e \u003ch1\u003e item \u003c/h1\u003e \u003c/li\u003e )) } \u003c/ul\u003e ) } const App = () =\u003e { const useState{data,setData} = useState(null); const useState{loading,setLoading} = useState(true); useEffect = (() =\u003e { fetch(\"https://example.com/dummydata\") .then((json) =\u003e json.json()) .then((repos) =\u003e { setData(repos) setLoading(false); ) }) if(loading) return \u003cdiv\u003e Loading Data Please Wait \u003c/div\u003e; if(!data) return \u003cdiv\u003e No data \u003c/div\u003e; if(!data.length) return \u003cdiv\u003e Data is Empty \u003c/div\u003e return \u003cListData data={data} /\u003e } export default App; In the above example, we first fetch the data from a given URL and this display the data in the form of a list. But Before displaying the data we first check:\nweather the data fetched or not? weather the website returned any data or not? and if data is returned is it empty or not?. If all of the above condition is satisfied then only we display the ListData component.\nNow suppose instead of listing data we want to display images. The process here will be the same, we will check:\nweather the data fetched or not? weather the website returned any data or not? and if data is returned is it empty or not?. After all these conditions get satisfied we display component ListImages\nconst ListImages = ({data}) =\u003e { return( \u003cul\u003e { data.map((url) =\u003e ( \u003cli\u003e \u003cimg src={url} /\u003e \u003c/li\u003e )) } \u003c/ul\u003e ) } const App = () =\u003e { const useState{data,setData} = useState(null); const useState{loading,setLoading} = useState(true); useEffect = (() =\u003e { fetch(\"https://example.com/dummyimages\") .then((json) =\u003e json.json()) .then((repos) =\u003e { setData(repos) setLoading(false); ) }) if(loading) return \u003cdiv\u003e Loading Data Please Wait \u003c/div\u003e; if(!data) return \u003cdiv\u003e No data \u003c/div\u003e; if(!data.length) return \u003cdiv\u003e Data is Empty \u003c/div\u003e return \u003cListImages data={data} /\u003e } export default App; Now if you look closely at both the code snippets above you will notice that in both the cases the validation part is common and the only thing which changes is the displaying component. Therefore to avoid this duplication of code and make the code more readable we can use High Order React Component (HOC).\nNow let’s see how the Higher-Order React Component (HOC) will help use avoid duplication of code:\n//Higher Order Component const withLoading = (Component) =\u003e ({url}) =\u003e { const useState{data,setData} = useState(null); const useState{loading,setLoading} = useState(true); //Fetching url useEffect = (() =\u003e { fetch(url) .then((json) =\u003e json.json()) .then((repos) =\u003e { setData(repos) setLoading(false); ) }) } //Checking the required Component if(loading) return \u003cdiv\u003e Loading Data Please Wait \u003c/div\u003e; if(!data) return \u003cdiv\u003e No data \u003c/div\u003e; if(!data.length) return \u003cdiv\u003e Data is Empty \u003c/div\u003e //displaying data return \u003cComponent {data}/\u003e } const ListImages = ({data}) =\u003e { return( \u003cul\u003e { data.map((url) =\u003e ( \u003cli\u003e \u003cimg src={url} /\u003e \u003c/li\u003e )) } \u003c/ul\u003e ) } const ListData = ({data}) =\u003e { return( \u003cul\u003e { data.map((item) =\u003e ( \u003cli\u003e \u003ch1\u003e item \u003c/h1\u003e \u003c/li\u003e )) } \u003c/ul\u003e ) } const DisplayData = withLoading(ListData); const DisplayImages = withLoading(ListImages); const App = () =\u003e { return ( \u003cdiv\u003e \u003cDisplayData url=\"https://examples.com/dummydata\"\u003e \u003cDisplayImages url=\"https://examples.com/dummyimages\"\u003e \u003c/div\u003e ) } Now if you look at the code we have created a Higher Order Component withLoading which takes another component as an argument. The job of this component is to fetch the URL and check our validation condition. If all goes well it just displays the component which is passed to it as an argument. It does not care how the component is rendering the data which is exactly what we wanted.\nThen we have created two-component ListData and ListImages which we display the data and images respectively. We then pass this component to withLoading component. Now if we call DisplayData or DisplayImages component both will first fetch the URL, then check our validation condition, and then it will render the component.\nYou can see how we as developers were able to adhere to the principles of functional programming here by employing the composition of functions as well as an additional wrapper function for the configuration. This composition would still work if one of the higher-order components refused to accept a configuration (just by not calling it like the other ones that take a configuration).\n","description":"In this course, we will learn about higher-order components, including their syntax and applications. We will create a higher-order component from an existing React component in the process. You will learn the fundamentals of higher-order components and how to construct them at the end of this lesson.","tags":["programming","react"],"title":"ReactJS - Understanding Higher Order Components in React","uri":"/collections/programming/react/react-higher-order-components/"},{"content":"Mastering React Conditional Rendering: A Comprehensive Guide Conditional rendering is a crucial aspect of React that allows developers to render different components based on certain conditions. This feature helps to create dynamic and responsive user interfaces, making React a popular choice for building complex web applications.\nIn this comprehensive guide, we’ll cover everything you need to know about conditional rendering in React. We’ll start with the basics and gradually move towards advanced techniques, helping you master React’s conditional rendering functionality.\nThe Basics of Conditional Rendering in React Conditional rendering in React is achieved through the use of conditional statements, such as if-else or ternary operators. These statements allow developers to check for certain conditions and render specific components accordingly.\nFor example, let’s say we want to render a component that displays a message if a user is logged in and another message if they’re not logged in. We can achieve this using the following code:\nfunction Greeting(props) { const isLoggedIn = props.isLoggedIn; if (isLoggedIn) { return \u003ch1\u003eWelcome back!\u003c/h1\u003e; } return \u003ch1\u003ePlease sign up.\u003c/h1\u003e; } In this code, we’re checking the value of the isLoggedIn prop and rendering either the “Welcome back!” message or the “Please sign up.” message based on the result of the check.\nAdvanced Techniques for Conditional Rendering in React While conditional statements are a powerful tool for conditional rendering in React, there are other techniques that can be used to achieve more complex functionality.\nConditional Rendering with if..else in React React’s conditional rendering operates in the same way that JavaScript’s conditions do. If you use JavaScript operators like if, React will adjust the UI to match. With our condition, we use an if and return the element to be rendered.\nLet’s us consider this two components:\n// LoginIn Component function LoggedIn() { return ( \u003cdiv\u003e \u003ch1\u003eWelcome \u003c/h1\u003e \u003cButton\u003eLog out \u003c/Button\u003e \u003c/div\u003e ); } //LogOut Component function LoggedOut() { return ( \u003cdiv\u003e \u003ch1\u003eSign in, please! \u003c/h1\u003e \u003cButton\u003eLog out \u003c/Button\u003e \u003c/div\u003e ); } Depending on whether a user is logged in or not, we’ll construct a Status component that shows either of these components. Depending on the value of the isLogin, a different greeting is displayed.\nfunction Status({ isLogin }) { if (isLogin) { return \u003cLoggedIn /\u003e; } return \u003cLoggedOut /\u003e; } Conditional Rendering with Switch Case in React Using an if...else statement, you can conditionally return different markup from a component based on given circumstances, as illustrated earlier. A switch statement may be used to accomplish the same thing, allowing you to specify the markup for several scenarios.\nfunction Status({ isLogin }) { switch (isLogin) { case true: return \u003cLoggedIn /\u003e break; case false: return \u003cLoggedOut /\u003e break; default: return null; } When there are more than two alternative values or outcomes, the switch statement method is more practical.\nKeep in mind that you must always use default for the switch case operator in React since a component must always return an element or null.\nAdditionally, if a component returns null, it will conceal itself (display nothing). This is a handy technique to toggle component visibility.\nConditional Rendering with Ternary Operator in React The only JavaScript operator that takes three operands is the conditional (ternary) operator. The if statement is frequently replaced with this operator.\ncondition ? \" Render if True\" : \"Render if False\";\nThe operator returns “Render if True” if the condition evaluates to true; otherwise, it returns “Render if False” if the condition evaluates to false.\nfunction Status({isLogin}) { return( { isLogin ? \u003cLoggedIn /\u003e : \u003cLogdedOut /\u003e }) } Conditional Rendering Using Logical \u0026\u0026 (Short Circuit Evaluation) in React Short circuit evaluation is a technique for ensuring that no side effects occur when operands in an expression are evaluated. The logical \u0026\u0026 allows you to declare that an action should be performed only if one of the conditions is met; else, it will be disregarded.\nWarning: Below is an example of code that should be avoided since it is inefficient .\nfunction Status({isLogin}) { return( { isLogin \u0026\u0026 \u003cLoggedOut /\u003e } { !isLogin \u0026\u0026 \u003cLoggedIn /\u003e } ) } Based on the value of isLogin, this code would render the appropriate component. However, because there are better, cleaner techniques to create the same effect, this is not recommended. This usage of short circuit evaluation may become onerous and unintuitive as your application grows.\nConditional Rendering with Enum in React When used as a map of key-value pairs in JavaScript, an object can be utilised as an enum.\nconst ENUMOBJECT = { a: \"apple\", b: \"ball\", c: \"cat\", }; Now let’s create an ENUM object from our example above:\nconst ENUMSTATE = { login: \u003cLoggedIn /\u003e, logout: \u003cLoggedOut /\u003e } Create a function that takes state as an argument and returns components based on that value. The Status function in the example below is self-explanatory.\nfunction Status({ state }) { return \u003cdiv\u003e {ENUMSTATE[state]} \u003c/div\u003e; } Conclusion In conclusion, conditional rendering is an important feature of React that allows developers to create dynamic and responsive user interfaces. In this comprehensive guide, we’ve covered everything from the basics to advanced techniques, helping you master React’s conditional rendering functionality.\nWhether you’re a beginner or an experienced React developer, mastering conditional rendering will help you build better and more complex web applications. So, start experimenting with these techniques and take your React skills to the next level!\nReferences React React Hooks React Router React State Managment ","description":"Learn how to use conditional rendering in React to render different components based on certain conditions. Our comprehensive guide covers everything from the basics to advanced techniques, helping you master React's conditional rendering functionality.","tags":["programming","react"],"title":"Mastering React Conditional Rendering- A Comprehensive Guide","uri":"/collections/programming/react/react-conditional-rendering/"},{"content":"Candlestick Pattern - Rising Window and Falling Window All of the candle signals we have seen so far have been reversal. In fact, most candle signals are trend reversal. There are, however, a group of candle patterns that are continuation indicators. A continuation pattern is one in which the market should continue the same trend as theat in force before the continuation pattern. For instance, a continuation pattern following a rally means that the trend remains up and we should expect the rally to remain in force.\nRising Window Candlestick Pattern A gap-up is another name for a Rising Window candlestick pattern.The support and resistance zones of window candlestick patterns are particularly rigid.\nDuring an uptrend, the Rising Window is a two-candle bullish continuation pattern. With the exception of the Four-Price Doji, both candles in the pattern can be of any type. The most crucial feature of the pattern is a price gap between the high and low of the first and second candles. The space (window) between two bars denotes resistance to selling pressure.\nA bullish signal is issued when a Rising Window candlestick pattern appears, indicating that more shares are being acquired above the gap-up area than are being sold. When this occurs, market makers react quickly by raising the stock’s share price to a point where buying and selling demand are more evenly balanced.\nThis significant rise in the stock price frequently occurs outside of normal trading hours, such as following the release of positive news or a strong results announcement.\nWhat Does Rising Window Candlestick Pattern Tells Us ? Now that you’ve mastered the difficult lesson of how to build a Rising Window, let’s talk about what this pattern is trying to tell investors. The space between the candles represents the distance between the high of the previous candle and the low of the current candle.\nThis trend indicates that the bulls are in good shape, and we can expect them to keep pushing the price higher. Examine the size of the gap to acquire a better understanding of the pattern’s message. A high gap denotes a significant price increase, whereas a small gap denotes a modest (and unimportant) price change.\nAn Upside Gap Tasuki has created if the two candles that follow a Rising Window do not close the window or fill the gap (including their shadows). The two candles must be of opposing colours (the first white, the second black), with the black candle opening inside the white candle’s body and closing below it. This pattern indicates that there is a pause following the upswing as the bears attempt to force the price down. They are, however, unable to succeed. The gap has not been bridged, thus we can expect the rally to continue.\nRequirements for Rising Window Pattern The requirements for a Rising Window candlestick pattern are listed below.\nIt’s a continuation candlestick pattern that’s bullish.\nA Rising Window candlestick pattern is made up of two candles.\nBetween the two candlesticks in the pattern, there must be an empty gap where the intra-day prices do not overlap.\nThe first of the two candlestick patterns could have a red coloured true body, but the most likely possibility is that the pattern is made up of two green coloured candlesticks.\nIdentifying Rising Window Pattern Follow the steps below to locate a genuine rising window candlestick pattern on the price chart.\nA gap between two bullish candlesticks is a good sign.\nIt’s the distance between a newly formed candlestick’s low and the prior candlestick’s high.\nThe two bullish candlesticks should have a substantial body, indicating strong buyer momentum.\nDuring a bullish price trend, it should form.\nDuring an uptrend, the rising window candlestick is most effective. Because it is a signal for the continuation of a trend. If, on the other hand, the rising window pattern appears below the resistance zone, a reversal is possible. As a result, trading rising window candlestick patterns immediately below the resistance zone should be avoided.\nA rising window pattern will work if it occurs after the resistance zone has been broken, and it is a positive trend continuation signal.\nFalling Window Candlestick Pattern A bearish continuation candlestick pattern is a Falling Window candlestick pattern. A gap-down candlestick pattern is also known as a Falling Window candlestick pattern.\nThe falling window is a candlestick pattern made up of two bearish candlesticks separated by a downward gap. The down gap is the distance between the recent candlestick’s high and the prior candlestick’s low. It arises as a result of the market’s excessive amount of selling orders.\nIt’s a Bearish trend continuation candlestick pattern that shows how strong the market’s sellers are. When a falling window forms on the price chart, the price of an asset/security will continue to fall.\nWhat does Falling Window Tells Us? So, what is the message being conveyed by this pattern? The signal, in this case, displays a gap between the low of the previous candle and the high of the current candle. This price drop reflects the bears’ current strength and signals that the downturn will continue.\nAlthough a Falling Window is merely a gap down, why not capitalise on its dramatic formal name? It’s also known as a Western Gap. Most of the time, this signal functions as a bearish continuation pattern, but it’s always advisable to wait for confirmation. Examine the signal’s size to gain a better understanding of it. As you could expect, a wide gap indicates a significant price drop. A minor difference isn’t as meaningful as a large one.\nWhen the two candles appear after the Falling Window, examine them. A Downside Tasuki Gap pattern may have arisen if they do not close the window or fill the gap (this includes their shadows). The first and second candles must be bearish, but the third must be bullish, in order to qualify. This suggests that after a significant downturn (as evidenced by the gap down), the bulls attempted to force the price back up. However, they were unsuccessful, and the decline is projected to continue.\nIdentifying Falling Window Pattern ? To identify a perfect falling window pattern on the price chart, follow the steps below.\nLook for two bearish candlesticks, as well as a gap between the high of the most recent bearish candlestick and the low of the most recent bearish candlestick.\nKeep in mind that the Gap should not be very wide.\nBearish candlesticks should have large bodies since this indicates strong seller momentum.\nIt should appear during a downward price trend.\nRequirements for Falling Window Pattern The specifications for a Falling Window candlestick pattern are listed below.\nIt’s a continuation candlestick pattern that’s bearish.\nA Falling Window candlestick pattern is made up of two candles.\nBetween the two candlesticks in the pattern, there must be an empty gap where the intra-day prices do not overlap.\nOne or both candlesticks could have green coloured genuine bodies, but two candlesticks with red coloured true bodies are the most likely option.\nHow to Trade Falling Window Pattern ? The price will be held back by the down gap in the falling window, which will operate as a resistance zone. Because a big number of selling orders are placed in this zone, the price will remain low until all of the selling orders are filled.\nFollowing the falling window pattern, price will either continue to fall or give a minor pullback towards the gap zone to fill some selling orders.\nTrading falling window patterns can be done in two ways. You can either place sell orders right after this candlestick formation or wait until the price does not pull back into the gap zone. Later, I’ll show you how to make a high-risk, high-reward deal.\n","description":"In this article we will lean about continuation pattern like Rising Window and Falling Window","tags":["crypto"],"title":"Candlestick Pattern - Rising Window and Falling Window","uri":"/collections/crypto/rising-falling-window/"},{"content":"Candlestick Pattern - Tower Top and Tower Bottom Candlestick patterns are worth studying in depth, and while a strategy based purely on them will be unstable and unproductive, they can be a useful supplement to a whole trading system that incorporates other strategies.\nTower Top and Tower Bottom Candlestick Pattern:\nIn technical analysis, this is a classic pattern. When a trend reversal occurs, the price will often move in a flat (sideways) pattern before the trend direction shifts.\nTower Top Candlestick Pattern Several Japanese candlesticks make up a tower’s top framework. A series of little bullish (green) candlesticks, or a single huge candlestick, might comprise the pattern’s bullish phase. There is a slowdown on the subsequent candlesticks, resulting in lateral price movement with small candlesticks (which form the top). The structure is completed by one or more huge bearish candlesticks (red). This is the polar opposite of the bottom of the tower.\nA long bullish candle forms the Tower Top pattern, after which the market moves sideways. Candlesticks of various colours and sizes can be seen during the sideways movement. These are frequently celebrities. A large bearish candle down appears at the end, which is usually a sign that the flat trend has ended and a downtrend has begun.\nNotes:\nNormally, this would be a hint that the current Trend is about to reverse.\nAfter a great elevation, a tower top often appears, distinguished by multiple big green Japanese candlesticks.\nA tower top is a reversal pattern that suggests a bearish trend reversal. This represents the increasing fatigue of purchasers before the sellers reclaim control through force.\nIt is not necessary to have two large bearish candlesticks to confirm the pattern; one is sufficient.\nThe tower top structure is invalidated if the bearish candlestick(s) are not long.\nTower Top Requirements It happens during an uptrend, and the candles that follow the Pattern must confirm it.\nThe First Candle is a long, white candle.\nThe following Candles in the “Sideways” Phase are Spinning Tops (Black or White), which represent the Market’s uncertainty.\nThe Last Candle is lengthy and dark, signalling the commencement of the current Trend’s reversal.\nTower Top Candlestick Chart Pattern Above is an illustration of the tower top candlestick pattern. Five relatively tiny genuine body candlesticks follow a massive bullish candlestick that reaches a new high for the uptrend. The huge bearish candlestick that makes up the right side of the “tower” breached the lows set by these little bodied candlesticks. The chart above exhibited a head and shoulders pattern, with a bearish tower top formation being the “head” half of the pattern.\nTower Bottom Candlestick Pattern Bullish Tower Bottom candlestick patterns are reversal candlestick patterns that are bullish in nature. These patterns usually appear towards the bottom of a long-term decline in the price of a company.\nBecause the candlestick pattern takes several trading periods to mature, it must be detected through a series of observations.\nWhen a tall red candlestick pattern appears at the end of an extended decline for a stock’s share price, and the next couple of candlesticks fail to push lower than the tall red candlestick’s close, you may be watching the formation of a Bullish Tower Bottom candlestick pattern.\nWhen you observe a Bullish Harami candlestick pattern emerge, you can occasionally predict a Bullish Tower Bottom candlestick formation. A forerunner to a Bullish Tower Bottom candlestick pattern can be a Bullish Harami candlestick pattern.\nTower Bottom Candlestick Chart Pattern A massive bearish candlestick (left side of the tower) is followed by four smaller bearish candlesticks in the chart above. The enormous bullish candlestick (right side of the tower) that closed at the same price level as the bearish candlestick five days ago completes the tower bottom pattern.\nLimitations When correctly detected, tower top and bottom patterns are quite effective. When they are misinterpreted, though, they can be highly harmful. As a result, before leaping to conclusions, one must exercise utmost caution and patience.\nThere’s a considerable difference between a tower top and one that has failed, for example. A true tower top is a negative technical pattern that can result in a stock or asset dropping dramatically. To validate a tower top’s authenticity, however, patience and identification of the important support level are required. A faulty reading of a tower top based only on the emergence of two successive peaks could lead to an early withdrawal from a position.\n","description":"In this article we will learn what is tower top and tower bottom candlestick chart pattern and how to detect it.","tags":["crypto"],"title":"Candlestick Pattern - Tower Top and Tower Bottom","uri":"/collections/crypto/tower-top-bottom-pattern/"},{"content":"Candlestick Pattern - Dumpling Tops and Frypan Bottoms Dumpling top and frypan bottoms. Someone must have been hungry when they thought of these names !!\nDumpling Top Candlestick Pattern Several Japanese candlesticks form the top structure of a dumpling. With a small body, the first candlesticks are bullish or bearish. This candlestick arrangement should form a rounded top. Following that, a final candlestick is formed with a bearish gap opening.\nWhen small real body candlesticks slowly rise and then move in a neutral to downward direction, a dumpling top occurs. When a bearish candlestick gaps down from the other candlesticks, the dumpling top pattern is complete.\nAs the market forms a convex pattern, the dumpling top usually has small real bodies. A dumpling top is confirmed when the market gaps down. This is the same pattern as the Western rounded bottom top. As proof of a top, the dumpling top should have a bottom window.\nNotes:\n– Normally, this would indicate a Bearish reversal of the current Trend.\n– It occurs during an uptrend, and the candles that follow the Pattern must confirm it.\n– The Pattern begins during an Uptrend, then becomes a “Sideways” Trend (representing market indecision); at the end of the Pattern, the Trend reverses direction and becomes a Downtrend.\n– This Pattern is quite rare; it is important that there is a Gap Down after the “Sideways” Trend and just before the Downtrend begins (To obtain a further confirmation of the reversal of the Trend, as the Pattern suggests).\nDumpling Top Chart Example The chart above depicts a dumpling top. Take note of how the top of the chart above has many small-bodied candlesticks that are mostly neutral. This demonstrates that neither the bulls nor the bears have complete control of this consolidation area. The dumpling top pattern is confirmed when a strong bearish candlestick gaps down away from the area of consolidation, and prices are expected to fall further.\nFrypan Bottom Candlestick Pattern The fry pan bottom pattern is the inverse of the dumpling top pattern. When small real body candlesticks move slowly downward and then in a neutral to upward direction, the fry pan bottom occurs. When a bullish candlestick gaps up from the rest of the candlesticks, the fry pan bottom pattern is complete.\nWhen a bullish candlestick gaps up from the rest of the candlesticks, the frypan bottom pattern is complete.\nThe bottom of the fry pan represents a market that is bottoming and whose price action forms a concave design before opening a window to the upside. It looks like a Western rounded bottom, but the Japanese fry pan bottom should have a window in an upmove to confirm the bottom.\nNotes:\n– Normally, this would indicate a Bullish reversal of the current Trend.\n– It occurs during a downtrend, and the candles that follow the Pattern must confirm it.\n– The Pattern begins during a Downtrend, then becomes a “Sideways” Trend (representing market indecision); at the end of the Pattern, the Trend reverses direction and becomes an Uptrend.\n– This Pattern is quite uncommon; it is critical that there is a Gap Up after the “Sideways” Trend and just before the start of the Uptrend (To obtain a further confirmation of the reversal of the Trend, as the Pattern suggests).\nFrypan Bottom Chart Example The diagram above depicts the bottom of a fry pan. Take note of all the small bodied candlesticks in the consolidation area after prices fell. A large bullish candlestick gapped up and away from the area of consolidation, confirming a fry pan bottom.\n","description":"Understanding Dumpling top and fry pan bottom candlestick chart pattern with examples","tags":["crypto"],"title":"Candlestick Pattern - Dumpling Tops and Frypan Bottoms","uri":"/collections/crypto/dumpling-top-and-frypan-bottoms/"},{"content":"Candlestick Pattern - Dumpling Tops and Frypan Bottoms Dumpling top and frypan bottoms. Someone must have been hungry when they thought of these names !!\nDumpling Top Candlestick Pattern Several Japanese candlesticks form the top structure of a dumpling. With a small body, the first candlesticks are bullish or bearish. This candlestick arrangement should form a rounded top. Following that, a final candlestick is formed with a bearish gap opening.\nWhen small real body candlesticks slowly rise and then move in a neutral to downward direction, a dumpling top occurs. When a bearish candlestick gaps down from the other candlesticks, the dumpling top pattern is complete.\nAs the market forms a convex pattern, the dumpling top usually has small real bodies. A dumpling top is confirmed when the market gaps down. This is the same pattern as the Western rounded bottom top. As proof of a top, the dumpling top should have a bottom window.\nNotes:\n– Normally, this would indicate a Bearish reversal of the current Trend.\n– It occurs during an uptrend, and the candles that follow the Pattern must confirm it.\n– The Pattern begins during an Uptrend, then becomes a “Sideways” Trend (representing market indecision); at the end of the Pattern, the Trend reverses direction and becomes a Downtrend.\n– This Pattern is quite rare; it is important that there is a Gap Down after the “Sideways” Trend and just before the Downtrend begins (To obtain a further confirmation of the reversal of the Trend, as the Pattern suggests).\nDumpling Top Chart Example The chart above depicts a dumpling top. Take note of how the top of the chart above has many small-bodied candlesticks that are mostly neutral. This demonstrates that neither the bulls nor the bears have complete control of this consolidation area. The dumpling top pattern is confirmed when a strong bearish candlestick gaps down away from the area of consolidation, and prices are expected to fall further.\nFrypan Bottom Candlestick Pattern The fry pan bottom pattern is the inverse of the dumpling top pattern. When small real body candlesticks move slowly downward and then in a neutral to upward direction, the fry pan bottom occurs. When a bullish candlestick gaps up from the rest of the candlesticks, the fry pan bottom pattern is complete.\nWhen a bullish candlestick gaps up from the rest of the candlesticks, the frypan bottom pattern is complete.\nThe bottom of the fry pan represents a market that is bottoming and whose price action forms a concave design before opening a window to the upside. It looks like a Western rounded bottom, but the Japanese fry pan bottom should have a window in an upmove to confirm the bottom.\nNotes:\n– Normally, this would indicate a Bullish reversal of the current Trend.\n– It occurs during a downtrend, and the candles that follow the Pattern must confirm it.\n– The Pattern begins during a Downtrend, then becomes a “Sideways” Trend (representing market indecision); at the end of the Pattern, the Trend reverses direction and becomes an Uptrend.\n– This Pattern is quite uncommon; it is critical that there is a Gap Up after the “Sideways” Trend and just before the start of the Uptrend (To obtain a further confirmation of the reversal of the Trend, as the Pattern suggests).\nFrypan Bottom Chart Example The diagram above depicts the bottom of a fry pan. Take note of all the small bodied candlesticks in the consolidation area after prices fell. A large bullish candlestick gapped up and away from the area of consolidation, confirming a fry pan bottom.\n","description":"Understanding Dumpling top and fry pan bottom candlestick chart pattern with examples","tags":["crypto"],"title":"Candlestick Pattern - Dumpling Tops and Frypan Bottoms","uri":"/post/dumpling-top-and-frypan-bottoms/"},{"content":"Candlestick Pattern - Understanding Counterattack candlestick pattern There is no shortage of reversal indicators in technical analysis and candlestick patterns. The counterattack candlestick pattern is one such trend reversal indicator that many traders use to enter a positional trade. Here’s everything you need to know about this one-of-a-kind technical indicator.\nThis indicator, also known as the counterattack lines candlestick pattern, consists of two candlesticks that move in opposite directions. It can occur during an uptrend or a downtrend and is useful for identifying trend reversals. The indicator is known as a bullish counterattack pattern when it occurs during a downtrend. When it happens during an uptrend, the indicator is called a bearish counterattack pattern.\nNotes\nBullish counterattack lines indicate a possible trend reversal from a downtrend to an uptrend.\nBearish counterattack lines indicate a possible trend reversal from an uptrend to a downtrend.\nThe pattern is made up of two candles with opposing color/direction. It is recommended to use a third and/or fourth candle to confirm the next price direction following the pattern.\nUnderstanding Counterattack Line Bullish Counterattack Line Candlestick Pattern The bullish counterattack line, also known as the bullish meeting line, is a two-candlestick pattern that appears after a downtrend and is regarded as a bottom reversal signal. The bullish counterattack line is a weaker bottom reversal signal than the nearly identical piercing pattern. The first candlestick indicates a bearish trend. The second candlestick opens far below the close of the first day’s bearish candlestick, but then rallies to close at roughly the same price as the first day’s candlestick. As a result, the second day candlestick is an extremely bullish candlestick. The large gap down on the second day gives bears confidence that the downward trend will continue; however, to bears’ surprise, prices reverse and fill the gap, closing at the same price level as the previous day’s close. That day, the bears made no progress.\nThe bullish counterattack lines candlestick pattern consists of the following characteristics:\nThe market is in a downward spiral.\nThe first candle has a long real body and is black (down).\nThe second candle gaps down on the open, is white, has a real body similar to the first candle’s real body, and closes near the first candle’s close.\nPsychology Behind Bullish Counterattack Lines Assume the market is in a strong downtrend. The first candle continues to fall, with the close falling well below the open, resulting in a long real body. This boosts bear confidence while putting bulls on the back foot. Their apprehension is justified by the opening of the second candle, which gaps down from the close of the previous session. The opening, on the other hand, depletes the supply of selling pressure, allowing bulls to lift the security in a reversal session that concludes near the close of the first candle. This price action indicates a possible bullish reversal, which will be confirmed on the third or fourth candle.\nIdentifying Bullish Counterattack Lines Candlestick Pattern The bullish counterattack candlestick pattern is a two-bar pattern that appears in the market during a downtrend. To be considered a bullish counterattack pattern, a pattern must meet the following criteria.\nThe market must be in a strong downtrend for the bullish counterattack pattern to form.\nThe first candle should be a long black candle with a solid body.\nThe second candle must also be long (ideally, the same size as the first), but it must be white and have a real body. The second candle must close near the first candle’s close.\nBearish Counterattack Line Candlestick Pattern The bearish counterattack line, also known as the bearish meeting line, is a two-candlestick pattern that appears after an uptrend and is considered a top reversal signal. The bearish counterattack line is a weaker top reversal signal than the associated dark cloud cover pattern. The first candlestick indicates a bullish trend. The second candlestick opens far above the close of the first day’s bullish candlestick, but then falls back to close at roughly the same price as the first day’s candlestick.As a result, the second day candlestick is a big bearish candlestick. The large gap up on the second day gives bulls confidence that the upward trend will continue; however, to bulls’ surprise, prices reverse downward and fill the gap, closing at the same price level as the previous day’s close. That day, the bull made no progress.\nThe bearish counterattack lines candlestick pattern is distinguished by the following characteristics:\nThe market is on the rise.\nThe first candle is white (up) and has a long genuine body.\nThe second candle gaps higher on the open, is black, has a real body similar to the first candle’s real body, and a close similar to the first candle’s close.\nPsychology Behind Bearish Counterattack Lines Assume the market is in an active uptrend. The first candle continues the upward trend, with the close well above the open, resulting in a long real body. This boosts bull confidence while putting bears on the back foot. Their apprehension is justified by the opening of the second candle, which opens with a gap up from the previous session’s close. However, the opening reduces buyer demand, allowing bears to drop the security in a reversal session that concludes near the close of the first candle. This price action indicates a possible bearish reversal, which will be confirmed on the third or fourth candle.\nIdentifying Bearish Counterattack Lines Candlestick Pattern The bearish counterattack candlestick pattern is a two-bar pattern with the characteristics listed below.\nThe market must be on an upward trend.\nThe first candle must be long and white, with a solid body.\nThe second candle must be black and of equal or similar size to the first.\nHow to use the Counterattack Candlestick Pattern It’s one thing to notice the pattern. Entering a trade using the identified pattern is a completely different story. As a result, here are some key considerations to bear in mind before engaging in a trade based on the counterattack lines candlestick pattern.\n– First, keep an eye out for a strong trend. It can be either a bullish or a bearish trend.\n– Once you’ve identified the trend, look for a candle that opens with either a ‘gap up’ or a ‘gap down.’ The openings should correspond to the current trend.\n– Pay attention to the movement of this candle. The candle should move in the opposite direction of the current trend.\n– Once that condition is met, ensure that the candle moving in the opposite direction closes close to the previous day’s close.\n– A pattern can be called a counterattack lines candlestick if it meets all of the above criteria.\n– Once the pattern has been correctly identified, it is best to wait for a confirmation candle before entering a position. In the case of a bullish counterattack pattern, for example, you should consider entering a trade only if the candle that appears after the pattern is bullish. The bullish reversal is said to have failed otherwise.\nLimitations of Counterattack Lines Counterattack lines may be untrustworthy on their own. They are typically used in conjunction with other confirming technical analysis and require confirmation candles.\nCandlestick patterns do not provide profit targets, so there is no indication of the magnitude of the reversal. The pattern may initiate a long-term reversal, or the reversal may be brief.\nAlthough the pattern does occur, it is uncommon. There will be few opportunities to use this candlestick pattern.\n","description":"On candlestick charts, counterattack lines are two-candle reversal patterns. There are bullish and bearish variations.","tags":["crypto"],"title":"Candlestick Pattern - Understanding Counterattack candlestick pattern","uri":"/collections/crypto/counter-attack-candlestick/"},{"content":"Candlestick Pattern - Understanding Counterattack candlestick pattern There is no shortage of reversal indicators in technical analysis and candlestick patterns. The counterattack candlestick pattern is one such trend reversal indicator that many traders use to enter a positional trade. Here’s everything you need to know about this one-of-a-kind technical indicator.\nThis indicator, also known as the counterattack lines candlestick pattern, consists of two candlesticks that move in opposite directions. It can occur during an uptrend or a downtrend and is useful for identifying trend reversals. The indicator is known as a bullish counterattack pattern when it occurs during a downtrend. When it happens during an uptrend, the indicator is called a bearish counterattack pattern.\nNotes\nBullish counterattack lines indicate a possible trend reversal from a downtrend to an uptrend.\nBearish counterattack lines indicate a possible trend reversal from an uptrend to a downtrend.\nThe pattern is made up of two candles with opposing color/direction. It is recommended to use a third and/or fourth candle to confirm the next price direction following the pattern.\nUnderstanding Counterattack Line Bullish Counterattack Line Candlestick Pattern The bullish counterattack line, also known as the bullish meeting line, is a two-candlestick pattern that appears after a downtrend and is regarded as a bottom reversal signal. The bullish counterattack line is a weaker bottom reversal signal than the nearly identical piercing pattern. The first candlestick indicates a bearish trend. The second candlestick opens far below the close of the first day’s bearish candlestick, but then rallies to close at roughly the same price as the first day’s candlestick. As a result, the second day candlestick is an extremely bullish candlestick. The large gap down on the second day gives bears confidence that the downward trend will continue; however, to bears’ surprise, prices reverse and fill the gap, closing at the same price level as the previous day’s close. That day, the bears made no progress.\nThe bullish counterattack lines candlestick pattern consists of the following characteristics:\nThe market is in a downward spiral.\nThe first candle has a long real body and is black (down).\nThe second candle gaps down on the open, is white, has a real body similar to the first candle’s real body, and closes near the first candle’s close.\nPsychology Behind Bullish Counterattack Lines Assume the market is in a strong downtrend. The first candle continues to fall, with the close falling well below the open, resulting in a long real body. This boosts bear confidence while putting bulls on the back foot. Their apprehension is justified by the opening of the second candle, which gaps down from the close of the previous session. The opening, on the other hand, depletes the supply of selling pressure, allowing bulls to lift the security in a reversal session that concludes near the close of the first candle. This price action indicates a possible bullish reversal, which will be confirmed on the third or fourth candle.\nIdentifying Bullish Counterattack Lines Candlestick Pattern The bullish counterattack candlestick pattern is a two-bar pattern that appears in the market during a downtrend. To be considered a bullish counterattack pattern, a pattern must meet the following criteria.\nThe market must be in a strong downtrend for the bullish counterattack pattern to form.\nThe first candle should be a long black candle with a solid body.\nThe second candle must also be long (ideally, the same size as the first), but it must be white and have a real body. The second candle must close near the first candle’s close.\nBearish Counterattack Line Candlestick Pattern The bearish counterattack line, also known as the bearish meeting line, is a two-candlestick pattern that appears after an uptrend and is considered a top reversal signal. The bearish counterattack line is a weaker top reversal signal than the associated dark cloud cover pattern. The first candlestick indicates a bullish trend. The second candlestick opens far above the close of the first day’s bullish candlestick, but then falls back to close at roughly the same price as the first day’s candlestick.As a result, the second day candlestick is a big bearish candlestick. The large gap up on the second day gives bulls confidence that the upward trend will continue; however, to bulls’ surprise, prices reverse downward and fill the gap, closing at the same price level as the previous day’s close. That day, the bull made no progress.\nThe bearish counterattack lines candlestick pattern is distinguished by the following characteristics:\nThe market is on the rise.\nThe first candle is white (up) and has a long genuine body.\nThe second candle gaps higher on the open, is black, has a real body similar to the first candle’s real body, and a close similar to the first candle’s close.\nPsychology Behind Bearish Counterattack Lines Assume the market is in an active uptrend. The first candle continues the upward trend, with the close well above the open, resulting in a long real body. This boosts bull confidence while putting bears on the back foot. Their apprehension is justified by the opening of the second candle, which opens with a gap up from the previous session’s close. However, the opening reduces buyer demand, allowing bears to drop the security in a reversal session that concludes near the close of the first candle. This price action indicates a possible bearish reversal, which will be confirmed on the third or fourth candle.\nIdentifying Bearish Counterattack Lines Candlestick Pattern The bearish counterattack candlestick pattern is a two-bar pattern with the characteristics listed below.\nThe market must be on an upward trend.\nThe first candle must be long and white, with a solid body.\nThe second candle must be black and of equal or similar size to the first.\nHow to use the Counterattack Candlestick Pattern It’s one thing to notice the pattern. Entering a trade using the identified pattern is a completely different story. As a result, here are some key considerations to bear in mind before engaging in a trade based on the counterattack lines candlestick pattern.\n– First, keep an eye out for a strong trend. It can be either a bullish or a bearish trend.\n– Once you’ve identified the trend, look for a candle that opens with either a ‘gap up’ or a ‘gap down.’ The openings should correspond to the current trend.\n– Pay attention to the movement of this candle. The candle should move in the opposite direction of the current trend.\n– Once that condition is met, ensure that the candle moving in the opposite direction closes close to the previous day’s close.\n– A pattern can be called a counterattack lines candlestick if it meets all of the above criteria.\n– Once the pattern has been correctly identified, it is best to wait for a confirmation candle before entering a position. In the case of a bullish counterattack pattern, for example, you should consider entering a trade only if the candle that appears after the pattern is bullish. The bullish reversal is said to have failed otherwise.\nLimitations of Counterattack Lines Counterattack lines may be untrustworthy on their own. They are typically used in conjunction with other confirming technical analysis and require confirmation candles.\nCandlestick patterns do not provide profit targets, so there is no indication of the magnitude of the reversal. The pattern may initiate a long-term reversal, or the reversal may be brief.\nAlthough the pattern does occur, it is uncommon. There will be few opportunities to use this candlestick pattern.\n","description":"On candlestick charts, counterattack lines are two-candle reversal patterns. There are bullish and bearish variations.","tags":["crypto"],"title":"Candlestick Pattern - Understanding Counterattack candlestick pattern","uri":"/post/counter-attack-candlestick/"},{"content":"OpenCV - Face Recognition using LBPH Classifier in C++ We learned HOW TO RECOGNIZE FACES in my OpenCV tutorial. Now, as shown below, we will gather and use our own data to recognize different faces using LBPH:\nTo complete a Face Recognition project, we must work on three distinct phases:\nFace Recognition and data collection Train the recognizer. Recognition of Faces The following block diagram summarises those phases:\nFace Recognition and Data Collection In our last tutorial we have seen how to recognize faces in the given photo using Cascade Classifier. In this tutorial we will go one step further.\nThat is we will give our algorithm different images and we will extract the faces from the images and store it in some variable for our next step.\nHere the input image which we will provide will be out dataset. Here you can give any person images but in our case I have given images of famous celebrities Alexandra Daddario and Scarlett Johansson.\nDirectory Structure is as follows:\n./datasets alexandra-daddari-0/ img1 img2 img3 scarlet-johansson-1/ img1 img2 img3 Extracting Faces from images:\ncv::CascadeClassifier classifier; int extractfaces(cv::Mat \u0026img, cv::Mat \u0026face) { cv::Mat img_gray; cv::cvtColor(img, img_gray, cv::COLOR_BGR2GRAY); std::vector\u003ccv::Rect\u003e features; classifier.detectMultiScale(img, features); /* Under Assumption that image consist of only single face */ if (features.size() \u003e 0) { face = img_gray(features[0]); return 1; } else return 0; } Train the Recognizer After extracting faces from the images we will train those images. For this Opencv provide us with a wonderfull algorithm called Local Binary Pattern Histogram(LBPH) Face-Recognition algorithm.It is well-known for its performance and ability to recognise a person’s face from both the front and side faces.\nTo know how LBPH Face Algorihtm Work read this.\ncv::Ptr\u003ccv::face::LBPHFaceRecognizer\u003e model = cv::face::LBPHFaceRecognizer::create(); void train_dataset(const std::string \u0026in_dir) { std::cout \u003c\u003c \"Starting To Train Dataset...\\n\"; std::vector\u003cint\u003e faceIds; std::vector\u003ccv::Mat\u003e faceSamples; for (const auto \u0026file : std::filesystem::directory_iterator(in_dir)) { std::string label = file.path().filename(); std::cout \u003c\u003c \"Images in directory: \" \u003c\u003c label \u003c\u003c \"...\\n\"; /*Directory Format: dir_name_0 ,dir_name_1 */ int faceID = std::stoi(label.substr(label.rfind(\"_\") + 1, label.size())); for (const auto \u0026f : std::filesystem::directory_iterator(file.path())) { cv::Mat src, face; std::cout \u003c\u003c f \u003c\u003c \"\\n\"; src = cv::imread(f.path()); if (extractfaces(src, face)) { faceSamples.emplace_back(face); faceIds.emplace_back(faceID); } } model-\u003etrain(faceSamples, faceIds); std::cout \u003c\u003c \"Done!!\\n\"; } std::cout \u003c\u003c \"Training Datasets Done !! \\n\"; } Here faceSamples variable stores faces and faceID variable is refrence to whoes face it belongs to i.e. 0 means that face is of Alexandra Daddario and 1 means face is of Scarlett Johansson.\nRecognition of Faces After our classifier(LBPH) is trainned enough images it’s time to test our model.\nFor this just create a test directory and put the images which you want to test. In testing phase once again we will extract faces from our test image and then feed it to our model for pridiction.\nvoid test_dataset(const std::string \u0026in_dir) { std::cout \u003c\u003c \"Testing Datasets... \\n\"; const std::vector\u003cstd::string\u003e names = { \"Alexender Dadario\", \"Scarlet Johanson\"}; // Arrange name according to // directory in train_dataset for (const auto \u0026file : std::filesystem::directory_iterator(in_dir)) { cv::Mat src = cv::imread(file.path()); cv::Mat face; extractfaces(src, face); int predictedLabel = -1; double confidence = 0.0; model-\u003epredict(face, predictedLabel, confidence); std::cout \u003c\u003c predictedLabel \u003c\u003c \"\\n\"; /* If No Match Found prediction will be -1 */ if (predictedLabel != -1) { cv::putText(src, names[predictedLabel], cv::Point(10, src.rows / 2), cv::FONT_HERSHEY_DUPLEX, 1.0, CV_RGB(118, 185, 0), 1); cv::imshow(file.path().filename(), src); std::cout \u003c\u003c \"File: \" \u003c\u003c file.path().filename() \u003c\u003c \"\\nPrediction: \" \u003c\u003c names[predictedLabel] \u003c\u003c \"\\nConfidence: \" \u003c\u003c confidence \u003c\u003c \"\\n\\n\\n\"; } else { cv::putText(src, \"Match Not Found\", cv::Point(10, src.rows / 2), cv::FONT_HERSHEY_DUPLEX, 1.0, CV_RGB(118, 185, 0), 1); cv::imshow(file.path().filename(), src); std::cout \u003c\u003c \"File: \" \u003c\u003c file.path().filename() \u003c\u003c \"Prediction: Unknown To Model\" \u003c\u003c \"\\n\"; } } } Output References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook OpenCV Docs OpenCV Docs Code #include \"opencv2/face.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ccassert\u003e #include \u003cfilesystem\u003e #include \u003ciostream\u003e #include \u003cvector\u003e cv::CascadeClassifier classifier; cv::Ptr\u003ccv::face::LBPHFaceRecognizer\u003e model = cv::face::LBPHFaceRecognizer::create(); int extractfaces(cv::Mat \u0026img, cv::Mat \u0026face) { cv::Mat img_gray; cv::cvtColor(img, img_gray, cv::COLOR_BGR2GRAY); std::vector\u003ccv::Rect\u003e features; classifier.detectMultiScale(img, features); /* Under Assumption that image consist of only single face */ if (features.size() \u003e 0) { face = img_gray(features[0]); return 1; } else return 0; } void train_dataset(const std::string \u0026in_dir) { std::cout \u003c\u003c \"Starting To Train Dataset...\\n\"; std::vector\u003cint\u003e faceIds; std::vector\u003ccv::Mat\u003e faceSamples; for (const auto \u0026file : std::filesystem::directory_iterator(in_dir)) { std::string label = file.path().filename(); std::cout \u003c\u003c \"Images in directory: \" \u003c\u003c label \u003c\u003c \"...\\n\"; /*Directory Format: dir_name_0 ,dir_name_1 */ int faceID = std::stoi(label.substr(label.rfind(\"_\") + 1, label.size())); for (const auto \u0026f : std::filesystem::directory_iterator(file.path())) { cv::Mat src, face; std::cout \u003c\u003c f \u003c\u003c \"\\n\"; src = cv::imread(f.path()); if (extractfaces(src, face)) { faceSamples.emplace_back(face); faceIds.emplace_back(faceID); } } model-\u003etrain(faceSamples, faceIds); std::cout \u003c\u003c \"Done!!\\n\"; } std::cout \u003c\u003c \"Training Datasets Done !! \\n\"; } void test_dataset(const std::string \u0026in_dir) { std::cout \u003c\u003c \"Testing Datasets... \\n\"; const std::vector\u003cstd::string\u003e names = { \"Alexender Dadario\", \"Scarlet Johanson\"}; // Arrange name according to // directory in train_dataset for (const auto \u0026file : std::filesystem::directory_iterator(in_dir)) { cv::Mat src = cv::imread(file.path()); cv::Mat face; extractfaces(src, face); int predictedLabel = -1; double confidence = 0.0; model-\u003epredict(face, predictedLabel, confidence); std::cout \u003c\u003c predictedLabel \u003c\u003c \"\\n\"; /* If No Match Found prediction will be -1 */ if (predictedLabel != -1) { cv::putText(src, names[predictedLabel], cv::Point(10, src.rows / 2), cv::FONT_HERSHEY_DUPLEX, 1.0, CV_RGB(118, 185, 0), 1); cv::imshow(file.path().filename(), src); std::cout \u003c\u003c \"File: \" \u003c\u003c file.path().filename() \u003c\u003c \"\\nPrediction: \" \u003c\u003c names[predictedLabel] \u003c\u003c \"\\nConfidence: \" \u003c\u003c confidence \u003c\u003c \"\\n\\n\\n\"; } else { cv::putText(src, \"Match Not Found\", cv::Point(10, src.rows / 2), cv::FONT_HERSHEY_DUPLEX, 1.0, CV_RGB(118, 185, 0), 1); cv::imshow(file.path().filename(), src); std::cout \u003c\u003c \"File: \" \u003c\u003c file.path().filename() \u003c\u003c \"Prediction: Unknown To Model\" \u003c\u003c \"\\n\"; } } } int main(int argc, char **argv) { if (argc \u003c 3) { std::cout \u003c\u003c \"!! ERROR !!!\\n\"; std::cout \u003c\u003c \"\u003cProgram\u003e \u003cDatasetFolder\u003e \u003cTest Folder\u003e\\n\"; return -1; } std::cout \u003c\u003c \"Loading Haarcascade File ...\\n\"; assert(classifier.load(\"haarcascade_frontalface_alt.xml\")); std::cout \u003c\u003c \"Done!!\\n\"; train_dataset(argv[1]); test_dataset(argv[2]); cv::waitKey(0); return 0; } ","description":"In this tutorial we will walk through creating and trainning our own model which will recognize different faces using LBPHFaceRecognizer","tags":["programming","cpp","opencv"],"title":"OpenCV - Face Recognition using LBPH Classifier in C++","uri":"/collections/programming/cpp/opencv/opencv-creating-own-dataset-cpp/"},{"content":"OpenCV - Face Detection using Haar cascade classifiers (C++) The fact that computer vision makes very futuristic-sounding jobs a reality is one of the many reasons why it is a fascinating subject. Face detection is one such feature. Face detection is a built-in feature of OpenCV, and it has nearly endless uses in the real world, ranging from security to entertainment.\nThis article walks you through some of OpenCV’s face detection features, as well as the data files that define different sorts of trackable objects. We focus on Haar cascade classifiers, which use contrast between adjacent image regions to evaluate if a particular image or subimage corresponds to a recognised type.\nVisualizing Haar cascades What exactly are we hoping to pinpoint when we talk about classifying objects and tracking their location? What constitutes a distinguishable feature of an object?\nPhotographic images, even those captured by a webcam, can contain a great deal of detail for our (human) viewing pleasure. Image detail, on the other hand, is prone to instability due to changes in lighting, viewing angle, viewing distance, camera shake, and digital noise. Furthermore, even real differences in physical detail may not be of interest to us for classification purposes.\nAs a result, some method of abstracting image detail is beneficial in producing consistent classification and tracking results. The abstractions are known as features, and they are said to be extracted from image data.\nThough each pixel may influence multiple features, there should be far fewer features than pixels. The degree of similarity between two images can be calculated using Euclidean distances between their corresponding features.\nDistance, for example, could be defined in terms of spatial coordinates or colour coordinates. Haar-like features are a common type of feature used in real-time face tracking. Each Haar-like feature describes the contrast pattern between adjacent image regions. Edges, vertices, and thin lines, for example, all produce distinct features.\nThe features of any given image may vary depending on the size of the region; this is referred to as the window size. Two images that differ only in scale should produce similar results, albeit for different window sizes. As a result, it is advantageous to generate features for a variety of window sizes. A cascade is a collection of such features. A Haar cascade is said to be scale-invariant, or resistant to changes in scale. OpenCV includes a classifier and tracker for scale-invariant Haar cascades that are in a specific file format.\nTheroy We will be working with face detection in this section. To train the classifier, the algorithm requires a large number of positive images (images of faces) and negative images (images without faces). Then we must extract characteristics from it. Haar features such as those shown in the image below are used for this. They are analogous to our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\nMany features are now calculated using all possible sizes and locations of each kernel. (Can you imagine how much computation is required? Even a 24x24 window yields more than 160000 features). We must find the sum of the pixels under white and black rectangles for each feature calculation. They used the integral image to solve this problem. Whatever the size of your image, it reduces the calculations for a given pixel to a four-pixel operation. Isn’t it lovely? It accelerates everything.\nHowever, the majority of the features we calculated are irrelevant. Take a look at the image below. Two good features are shown in the top row. The first feature chosen appears to emphasise the fact that the area around the eyes is frequently darker than the area around the nose and cheeks. The second feature chosen is the fact that the eyes are darker than the bridge of the nose. However, applying the same windows to the cheeks or any other location is pointless. So, how do we pick the best features from a list of over 160000? Adaboost accomplishes this.\nFor this, we use every feature on all of the training images. It determines the best threshold for each feature to classify the faces as positive or negative. Errors and misclassifications are unavoidable. We choose the features with the lowest error rate, which means they accurately classify face and non-face images. (The procedure is not as straightforward as this. At first, each image is given the same weight. The weights of misclassified images are increased after each classification. Then the process is repeated. Error rates are calculated at a new level. There are also new weights. The process is repeated until the required accuracy or error rate is obtained (or until the required number of features is discovered).\nThe final classifier is a weighted average of these poor classifiers. It is called weak because it cannot classify an image on its own, but when combined with others, it forms a strong classifier. According to the paper, even 200 features provide detection with 95% accuracy. Their final configuration included around 6000 features. (Imagine going from 160000+ features to 6000 features. That is a significant gain). So you take a picture now. Consider each 24x24 window. Apply a total of 6000 features to it. Check to see if it is a face or not. Wow.. Isn’t it inefficient and time-consuming? It is, indeed. The authors have a good solution to this problem.\nThe majority of an image is non-face region. As a result, it is preferable to have a simple method for determining whether or not a window is a face region. If it isn’t, discard it in one go and don’t process it again. Instead, concentrate on areas where a face could appear. This allows us to spend more time inspecting potential face regions.\nThey introduced the concept of Cascade of Classifiers to accomplish this. Instead of applying all 6000 features to a single window, the features are divided into stages of classifiers and applied one at a time. (Normally, the first few stages will have far fewer features.) If a window fails the first stage, it should be discarded. We do not take into account the remaining features on it. If it passes, move on to the second stage of features and repeat the process. A face region is a window that passes through all stages. How’s that plan coming along?\nThe authors’ detector had more than 6000 features and 38 stages, with 1, 10, 25, 25, and 50 features in the first five stages. (The two features in the above image are the best two features obtained from Adaboost.) According to the authors, 10 features out of 6000+ are evaluated on average per sub-window.\nImplementing Haar Cascade Classifier Now let’s see how to implement haar cascade classifier using OpenCV and C++\n1. Reading Image(or Video) and Loading Haar Cascade pre-trainned model In the example below I will be using a image and pre-trainned model of haar cascade for face detection. There are few more pre-trainned model provided by OpenCV here. All you have to do is download xml file of pre-trainned model and load it into your code.\ncv::Mat src = cv::imread(argv[1]); assert(!src.empty()); cv::CascadeClassifier classifier; //Haar Cascade Classifier assert(classifier.load(argv[2])); //Loading xml here 2. Converting original Image(or frame if Video) to grayscale void detectFeatures(cv::CascadeClassifier \u0026cls, cv::Mat \u0026img) { ... cv::Mat img_gray; cv::cvtColor(img, img_gray, cv::COLOR_BGR2GRAY); ... } 3. Detecting Feature After loading haarcascades classifier with our pre-trainned model of face detection,we will now extract this feature from our input image(or frame if video):\nvoid detectFeatures(cv::CascadeClassifier \u0026cls, cv::Mat \u0026img) { ... std::vector\u003ccv::Rect\u003e features; cls.detectMultiScale(img, features); ... } 4. Displaying Detected Features Now let us dispaly features which are being detected:\nvoid detectFeatures(cv::CascadeClassifier \u0026cls, cv::Mat \u0026img) { ... cv::Scalar green(0, 255, 0); for (auto \u0026feature : features) cv::rectangle(img, feature, green, 2, 1, 0); cv::imshow(\"Output\", img); ... } Output References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook OpenCV Docs Code #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \"opencv2/objdetect.hpp\" #include \u003ccassert\u003e #include \u003ciostream\u003e #include \u003cvector\u003e void detectFeatures(cv::CascadeClassifier \u0026cls, cv::Mat \u0026img) { cv::Mat img_gray; cv::cvtColor(img, img_gray, cv::COLOR_BGR2GRAY); std::vector\u003ccv::Rect\u003e features; cls.detectMultiScale(img, features); cv::Scalar green(0, 255, 0); for (auto \u0026feature : features) cv::rectangle(img, feature, green, 2, 1, 0); cv::imshow(\"Output\", img); cv::imwrite(\"output.png\", img); } int main(int argc, char **argv) { if (argc \u003c 3) { std::cout \u003c\u003c \"!! ERROR !!!\\n\"; std::cout \u003c\u003c \"\u003cProgram\u003e \u003cimage\u003e \u003charcascade file\u003e\\n\"; return -1; } cv::Mat src = cv::imread(argv[1]); assert(!src.empty()); cv::CascadeClassifier classifier; assert(classifier.load(argv[2])); detectFeatures(classifier, src); cv::waitKey(0); return 0; } ","description":"In this tutorial we will learn how to detect face using Haar Cascade classifier pre-trainned model using OpenCV and C++","tags":["programming","cpp","opencv"],"title":"OpenCV - Face Detection using Haar cascade classifiers (C++)","uri":"/collections/programming/cpp/opencv/opencv-face-detection/"},{"content":"OpenCV - Removing Background from an Image using GrabCut Algorithm (C++) This may be appropriate for you if you are a newbie in the field of computer vision, as I am. Let’s get this party started! Oh, and without the code, as well… Making object segmentation work on computers has been a difficult task for computers and even engineers. Until recently (well, maybe not so recently), an algorithm was used to make this task appear simple. Before we get into detail about the algorithm, let’s define object segmentation at a high level.\nObject segmentation is the process of breaking down an image into smaller pieces (as the name implies). Specifically, following object segmentation, the computer can determine how many items are there in the image and whether an object in the image belongs in the forefront or background. This is where the GrabCut algorithm comes in. To segment the image into foregrounds and backgrounds, we’ll utilise the GrabCut algorithm.\nGrabCut Carsten Rother, Vladimir Kolmogorov, and Andrew Blake of Microsoft Research Cambridge, UK, developed the GrabCut method in their article “GrabCut”: interactive foreground extraction with iterated graph cuts. GrabCut was created to solve the problem of foreground extraction with minimal user interaction.\nHow does it function from the user’s perspective? The user first draws a rectangle around the foreground area (foreground region should be completely inside the rectangle). The algorithm then segments it iteratively until the best result is obtained. Done. However, the segmentation may not be perfect in other circumstances, such as when it mistakenly labels some foreground regions as background and vice versa. In this instance, the user must perform delicate touch-ups.\nSimply draw some strokes on the photographs where there are some erroneous results. Strokes essentially indicate “Hey, this section should be foreground, but you marked it as background; fix it in the next iteration” or the inverse for background. Then you achieve better results in the next iteration.\nHow GrabCut Algorithm Works You begin by sketching a rectangle on an image, which should include the image’s subjects, such as a human or a dog. As a result, the background is automatically determined as the area outside of the rectangle you just drew. Within the defined rectangle, the data in the background is used as a reference to distinguish background from foreground locations.\nBased on the facts we provided, the computer does an initial labelling. It identifies the pixels in the foreground and background (or it hard-labels).\nTo put it simply, this approach uses the Gaussian Mixture Model (GMM) to define the area in the rectangle as a colour distribution model, with each pixel labelled to indicate whether it is foreground, background, or unknown. If you know a little bit about image processing, you’ll know that each pixel is connected to the next by a gradient, therefore this model will encourage pixels with similar colour distributions to have the same label.\nGMM learns and creates new pixel distributions based on the data we provide. In other words, the unknown pixels are labelled as probable foreground or probable background based on their colour statistical relationship with the other hard-labelled pixels (It is just like clustering).\nThis pixel distribution is used to create a graph. Pixels are the nodes in the graphs. Two new nodes have been added: a Source node and a Sink node. Every pixel in the foreground is connected to the Source node, while every pixel in the background is connected to the Sink node.\nThe likelihood of a pixel being foreground/background determines the weights of edges connecting pixels to source node/end node. The edge information or pixel similarity determines the weights between the pixels. If there is a significant colour difference between pixels, the edge between them will have a low weight.\nAfter creating graph with initialized weghts, we use Min-Cut algorithm, to yield 2 group of vertices.\nThen we create a graph with N vertices (N=number of pixels) and connect them with edges whose weights are determined by the colour similarity of the vertices (pixels). After that, we’ll add two vertices to the network (labels for foreground and background), each of which will be linked to N pixels based on the likelihood of the pixel matching a colour distribution of the backdrop or foreground. To put it another way, the GrabCut approach creates two labels, one for the background and one for the foreground, and uses the colour distribution of each pixel to link all the pixels to itself.\nImplementing GrabCut Algorithm in OpenCV using C++ Now let’s looks how to implement grabcut algorithm using c++\n1. Reading Image cv::Mat src = cv::imread(argv[1]); assert(!src.empty()); 2. Getting Bounding Box The rectangle is entered by the user. Outside of this region, everything will be treated as a solid background (That is the reason it is mentioned before that your rectangle should include all the objects). The contents of the rectangle are unknown. Similarly, any user input that specifies foreground and background is considered hard-labelling, meaning it will not alter during the procedure.\nvoid showImg(const std::string\u0026 name,cv::Mat\u0026 img) { cv::imshow(name,img); cv::waitKey(0); } void getBoundingBox(cv::Mat\u0026 img,cv::Rect\u0026 rect) { cv::Point pt,pt2; int size; for(;;) { cv::Mat temp = img.clone(); std::cout \u003c\u003c \"Insert x1,y1,x2,y2 Point:\\n\"; std::cin \u003e\u003e pt.x \u003e\u003e pt.y \u003e\u003e pt2.x \u003e\u003e pt2.y; cv::rectangle(temp,pt,pt2,cv::Scalar(0,255,0),3); showImg(\"boundingBox\",temp); char choice; std::cout \u003c\u003c \"Do You want to change paramters (y/n)\"; std::cin \u003e\u003e choice; if(choice == 'n'){ rect = cv::Rect(pt.x,pt.y,pt2.x,pt2.y); break; } } } ... cv::React boundingBox; getBoundingBox(src,boundingBox) ... 3. Creating Gaussain Mixture Model(GMM) Based on the facts we provided, the computer does an initial labelling. It identifies the pixels in the foreground and background (or it hard-labels) The foreground and background are now modelled using a Gaussian Mixture Model (GMM). GMM learns and creates new pixel distributions based on the data we provide. In other words, the unknown pixels are labelled as probable foreground or probable background based on their colour statistical relationship with the other hard-labelled pixels (It is just like clustering). ... cv::Mat mask = cv::Mat::zeros(src.rows,src.cols,CV_8UC1); cv::Mat bgModel,fgModel; //Model which will store background and foreground pixel respectively ... 4. Implementing GrabCut Algorithm ... unsigned int iteration = 5; //Tune Parameter according to need cv::grabCut(src,mask,boundingBox,bgModel,fgModel,iteration,cv::GC_INIT_WITH_RECT); cv::Mat mask2 = (mask == 1) + (mask == 3); // 0 = cv::GC_BGD, 1 = cv::GC_FGD, 2 = cv::PR_BGD, 3 = cv::GC_PR_FGD ... 5. Displaying Image ... cv::Mat dest; src.copyTo(dest, mask2); showImg(\"dest\", dest); ... Output References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook OpenCV Docs Code #include \u003ciostream\u003e #include \u003ccassert\u003e #include \u003copencv2/opencv.hpp\u003e void showImg(const std::string\u0026 name,cv::Mat\u0026 img) { cv::imshow(name,img); cv::waitKey(0); } void getBoundingBox(cv::Mat\u0026 img,cv::Rect\u0026 rect) { cv::Point pt,pt2; int size; for(;;) { cv::Mat temp = img.clone(); std::cout \u003c\u003c \"Insert x1,y1,x2,y2 Point:\\n\"; std::cin \u003e\u003e pt.x \u003e\u003e pt.y \u003e\u003e pt2.x \u003e\u003e pt2.y; cv::rectangle(temp,pt,pt2,cv::Scalar(0,255,0),3); showImg(\"boundingBox\",temp); char choice; std::cout \u003c\u003c \"Do You want to change paramters (y/n)\"; std::cin \u003e\u003e choice; if(choice == 'n'){ rect = cv::Rect(pt.x,pt.y,pt2.x,pt2.y); break; } } } int main(int argc,char** argv) { cv::Mat src = cv::imread(argv[1]); assert(!src.empty()); // cv::Rect boundingBox(150,80,410,500); //My Image Bounding Box Value /* Reading Bounding Box */ cv::React boundingBox; getBoundingBox(src,boundingBox); cv::Mat mask = cv::Mat::zeros(src.rows,src.cols,CV_8UC1); cv::Mat bgModel,fgModel; unsigned int iteration = 5; //Tune Parameter according to need cv::grabCut(src,mask,boundingBox,bgModel,fgModel,iteration,cv::GC_INIT_WITH_RECT); cv::Mat mask2 = (mask == 1) + (mask == 3); // 0 = cv::GC_BGD, 1 = cv::GC_FGD, 2 = cv::PR_BGD, 3 = cv::GC_PR_FGD cv::Mat dest; src.copyTo(dest, mask2); showImg(\"dest\", dest); cv::waitKey(0); return 0; } ","description":"In this article we will learn to seprate foreground from background of an image using grabcut algorithm","tags":["programming","cpp","opencv"],"title":"OpenCV - Removing Background from an image using GrabCut algorithm(C++)","uri":"/collections/programming/cpp/opencv/opencv-grabcut/"},{"content":"OpenCV - Image Segmentation using Watershed algorithm C++ The watershed technique is a well-known segmentation algorithm that is particularly good for extracting touching or overlapping items in photographs.\nMarker-based Watershed algorithm We must start with user-defined markers when using the watershed method. These markers can be manually defined using point-and-click, or they can be defined automatically or heuristically using thresholding and/or morphological processes.\nThe watershed technique treats pixels in our input image as local elevation (called topography) based on these markers — the method “floods” valleys from the markers outwards until the valleys of various markers meet. The markers must be set accurately in order to produce an accurate watershed segmentation.\nWe used an OpenCV marker-based watershed technique to designate which valley points should be merged and which should not. It is an interactive image segmentation rather than an automatic image segmentation.\nTheory Any grayscale image can be regarded as a topographic surface, with peaks and hills denoting high intensity and valleys denoting low intensity. You begin by filling isolated valleys (local minima) with a variety of coloured water (labels). River from separate valleys, plainly of different colours, will start to blend as the water rises, depending on the neighbouring peaks (gradients). To avoid this, you construct barriers where water meets water. You keep filling the water and erecting obstacles until all of the peaks are submerged. The segmentation outcome is then determined by the obstacles you created. The watershed’s “philosophy” is as follows. You may learn more about watershed on the CMM website, which includes animations.\nHowever, due to noise or other anomalies in the image, this method produces an oversegmented result. As a result, OpenCV created a marker-based watershed method that allows you to select which valley points should be merged and which should not. It’s a segmentation of an image that you may interact with. What we do is assign distinct labels to the objects we are familiar with. Label the zone that we are certain is the foreground or object with one colour (or intensity), the region that we are certain is the background or non-object with another colour, and the region that we are unsure about with 0.That’s where we’ll put our marker. After that, use the watershed algorithm. The bounds of objects will have a value of -1, and our marker will be updated with the labels we provided.\nImplementing Watershed Algorithm Using OpenCV and C++ 1. Read Image if(argc \u003c 2) { std::cerr \u003c\u003c \"Error\\n\"; std::cerr \u003c\u003c \"Provide Input Image:\\n\u003cprogram\u003e \u003cinputimage\u003e\\n\"; return -1; } cv::Mat original_img = cv::imread(argv[1]); if(original_img.empty()) { std::cerr \u003c\u003c \"Error\\n\"; std::cerr \u003c\u003c \"Cannot Read Image\\n\"; return -1; } 2. Applying Filter to remove noise from the image cv::Mat shifted; cv::pyrMeanShiftFiltering(original_img,shifted,21,51); showImg(\"Mean Shifted\",shifted); Mean shift blur is a type of image edge preservation filtering algorithm that is frequently used to eliminate noise before image watershed segmentation, which can considerably improve the watershed segmentation effect.\n3. Converting the original image to grayscale and binary image cv::Mat gray_img; cv::cvtColor(original_img,gray_img,cv::COLOR_BGR2GRAY); showImg(\"GrayIMg\",gray_img); cv::Mat bin_img; cv::threshold(gray_img,bin_img,0,255,cv::THRESH_BINARY | cv::THRESH_OTSU); showImg(\"thres img\",bin_img); 4. Finding Sure Background of Image In this step we find the area in the image which is sure to us that it is image background.\nvoid getBackground(const cv::Mat\u0026 source,cv::Mat\u0026 dst) { cv::dilate(source,dst,cv::Mat::ones(3,3,CV_8U)); //Kernel 3x3 } 5. Finding Sure Foreground of Image For finding foreground of an image we use distance transform algorithm\nvoid getForeground(const cv::Mat\u0026 source,cv::Mat\u0026 dst) { cv::distanceTransform(source,dst,cv::DIST_L2,3,CV_32F); cv::normalize(dst, dst, 0, 1, cv::NORM_MINMAX); } 6. Finding Marker Before applying watershed algorithm we need markers. For this, we will use findContour() function provided in opencv to find marker in the image.\nvoid findMarker(const cv::Mat\u0026 sureBg,cv::Mat\u0026 markers, std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e\u0026 contours) { cv::findContours(sureBg,contours,cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE); // Draw the foreground markers for (size_t i = 0,size = contours.size(); i \u003c size; i++) drawContours(markers, contours, static_cast\u003cint\u003e(i), cv::Scalar(static_cast\u003cint\u003e(i)+1), -1); } 7. Applying Watershed algorithm cv::watershed(original_img,markers); cv::Mat mark; markers.convertTo(mark, CV_8U); cv::bitwise_not(mark, mark); //Convert white to black and black to white showImg(\"MARKER\",mark); References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook Code #include \u003ciostream\u003e #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003cvector\u003e void showImg(const std::string\u0026 windowName,const cv::Mat\u0026 img){ cv::imshow(windowName,img); } void getBackground(const cv::Mat\u0026 source,cv::Mat\u0026 dst) { cv::dilate(source,dst,cv::Mat::ones(3,3,CV_8U)); //Kernel 3x3 } void getForeground(const cv::Mat\u0026 source,cv::Mat\u0026 dst) { cv::distanceTransform(source,dst,cv::DIST_L2,3,CV_32F); cv::normalize(dst, dst, 0, 1, cv::NORM_MINMAX); } void findMarker(const cv::Mat\u0026 sureBg,cv::Mat\u0026 markers, std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e\u0026 contours) { cv::findContours(sureBg,contours,cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE); // Draw the foreground markers for (size_t i = 0,size = contours.size(); i \u003c size; i++) drawContours(markers, contours, static_cast\u003cint\u003e(i), cv::Scalar(static_cast\u003cint\u003e(i)+1), -1); } void getRandomColor(std::vector\u003ccv::Vec3b\u003e\u0026 colors,size_t size) { for (int i = 0; i \u003c size ; ++i) { int b = cv::theRNG().uniform(0, 256); int g = cv::theRNG().uniform(0, 256); int r = cv::theRNG().uniform(0, 256); colors.emplace_back(cv::Vec3b((uchar)b, (uchar)g, (uchar)r)); } } int main (int argc,char** argv) { if(argc \u003c 2) { std::cerr \u003c\u003c \"Error\\n\"; std::cerr \u003c\u003c \"Provide Input Image:\\n\u003cprogram\u003e \u003cinputimage\u003e\\n\"; return -1; } cv::Mat original_img = cv::imread(argv[1]); if(original_img.empty()) { std::cerr \u003c\u003c \"Error\\n\"; std::cerr \u003c\u003c \"Cannot Read Image\\n\"; return -1; } cv::Mat shifted; cv::pyrMeanShiftFiltering(original_img,shifted,21,51); showImg(\"Mean Shifted\",shifted); cv::Mat gray_img; cv::cvtColor(original_img,gray_img,cv::COLOR_BGR2GRAY); showImg(\"GrayIMg\",gray_img); cv::Mat bin_img; cv::threshold(gray_img,bin_img,0,255,cv::THRESH_BINARY | cv::THRESH_OTSU); showImg(\"thres img\",bin_img); cv::Mat sure_bg; getBackground(bin_img,sure_bg); showImg(\"Sure Background\",sure_bg); cv::Mat sure_fg; getForeground(bin_img,sure_fg); showImg(\"Sure ForeGround\",sure_fg); cv::Mat markers = cv::Mat::zeros(sure_bg.size(),CV_32S); std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e contours; findMarker(sure_bg,markers,contours); cv::circle(markers, cv::Point(5, 5), 3, cv::Scalar(255), -1); //Drawing Circle around the marker cv::watershed(original_img,markers); cv::Mat mark; markers.convertTo(mark, CV_8U); cv::bitwise_not(mark, mark); //Convert white to black and black to white showImg(\"MARKER\",mark); /* Highliting Markers in the image */ std::vector\u003ccv::Vec3b\u003e colors; getRandomColor(colors,contours.size()); // Create the result image cv::Mat dst = cv::Mat::zeros(markers.size(), CV_8UC3); // Fill labeled objects with random colors for (int i = 0; i \u003c markers.rows; i++) { for (int j = 0; j \u003c markers.cols; j++) { int index = markers.at\u003cint\u003e(i,j); if (index \u003e 0 \u0026\u0026 index \u003c= static_cast\u003cint\u003e(contours.size())) dst.at\u003ccv::Vec3b\u003e(i,j) = colors[index-1]; } } showImg(\"Final Result\",dst); cv::waitKey(0); return 0; } ","description":"In this tutorial I will be showing principle of watershed algorithm and how to implement it using opencv and c++,","tags":["programming","cpp","opencv"],"title":"OpenCV - Image Segmentation using Watershed algorithm C++","uri":"/collections/programming/cpp/opencv/opencv-watershed/"},{"content":"OpenCV - Stereo Vision and Depth Estimation using C++ Have you ever wondered how robots travel by themselves, grip various things, or avoid collisions while moving? For such purposes, stereo vision-based depth estimate is a common method. In this piece, we’ll look at traditional stereo matching and depth perception approaches.\nWhat is Stereo Vision ? Two cameras are positioned parallel to each other in the stereo vision technique. The cameras should have the same focal length for the application, with their X-axis intersecting and aligning with the baseline.\nWhy Do we need Stereo Vision ? Try this entertaining experiment: Close one eye and hold your hands in front of you, about a foot apart, with your index fingers extended. Now try to touch the tips of your index fingers by moving your hands closer together. Isn’t it harder than it appears? That is the value of in-depth knowledge.\nBy default, cameras capture images in such a way that depth information is lost. This is due to the fact that a camera converts a 3D scene into a 2D image.\nHowever, because to the geometry of the scene and the camera, if you capture two photographs of the same scene with slightly different camera locations, you’ll receive two images with a relationship between them. We can use this geometry to figure out the scene’s depth relationship with the camera. Look up Epipolar geometry for further details on how to do this.\nThis setup of two pictures slightly shifted is the same as having two cameras placed slightly apart and each camera giving us one of the two images. This is what we call stereo vision and its primary application is being able to estimate depth of a scene.\nOur eyes work in the same way, with each eye acting as a camera. Our brain is amazing enough to apply Epipolar geometry directly to these two slightly shifted pictures, allowing us to detect depth of objects in our perspective and so effortlessly touch our index fingers.\nWhat is Depth Estimation ? Depth estimate is an important step in inferring scene geometry from two-dimensional photographs. Given only a single RGB image as input, the purpose of depth estimation is to forecast the depth value of each pixel or infer depth information. This example will demonstrate how to use a convnet and simple loss functions to build a depth estimation model.\nIn Simple terms Depth Estimation is the process of figuring out the distance of any object from the camera or distance of one object from other that is weather the object is nearer or far from camera or from each other\nCalculating Depth from stereo image or camera The above diagram contains equivalent triangles. Writing their equivalent equations will yield us following result:\ndisparity =x−x′=Bf/Z\nThe distance between points in the image plane corresponding to scene points 3D and their camera centre is given by x and x′. B is the known distance between two cameras, and f is the focal length of the camera (already known). In other words, the depth of a point in a scene is inversely proportional to the distance between matching picture points and their camera centres, according to the preceding equation. We can calculate the depth of all pixels in an image using this information.\nWhat is Disparity ? If you examine the photos obtained by the mono cameras closely, you will notice that they are not identical. By integrating the two photos into a single image with 50 percent contribution from each image, the difference is easily visible. The placements of the corresponding points are different. Disparity is the term for this disparity.\nDepth is inversely proportional to the disparity.\nUsing template matching or similar methods, you may find the comparable locations in the second image. Millions of pixels make up an image obtained by high-resolution cameras. As a result, if we do it for the complete picture, it will be quite time consuming. Fortunately, our cameras have been calibrated, and the photos have been corrected. As a result, we simply need to look for the horizontal line.\nCalculating Depth Estimation of Stereo Image using OpenCV and C++ 1. Reading Stereo Image Since Stereo images consists of two image we will read both image left and right respectively:\n... if(argc \u003c 2) { std::cerr \u003c\u003c \"!!! ERROR !!! \\n\"; std::cerr \u003c\u003c \"\u003cprogram\u003e \u003cimg1\u003e \u003cimg1\u003e\\n\"; } imgL = cv::imread(argv[1]); imgR = cv::imread(argv[2]); ... 2. Calculating Disparity of two images Two calculate disparity of image we will using StereoSGBM class provided in opencv:\n... cv::Ptr\u003ccv::StereoSGBM\u003e stereo = cv::StereoSGBM::create(); ... We will also need two parameter used two compute disparity of image:\nNumber of disparity (numDisparities): Sets the range of disparity values to be searched. The overall range is from minimum disparity value to minimum disparity value + number of disparities. The following pair of images shows the disparity map calculated for two different disparity ranges. It is clearly visible that increasing the number of disparities increases the accuracy of the disparity map. stereo-\u003esetNumDisparities(numDisparity * 16); Block size(blockSize): The size of the sliding window used for block matching in a corrected stereo picture pair to find corresponding pixels. A larger window size is indicated by a higher value. Increasing this value results in more smooth disparity maps, as shown in the GIF below.\nstereo-\u003esetBlockSize(blockSize); Computing disparity of two images:\nstereo-\u003ecompute(imgL,imgR,disp); disp.convertTo(disparity,CV_8U); //converting to 8bit/pixel i.e pixel can have value from 0-255 Output In the above output blue color represent the object in nearer and red color represent the object is far.\nReferences OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook OpenCV Docs Code #include \u003ciostream\u003e #include \"opencv2/imgproc.hpp\" #include \"opencv2/highgui.hpp\" #include \u003copencv2/calib3d.hpp\u003e int numDisparity = 8; int blockSize = 5; cv::Ptr\u003ccv::StereoSGBM\u003e stereo = cv::StereoSGBM::create(); cv::Mat disp,disparity; //Disparity cv::Mat imgL; cv::Mat imgR; std::string disparity_window = \"disparity\"; static void trackbar1(int , void* ) { stereo-\u003esetNumDisparities(numDisparity * 16); numDisparity = numDisparity * 16; stereo-\u003ecompute(imgL,imgR,disp); disp.convertTo(disparity,CV_8U); cv::applyColorMap(disparity,disparity,cv::COLORMAP_JET); cv::imshow(disparity_window.c_str(),disparity); } static void trackbar2(int , void* ) { stereo-\u003esetBlockSize(blockSize); blockSize = blockSize; stereo-\u003ecompute(imgL,imgR,disp); disp.convertTo(disparity,CV_8U); cv::applyColorMap(disparity,disparity,cv::COLORMAP_JET); cv::imshow(disparity_window.c_str(),disparity); } int main(int argc,char** argv) { if(argc \u003c 2) { std::cerr \u003c\u003c \"!!! ERROR !!! \\n\"; std::cerr \u003c\u003c \"\u003cprogram\u003e \u003cimg1\u003e \u003cimg1\u003e\\n\"; } imgL = cv::imread(argv[1]); imgR = cv::imread(argv[2]); cv::namedWindow(disparity_window); cv::createTrackbar(\"numDisparities\", disparity_window.c_str(), \u0026numDisparity, 18, trackbar1); cv::createTrackbar(\"blockSize\", disparity_window.c_str(), \u0026blockSize, 50, trackbar2); cv::waitKey(0); return 0; } ","description":"Stereo Camera and OpenCV Depth Perception (disparity map for rectified stereo image pair, depth map from disparity map).","tags":["programming","cpp","opencv"],"title":"OpenCV - Stereo Vision and Depth Estimation using C++","uri":"/collections/programming/cpp/opencv/opencv-stereo-image-depth-estimation/"},{"content":"OpenCV - Understanding Hough Transform Using C++ Before we can apply the Hough transform to images, we must first learn what a Hough space is, which we shall do through an example.\nParameter Space When working with photos, we can consider the image to be a 2d matrix with x and y coordinates, with a line defined as y = mx + b.\nBut in Hough Space,instead of representing that identical line as m vs b, I can express it as m-b, which means that the characterisation of a line in image space will be a single point at m-b in Hough space.\nHowever, we have a problem: we can’t describe a vertical line with y = mx + b because the slope is infinite. As a result, we require a more efficient method of parametrization, polar coordinates (rho and theta).\nHough Space rho: describes the distance between the origin and the line. theta: specifies the angle between horizontal and vertical. What occurs when we take numerous points around a line and change into our Hough space is an essential insight.\nA single dot in image space corresponds to a curve in Hough space, with the exception that points within a line in image space are represented by several curves with a single touchpoint.\nFinding the spots where a set of curves intersects will be our goal.\nWhat is Hough Transform ? In an image, the Hough transform is a feature extraction method for detecting simple objects like circles and lines.\nA “simple” shape is one that has only a few parameters to represent it. A line, for example, can be represented by two parameters (slope and intercept), while a circle can be represented by three parameters (centre coordinates and radius) (x, y, r). When it comes to finding such shapes in a picture, the Hough transform excels.\nThe Hough transform’s key advantage is that it is unaffected by occlusion.\nLet’s look at an example of how the Hough transform works.\nHow Does Hough Transform Works ? A line in image space can be described with two variables, as you may know. Consider the following scenario:\nParameters in the Cartesian coordinate system: (m,b).\nParameters in the Polar coordinate system: (r,θ)\nWe shall express lines in the Polar system for Hough Transforms. Hence, a line equation can be represented as:\nr=xcosθ+ysinθ\nIn general, we can define the family of lines that passes through each point (x0,y0) as follows: r0=x0⋅cosθ+y0⋅sinθ\nEach pair (r,θ) denotes one of the lines that passes by (x0,y0).\nA sinusoid is obtained by plotting the family of lines that passes through a given (x0,y0). For instance, for x0=8 and y0=6 we get the following graphic (on a plane θ - r):\nOnly points with r\u003e0 and 0\u003cθ\u003c2π. are considered.\nWe may repeat the procedure above for all of the points in an image. When the curves of two different points intersect in the plane - r, both points are part of the same line. For example, if we continue with the previous example and plot two more points: x1=4, y1=9 and x2=12, y2=3, we get:\nThe three plots come together at a single location (0.925,9.6); these coordinates are the parameters (,r), or the line that connects (x0,y0), (x1,y1), and (x2,y2).\nWhat does all of the foregoing imply? It means that the number of intersections between curves can be used to detect a line in general. The more curves that connect, the more points there are on the line represented by that intersection. In general, we can set a minimum number of intersections required to detect a line as a threshold.\nThe Hough Line Transform accomplishes this. It maintains note of where the curves of each point in the image cross. If the number of intersections exceeds a certain threshold, it is declared as a line with the intersection point’s parameters (θ,r).\nHough Transform Using OpenCV and C++ First we read the image and convert it into grey scale form. ... //Reading Image if(argc \u003c= 1) { std::cerr \u003c\u003c \"!!! ERROR !!!\\n\"; std::cerr \u003c\u003c \"Please provide Image Path\\n\"; std::cerr \u003c\u003c \"\u003cprogram\u003e \u003cinput_image\u003e\\n\"; return -1; } cv::Mat src = cv::imread(argv[1]); cv::Mat img(src); cv::cvtColor(img,img,cv::COLOR_BGR2GRAY); showImg(img,\"input Image\"); if(img.empty()){ std::cerr \u003c\u003c \"!!! ERROR !!!\\n\"; std::cerr \u003c\u003c \"Error while Opening the Image\\n\"; return -1; } ... After Converting into gray scale we reduced the noise from the image using any blur function which in provided in OpenCV and then we apply Canny Edge Detection algorithm to find border in the image .... auto cannyEdgeDetection = [](cv::Mat\u0026 input, cv::Mat\u0026 output,double minThresh,double maxThresh) { cv::blur(input,output,cv::Size(5,5)); cv::Canny(input,output,minThresh,maxThresh); }; .... After getting border from the image we apply Hough Transform. Note: In the example below I have used Probabilistic Hough Line Transform Function which is a more efficient implementation of Hough Line transform then Standar Hough Line Transform.\nhoughLinesP function takes the following argument:\nedges: Output of the edge detector. lines: A vector to store the coordinates of the start and end of the line. rho: The resolution parameter \\rho in pixels. theta: The resolution of the parameter \\theta in radians. threshold: The minimum number of intersecting points to detect a line. .... std::vector\u003ccv::Vec4i\u003e* houghLineP(cv::Mat\u0026 canny_img,double thresh){ auto linesP = new std::vector\u003ccv::Vec4i\u003e; cv::HoughLinesP(canny_img,*linesP,1,CV_PI/180,thresh,10,10); return linesP; } .... Line Detection Output References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook live code stream OpenCV Docs Code #include \u003ciostream\u003e #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003calgorithm\u003e std::vector\u003ccv::Vec4i\u003e* houghLineP(cv::Mat\u0026 canny_img,double thresh){ auto linesP = new std::vector\u003ccv::Vec4i\u003e; cv::HoughLinesP(canny_img,*linesP,1,CV_PI/180,thresh,10,10); return linesP; } int main(int argc,char** argv) { // Defining Functions auto showImg = [](cv::Mat\u0026 img,const std::string\u0026 win_name){ cv::imshow(win_name,img); }; auto cannyEdgeDetection = [](cv::Mat\u0026 input, cv::Mat\u0026 output, double minThresh,double maxThresh) { cv::blur(input,output,cv::Size(5,5)); cv::Canny(input,output,minThresh,maxThresh); }; //Reading Image if(argc \u003c= 1) { std::cerr \u003c\u003c \"!!! ERROR !!!\\n\"; std::cerr \u003c\u003c \"Please provide Image Path\\n\"; std::cerr \u003c\u003c \"\u003cprogram\u003e \u003cinput_image\u003e\\n\"; return -1; } cv::Mat src = cv::imread(argv[1]); cv::Mat img(src); cv::cvtColor(img,img,cv::COLOR_BGR2GRAY); showImg(img,\"input Image\"); if(img.empty()){ std::cerr \u003c\u003c \"!!! ERROR !!!\\n\"; std::cerr \u003c\u003c \"Error while Opening the Image\\n\"; return -1; } // Finding Line Using Hough Detection cv::Mat dst; cannyEdgeDetection(img,dst,180,250); // Tweak Argument as per need showImg(dst,\"Canny Ouput\"); auto *phtld = houghLineP(dst,120); //Probablistic Hough Transform cv::Scalar blue(255,0,0); //for Probablistic Hough Transform std::for_each(phtld-\u003ecbegin(),phtld-\u003ecend(),[\u0026src,\u0026blue](const auto \u0026line){ // Drawing Line auto l = line; cv::line(src,cv::Point(l[0],l[1]),cv::Point(l[2],l[3]),blue,2,cv::LINE_AA); }); showImg(src,\"Hough Lines\"); cv::waitKey(); // Cleaning delete phtld; return 0; } ","description":"In this article we will cover in-depth knowledge of what is Hough Transform, How Does it works and also how to implement it in C++","tags":["programming","cpp","opencv"],"title":"OpenCV - Understanding Hough Transform Using C++","uri":"/collections/programming/cpp/opencv/opencv-hough-line-detection/"},{"content":"OpenCV - Understanding Convex Hull using C++ Finding and analysing the forms present in an image is one technique to solving most computer vision problems, and getting the contour is one of them. I would describe a contour to a newbie as “just a curve joining all the points lying on the shape’s perimeter.”\nLet’s say I have the image of my hand below, and the contour of the hand is represented by the green line. The red dots represent the spots that we link to form the contour curve.\nMy advanced mathematics lesson, where they introduced contours, is still fresh in my mind. However, it was difficult to connect with the importance of this topic because the teacher never highlighted any real-world applications of the contour. And today is the day when I discover just how crucial they are in computer vision.\nWhat is Convex Hull? An item that has no interior angles higher than 180 degrees is said to be convex. Non-Convex or Concave refers to a shape that is not convex. The exterior or shape of an object is referred to as the hull.\nAs a result, a shape’s or a group of points’ Convex Hull is a tightly fitting convex boundary around the points or the shape.\nA convex hull of an object, in layman’s terms, is the smallest boundary that can completely encircle or wrap the thing (or contour of that object).\nThe convex hull can be found using a variety of approaches. The following are some of the most frequent algorithms and their associated temporal complexities. The number of input points is n, while the number of points on the hull is h.\nSklansky (1982) — O(nlogn) ( OpenCV uses this algorithm) Gift wrapping, a.k.a. Jarvis march — O(nh) Graham scan — O(nlogn) Chan’s algorithm — O(nlogh) Implementing Convex Hull Using OpenCV 1. Read the Input Image src = cv::imread( argv[1],cv::IMREAD_GRAYSCALE); 2. Convert Input Image into Binary Form Convert Image to grayscale (Which we have done while reading image). Remove Noise from the image by applying any blurring algorithm (Here I have used Gaussain Blur) Then Thershold the image to make it into Binary Form. ... cv::GaussianBlur( src, src, cv::Size(3,3),0 ); //Applying GaussianBlur of Kernel(3x3) ShowImg(\"Image After Applying Blur\",src); const int max_thresh = 255; const std::string source_window = \"Canny \"; cv::createTrackbar( \"Canny thresh:\", source_window, \u0026thresh, max_thresh, thresh_callback ); thresh_callback( 0, 0 ); cv::waitKey(); return 0; } void thresh_callback(int, void* ) { cv::Mat canny_output; cv::Canny( src, canny_output, thresh, thresh*2 ); .... Finding Contour Next, we use OpenCV’s findContour function to find the contour around each image. If you’re a newbie, you might be wondering why we didn’t just utilise edge detection. Edge detection would simply have provided us with the position of the edges.\nHowever, we’re curious about how edges are connected to one another. findContour finds the connections and returns a list of the points that make up a contour.\n... std::vector\u003cstd::vector\u003ccv::Point\u003e \u003e contours; cv::findContours( canny_output, contours, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE ); ... Finding Convex Hull using convexHull function We can now get the Convex Hull for each of the contours now that we’ve got the contours. The convexHull function can be used to do this. ... std::vector\u003cstd::vector\u003ccv::Point\u003e \u003ehull( contours.size() ); for( size_t i = 0; i \u003c contours.size(); i++ ) { cv::convexHull( contours[i], hull[i] ); } ... Drawing Convex Hull The last step is to visualise the convex hulls we’ve discovered so far. Because a convex hull is essentially a contour, we can use OpenCV’s drawContours function to create one. ... cv::Scalar contours_color = cv::Scalar(255,0,0); //Blue Color cv::Scalar hull_color = cv::Scalar(0,0,255); //Red Color for( size_t i = 0; i\u003c contours.size(); i++ ) { cv::Scalar color = cv::Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) ); cv::drawContours( drawing, contours, (int)i, contours_color ); cv::drawContours( drawing, hull, (int)i, hull_color ); } ... Output Applications From a set of points, create a boundary Regular readers of our blog would recall that we utilised convexHull in our face swap application previously. We used the convex hull to find the face’s boundary based on the facial landmarks discovered by Dlib.\nWe can retrieve feature points information instead of contour information in a variety of additional applications. We recover a grayscale depth map that is a collection of points in several active illumination systems, such as Kinect. The convex hull of these points can be used to find the boundaries of an object in the scene.\nAvoidance of Collisions Consider an automobile to be a collection of points, with the polygon (minimum set) holding all of them. If the convex hull can avoid the obstructions, the automobile should be able to as well.\nFinding the intersection of random contours is substantially more computationally intensive than finding the collision of two convex polygons. As a result, a convex hull is preferable for collision detection and avoidance.\nReferences OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook OpenCV Docs Code #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e cv::Mat src; int thresh = 100; void thresh_callback(int, void* ); void ErrorMsg(std::string msg) { std::cout \u003c\u003c \"!! Error !! \\n\" ; std::cout \u003c\u003c msg \u003c\u003c std::endl; } void ShowImg(const std::string windowName,cv::Mat\u0026 img) { cv::namedWindow( windowName ); cv::imshow( windowName, img ); } int main( int argc, char** argv ) { if(argc \u003c 1) { ErrorMsg(\"Please Provide Input Image\\n\"); } src = cv::imread( argv[1],cv::IMREAD_GRAYSCALE); if( src.empty() ) { ErrorMsg(\"Could not open or find the image!\\n\"); return -1; } cv::GaussianBlur( src, src, cv::Size(3,3),0 ); //Applying GaussianBlur of Kernel(3x3) ShowImg(\"Image After Applying Blur\",src); const int max_thresh = 255; const std::string source_window = \"Canny \"; cv::createTrackbar( \"Canny thresh:\", source_window, \u0026thresh, max_thresh, thresh_callback ); thresh_callback( 0, 0 ); cv::waitKey(); return 0; } void thresh_callback(int, void* ) { cv::Mat canny_output; cv::Canny( src, canny_output, thresh, thresh*2 ); std::vector\u003cstd::vector\u003ccv::Point\u003e \u003e contours; cv::findContours( canny_output, contours, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE ); std::vector\u003cstd::vector\u003ccv::Point\u003e \u003ehull( contours.size() ); for( size_t i = 0; i \u003c contours.size(); i++ ) { cv::convexHull( contours[i], hull[i] ); } cv::Mat drawing = cv::Mat::zeros( canny_output.size(), CV_8UC3 ); cv::Scalar contours_color = cv::Scalar(255,0,0); //Blue Color cv::Scalar hull_color = cv::Scalar(0,0,255); //Red Color for( size_t i = 0; i\u003c contours.size(); i++ ) { cv::Scalar color = cv::Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) ); cv::drawContours( drawing, contours, (int)i, contours_color ); cv::drawContours( drawing, hull, (int)i, hull_color ); } ShowImg(\"Hull: \", drawing); } ","description":"A tutorial on determining a shape's or a group of points' Convex Hull.  The OpenCV code implementation in C++ and Python is shared.","tags":["programming","cpp","opencv"],"title":"OpenCV - Understanding Convex Hull using C++","uri":"/collections/programming/cpp/opencv/opencv-convexhull/"},{"content":"OpenCV - Contour Detection Using C++ Contour detection is an important task in computer vision, not only because of the obvious element of detecting contours of subjects in an image or video frame, but also because of the derivative processes associated with identifying contours.\nWe can recognise the edges of objects and simply pinpoint them in a picture using contour detection. Many intriguing applications, such as image-foreground extraction, simple-image segmentation, detection, and identification, use it as a starting point.\nSo, using OpenCV, let’s learn about contours and contour detection, and see how they may be utilised to create a variety of applications.\nWhat is Contour ? Contours are just a curve that connects all of the continuous points (along the border) that are the same colour or intensity. The contours are a useful tool for object detection and recognition as well as form analysis.\nIn this section we will learn how to find and draw contours using functions findContour() and drawContour().Let’s talk about the stages involved in detecting contours now that you’ve learned about them.\nStep involved for Finding and Drawing Contour Reading and Converting Image to GrayScale:\nThe image should be read and converted to grayscale format. The conversion to grayscale is crucial because it prepares the image for the following stage. The image must be converted to a single channel grayscale image for thresholding, which is required for the contour detection technique to perform effectively.\ninput = cv::imread(argv[1]); showImg(input,\"Input Image\"); cv::cvtColor(input,input_grey,cv::COLOR_BGR2GRAY); Applying Gaussian Blur:\ncv::GaussianBlur(input_grey,blur_img,kernel,0); // Applying Gaussian Blur with kernel 5x5 showImg(blur_img,\"Blur Image\"); Apply Binary Thresholding\nAlways perform binary thresholding or Canny edge detection to the grayscale image before looking for contours. We’ll use binary thresholding in this case.\nThis turns the image to black and white, highlighting the points of interest to make the contour-detection algorithm’s job easier. Thresholding makes the image’s object’s border totally white, with the same intensity across all pixels. From these white pixels, the programme can now discern the object’s edges.\nNote that black pixels with a value of 0 are considered background pixels and are disregarded.\nOne question may arise at this moment. What if we used single channels instead of grayscale (thresholded) images, such as R (red), G (green), or B (blue)? The contour detection algorithm will not perform well in this scenario. As previously stated, the algorithm detects contours by looking for borders and pixels of similar intensity. This information is significantly better provided by a binary image than by a single (RGB) colour channel image.\ncv::threshold(blur_img,thresh_img,200,255,cv::THRESH_BINARY ); showImg(thresh_img,\"Thresh Image\"); A value of 255 will be assigned to any pixel with a value greater than 200 (white).In the resultant image, all remaining pixels will be set to 0. (black). You can play around with the threshold value of 200 because it is a configurable option.\nDrawing Contours Begin by calling the findContours() method. There are three arguments that must be provided, as indicated below. Please see the documentation page for more information on optional arguments.\nimage: The binary input picture obtained in the preceding step is called image.\ncontour-retrieval mode: The algorithm will retrieve all potential contours from the binary picture if this is set to RETR_TREE. More contour retrieval modes are available. More information on these alternatives can be found here.\nmethod: This is where the contour-approximation approach is defined. We’ll use CHAIN_APPROX_NONE in this case. We’ll use this method to store ALL contour points, even though it’s a little slower than CHAIN_APPROX_SIMPLE.\nIt’s important to note that mode relates to the type of contours that will be retrieved, whilst method refers to the points that will be saved within a contour. Both will be discussed in greater depth further down.\nOn the same image, the outcomes of several methods are easily visualised and understood.\nAs a result, in the code samples below, we make a clone of the original image before demonstrating the methods (not wanting to edit the original).\nThen, to overlay the contours over the RGB image, use the drawContours() function. There are four necessary arguments and several optional arguments for this function. The first four arguments are mandatory.\nPlease see the documentation page for more information on the optional arguments.\nimage: This is the RGB input image on which the contour should be drawn. contours: Returns the contours obtained by calling findContours(). contourIdx: In the obtained contours, the pixel coordinates of the contour points are reported. You can provide the index position from this list with this input, indicating exactly the contour point you want to draw. If you enter a negative value, all of the contour points will be drawn. colour: The colour of the contour points you want to draw is indicated here. The points are drawn in green. The thickness of contour points is measured in this parameter. std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e contours; std::vector\u003ccv::Vec4i\u003e hierarchy; cv::findContours(thresh_img,contours,hierarchy,cv::RETR_TREE,cv::CHAIN_APPROX_SIMPLE); cv::Scalar red(0,0,255); cv::drawContours(input,contours,-1,red,2); showImg(input,\"Detected Contour\"); Output Contours Hierarchies The parent-child relationship between contours is represented by hierarchies. You’ll learn how each contour-retrieval option impacts image contour detection and generates hierarchical results.\nParent-Child Relationship Contour-detection algorithms may detect the following objects in an image:\nSingle things strewn about in an image (as in the first example), or objects and forms nestled together (as in the second example). We can fairly assume that the outer form is a parent of the inner shape in most circumstances where a shape contains other shapes.\nContour Relationship Representation The findContours() function gives two outputs: the contours list and the hierarchy, as you can see. Let’s take a closer look at the contour hierarchy output.\nThe contour hierarchy is represented as an array, which is made up of four different arrays. It’s written like this:\n[Next, Previous, First Child, Parent]\nSo, what exactly do all of these numbers mean?\nNext: Indicates the next contour in a picture that is on the same hierarchical level as the previous one. Previous: At the same hierarchical level, this denotes the previous contour. This means that the Previous value of contour 1 will always be -1. First_Child: The first child contour of the contour we’re looking at right now. Parent: Indicates the current contour’s parent contour’s index position. Different Contour Retrieval Techniques We’ve only used one retrieval approach, RETR_TREE, to discover and draw contours thus far, but OpenCV also includes three other contour retrieval techniques: RETR_LIST, RETR_EXTERNAL, and RETR_CCOMP.\nRETR_LIST There is no parent-child link between the extracted contours when using the RETR LIST contour retrieval method. As a result, the First Child and Parent index position values for all identified contour areas are always -1.\nRETR_EXTERNAL The contour retrieval method RETR EXTERNAL is a fascinating one. It only recognises parent contours and ignores child contours.\nRETR_CCOMP RETR_CCOMP, unlike RETR_EXTERNAL, recovers all of the contours in a picture. It also creates a two-level hierarchy for all of the forms or objects in the image.\nThis translates to:\nLevel 1 hierarchy will be applied to all outside outlines.\nLevel 2 hierarchy will be applied to all inner shapes.\nRETR_TREE RETR_TREE, like RETR_CCOMP, retrieves all of the contours. It also generates a whole hierarchy, with levels that aren’t limited to one or two. Each contour can have its own hierarchy, which is determined by the level it is on and the parent-child relationship it has.\nReferences Learning OpenCV 3 with Python Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://docs.opencv.org/3.4/da/d5c/tutorial_canny_detector.html Code #include \u003ciostream\u003e #include \"opencv2/imgproc.hpp\" #include \"opencv2/highgui.hpp\" #include \u003cvector\u003e int ErrorMsg(const std::string\u0026 msg) { std::cerr \u003c\u003c \"\\n !!! Error !!!\\n \" \u003c\u003c msg \u003c\u003c \"\\n\"; return -1; } void showImg(const cv::Mat\u0026 img, const std::string\u0026 name) { cv::namedWindow(name.c_str()); cv::imshow(name.c_str(),img); } void detect_contour(cv::Mat\u0026 thresh_img,cv::Mat input) { std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e contours; std::vector\u003ccv::Vec4i\u003e hierarchy; cv::findContours(thresh_img,contours,hierarchy,cv::RETR_TREE,cv::CHAIN_APPROX_SIMPLE); cv::Scalar red(0,0,255); cv::drawContours(input,contours,-1,red,2); showImg(input,\"Detected Contour\"); } int main ( int argc,char** argv) { /** Variable Declaration **/ int threshold = 100; cv::Mat input,input_grey,blur_img,thresh_img; cv::Size kernel(3,3); /***/ if (argc \u003c 2 ) return ErrorMsg(\"Specify Image in command line\"); input = cv::imread(argv[1]); showImg(input,\"Input Image\"); cv::cvtColor(input,input_grey,cv::COLOR_BGR2GRAY); if(input.empty()) return ErrorMsg(\"Could Not Open Image\"); cv::GaussianBlur(input_grey,blur_img,kernel,0); showImg(blur_img,\"Blur Image\"); cv::threshold(blur_img,thresh_img,200,255,cv::THRESH_BINARY ); showImg(thresh_img,\"Thresh Image\"); detect_contour(thresh_img,input); cv::waitKey(0); return 0; } ","description":"Learn how to use OpenCV to detect contours.We will cover not just the theory, but also complete hands-on coding in C++ for first-hand experience.","tags":["programming","cpp","opencv"],"title":"OpenCV - Contour Detection Using C++","uri":"/collections/programming/cpp/opencv/contour-detection/"},{"content":"OpenCV - Understanding Canny Edge Detection Using C++ Edge detection is a type of image processing that identifies the borders (edges) of objects or regions inside a picture. The edges of an image are one of the most important aspects of it. Through an image’s edges, we can learn about its underlying structure. As a result, edge detection is widely used in applications in computer vision processing pipelines.\nHow to Identify Edges ? Sudden fluctuations in pixel intensity characterise edges. We need to check for similar changes in surrounding pixels to find edges. Come learn how to use Canny Edge Detection, edge-detection algorithm available in OpenCV. We’ll go over the theory and illustrate how to apply each in OpenCV.\nCanny Edge Detection John F. Canny invented the Canny Edge detector in 1986. The Canny algorithm, often known as the best detector, seeks to meet three basic criteria:\nLow error rate: This refers to the ability to detect only existing edges.\nGood localization: The distance between detected edge pixels and real edge pixels must be kept to a minimum.\nOnly one detector response per edge is required for a minimal response.\nFor extracting edges from an image, the algorithm uses a three-stage method. Then there’s image blurring, which is an essential preprocessing step for reducing noise. This results in a four-stage procedure, which contains the following steps:\nNoise Reduction\nCalculating the Image’s Intensity Gradient\nSuppression of False Edges\nHysteresis Thresholding\nNoise Reduction Because noisy edges are frequently caused by raw picture pixels, it is critical to decrease noise before computing edges. A Gaussian blur filter is employed in Canny Edge Detection to essentially remove or minimise superfluous detail that could contribute to undesired edges.\nTake a look at the the two photographs below; the image on the right has been blurred with Gaussian blur. As you can see, it’s little blurred, but there’s still enough detail to compute edges from. Calculating Intensity After the image has been smoothed (blurred), it is filtered horizontally and vertically with a Sobel kernel. The intensity gradient magnitude (G) and direction (Θ) for each pixel are calculated using the results of these filtering operations, as shown below.\nAfter that, the gradient is rounded to the nearest 45 degree angle. The result of this combined processing phase is shown in the picture below (right).\nRemoving False Edge The method in this phase uses a technique called non-maximum suppression of edges to filter out undesired pixels after decreasing noise and computing the intensity gradient (which may not actually constitute an edge). Each pixel is compared to its neighbours in both the positive and negative gradient directions to achieve this. If the current pixel’s gradient magnitude is greater than that of its neighbours, it is left alone. Otherwise, the current pixel’s magnitude is set to zero. An example can be seen in the image below. As you can see, the tiger’s fur has a lot of ’edges’ that have been greatly reduced.\nHysteresis Thresholding The gradient magnitudes are compared with two threshold values, one smaller than the other, in the last phase of Canny Edge Detection.\nThose pixels are associated with strong edges and are included in the final edge map if the gradient magnitude value is greater than the larger threshold value.\nThe pixels are suppressed and excluded from the final edge map if the gradient magnitude values are less than the smaller threshold value.\nAll remaining pixels with gradient magnitudes in the middle of these two thresholds are labelled as ‘weak’ edges (i.e. they become candidates for being included in the final edge map).\nThe ‘weak’ pixels are included in the final edge map if they are connected to those associated with strong edges.\nReferences Learning OpenCV 3 with Python Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://docs.opencv.org/3.4/da/d5c/tutorial_canny_detector.html Code Explanation Declaring Variable:\nconst std::string window = \"Canny Edge Detection\"; //Window Name int lowThreshold = 0; const int maxThreshold = 100; const int ratio = 3; const int kernel_size = 3; cv::Mat input,output; //input and output images; cv::Mat detected_edges; //Edge Detected image; Loads the source image:\nif (argc \u003c 2 ) return ErrorMsg(\"Specify Image in command line\"); input = cv::imread(argv[1],cv::IMREAD_GRAYSCALE); if(input.empty()) return ErrorMsg(\"Could Not Open Image\"); Now let’s check CannyEdgeDetection() step by step:\nFirst we blur the image using GaussianBlur: cv::GaussianBlur(input,detected_edges,cv::Size(5,5),0);\nThe we appy cv::Canny() function:\ncv::Canny(detected_edges,detected_edges,lowThreshold,lowThreshold * ratio,kernel_size);\nthe following are the arguments:\ndetected_edges: grayscale source image detected_edges: The detector’s output (can be the same as the input) lowThreshold: The value input by the user when he or she moves the Trackbar. maxThreshold: Three times the lower threshold (as per Canny’s recommendation) is set in the programme. kernel size: i.e. 3 A dst picture is filled with zeros (meaning the image is completely black). output = cv::Scalar::all(0);\nFinally, we’ll use the cv::Mat::copy function. Only the parts of the image that have been identified as edges should be mapped (on a black background). cv::Mat::copy To copy the src picture to the destination (dst). However, it will only copy pixels with non-zero values in non-zero locations. Because the Canny detector’s output is edge contours on a black background, the dst will be black everywhere except the detected edges.\nResult Code #include \u003ciostream\u003e #include \"opencv2/imgproc.hpp\" #include \"opencv2/highgui.hpp\" int ErrorMsg(const std::string\u0026 msg) { std::cerr \u003c\u003c \"\\n !!! Error !!!\\n \" \u003c\u003c msg \u003c\u003c \"\\n\"; return -1; } /** Variable Declaration **/ const std::string window = \"Canny Edge Detection\"; //Window Name int lowThreshold = 0; const int maxThreshold = 100; const int ratio = 3; const int kernel_size = 3; cv::Mat input,output; //input and output images; cv::Mat detected_edges; //Edge Detected image; /***/ static void CannyEdgeDetection(int,void*) { cv::GaussianBlur(input,detected_edges,cv::Size(5,5),0); // Applying Gaussian Blur with kernel 5x5 cv::Canny(detected_edges,detected_edges,lowThreshold,lowThreshold * ratio,kernel_size); output = cv::Scalar::all(0); input.copyTo(output,detected_edges); cv::imshow(window,output); } int main ( int argc,char** argv) { if (argc \u003c 2 ) return ErrorMsg(\"Specify Image in command line\"); input = cv::imread(argv[1],cv::IMREAD_GRAYSCALE); if(input.empty()) return ErrorMsg(\"Could Not Open Image\"); output.create(input.size(),input.type()); cv::namedWindow(window,cv::WINDOW_AUTOSIZE); cv::createTrackbar(\"Threshold: \",window,\u0026lowThreshold,maxThreshold,CannyEdgeDetection); CannyEdgeDetection(0,0); cv::waitKey(0); return 0; } ","description":"Learn how to use OpenCV to detect edges.In OpenCV, experiment with various edge detection algorithms such as Canny.","tags":["programming","cpp","opencv"],"title":"OpenCV - Understanding Canny Edge Detection Using C++","uri":"/collections/programming/cpp/opencv/canny-edge-detection/"},{"content":"OpenCV - Understanding Low Pass And High Pass Filter using C++ In the past post, we took in a few basic insights regarding the Fourier change and why it merits learning. We additionally told the best way to change a picture into its recurrence area. I suggest you read about it here in the event that you haven’t. In this part, we would zero in on sifting in the recurrence space. We would see the impacts of applying a low and high pass filter\n1. Low Pass Filter Let’s start with an example by looking at the spectrum of a real image on the right side of the page.\nYou can see this incredibly white dazzling shiny item in the middle of the magnitude spectrum above. You can see that the centre has a lot of power. With that in mind, you might as well pay attention as you go away from the bright shiny thing and toward the edges. It begins to fade, or to put it another way, it darkens.\nWhat if we wanted to get rid of all low-frequency content? It’s where the white dot is on the power spectrum. So, what should we do? First, we must create a power spectrum from the image. Second, after that, we must delete everything associated with the lower frequency. To put it another way, zero out all the numbers surrounding the brighter section of the magnitude spectrum. There would, without a doubt, be no high-frequency components.\nWe’ll recreate the image for the sake of illustration, which will entail using the inverse Discrete Fourier transform. As a result, the image is reduced to a series of unsightly lines. Ringing is a common term for this. We’ve included some code examples below to show you how to do it yourself.\n#include \u003copencv2/highgui.hpp\u003e #include \u003copencv2/imgcodecs.hpp\u003e #include \u003copencv2/imgproc.hpp\u003e #include \u003ciostream\u003e void showImg(cv::Mat\u0026 img,const std::string\u0026 name) { cv::namedWindow(name.c_str()); cv::imshow(name.c_str(),img); } void expand_img_to_optimal(cv::Mat\u0026 padded,cv::Mat\u0026 img); cv::Mat fourier_transform(cv::Mat\u0026 ); void lowpassFilter( cv::Mat\u0026 dft_filter,int distance); void crop_and_rearrange(cv::Mat\u0026 magI); void showMagnitudeSpectrum(cv::Mat\u0026,const std::string\u0026 ); int main(int argc,char** argv) { cv::Mat input_img,fourier_img; input_img = cv::imread(argv[1],cv::IMREAD_GRAYSCALE); if(input_img.empty()) { fprintf(stderr,\"Could not Open image\\n\\n\"); return -1; } showImg(input_img,\"Input Image\"); cv::Mat complexI = fourier_transform(input_img); showMagnitudeSpectrum(complexI,\"Input Image Magnitude Spectrum\"); cv::Mat filter = complexI.clone(); lowpassFilter(filter,30); //Our Low Pass Filter of Radius 30 crop_and_rearrange(complexI); cv::mulSpectrums(complexI,filter,complexI,0); //Multiplying original image with filter image to get final image crop_and_rearrange(complexI); showMagnitudeSpectrum(complexI,\"Filter Image Magnitude Spectrum\"); cv::Mat planes[2],imgOutput; cv::idft(complexI,complexI); //Reversing dft process to get our final image cv::split(complexI,planes); cv::normalize(planes[0], imgOutput, 0, 1, cv::NORM_MINMAX); showImg(imgOutput,\"Low Pass Filter\"); cv::waitKey(); cv::destroyAllWindows(); return 0; } cv::Mat fourier_transform(cv::Mat\u0026 img) { cv::Mat padded; expand_img_to_optimal(padded,img); // Since the result of Fourier Transformation is in complex form we make two planes to hold real and imaginary value cv::Mat planes[] = {cv::Mat_\u003cfloat\u003e(padded),cv::Mat::zeros(padded.size(),CV_32F)}; cv::Mat complexI; cv::merge(planes,2,complexI); cv::dft(complexI,complexI,cv::DFT_COMPLEX_OUTPUT); // Fourier Transform return complexI; } void expand_img_to_optimal(cv::Mat\u0026 padded,cv::Mat\u0026 img) { int row = cv::getOptimalDFTSize(img.rows); int col = cv::getOptimalDFTSize(img.cols); cv::copyMakeBorder(img,padded,0,row - img.rows,0,col - img.cols,cv::BORDER_CONSTANT,cv::Scalar::all(0)); } void lowpassFilter( cv::Mat\u0026 dft_filter,int distance) { cv::Mat tmp = cv::Mat(dft_filter.rows,dft_filter.cols,CV_32F); cv::Point center = cv::Point(dft_filter.rows/2,dft_filter.cols/2); double radius; for(int i = 0; i \u003c dft_filter.rows;++i) { for(int j=0; j \u003c dft_filter.cols;++j) { radius = (double)(sqrt(pow((i-center.x),2.0)+pow((j-center.y),2.0))); if(radius \u003c distance) tmp.at\u003cfloat\u003e(i,j) = 0.0; //if point is in the radius make it zero else tmp.at\u003cfloat\u003e(i,j) = 1.0;// Else make it one } } cv::Mat toMerge[] = {tmp,tmp}; cv::merge(toMerge,2,dft_filter); //since we are dealing with Two Channel image which is greyscale } void crop_and_rearrange(cv::Mat\u0026 magI) { magI = magI(cv::Rect(0, 0, magI.cols \u0026 -2, magI.rows \u0026 -2)); int cx = magI.cols/2; int cy = magI.rows/2; cv::Mat q0(magI, cv::Rect(0, 0, cx, cy)); cv::Mat q1(magI, cv::Rect(cx, 0, cx, cy)); // Top-Right cv::Mat q2(magI, cv::Rect(0, cy, cx, cy)); // Bottom-Left cv::Mat q3(magI, cv::Rect(cx, cy, cx, cy)); // Bottom-Right cv::Mat tmp; // swap quadrants (Top-Left with Bottom-Right) q0.copyTo(tmp); q3.copyTo(q0); tmp.copyTo(q3); q1.copyTo(tmp); // swap quadrant (Top-Right with Bottom-Left) q2.copyTo(q1); tmp.copyTo(q2); } void showMagnitudeSpectrum(cv::Mat\u0026 complexI,const std::string\u0026 name) { cv::Mat p[2]; cv::split(complexI,p); cv::magnitude(p[0],p[1],p[0]); cv::Mat magI = p[0]; magI += cv::Scalar::all(1); cv::log(magI,magI); crop_and_rearrange(magI); cv::normalize(magI, magI, 0, 1, cv::NORM_MINMAX); showImg(magI,name.c_str()); } 2. High Pass Filter We’d like to do something a little different to liven things up a little. We’re going to do the polar opposite of keeping low-frequency stuff. This means retaining high-frequency content while eliminating all low-frequency content. In other words, zero out the white dot in the centre of the magnitude spectrum while leaving the rest alone.\nWhat we’re going to see is a better question now. Before we provide the solution. Take 5 seconds to think about what our outcome will be. Are you finished? Okay. We are left with an edge image after removing the centre region and reconstructing it. If you think about it, the high-frequency components show us where the image’s edges are. Also included is the code that was used to create these images.\n#include \u003copencv2/highgui.hpp\u003e #include \u003copencv2/imgcodecs.hpp\u003e #include \u003copencv2/imgproc.hpp\u003e #include \u003ciostream\u003e void showImg(cv::Mat\u0026 img,const std::string\u0026 name) { cv::namedWindow(name.c_str()); cv::imshow(name.c_str(),img); } void expand_img_to_optimal(cv::Mat\u0026 padded,cv::Mat\u0026 img); cv::Mat fourier_transform(cv::Mat\u0026 ); void lowpassFilter( cv::Mat\u0026 dft_filter,int distance); void crop_and_rearrange(cv::Mat\u0026 magI); void showMagnitudeSpectrum(cv::Mat\u0026,const std::string\u0026 ); int main(int argc,char** argv) { cv::Mat input_img,fourier_img; input_img = cv::imread(argv[1],cv::IMREAD_GRAYSCALE); if(input_img.empty()) { fprintf(stderr,\"Could not Open image\\n\\n\"); return -1; } showImg(input_img,\"Input Image\"); cv::Mat complexI = fourier_transform(input_img); showMagnitudeSpectrum(complexI,\"Input Image Magnitude Spectrum\"); cv::Mat filter = complexI.clone(); lowpassFilter(filter,30); //Our Low Pass Filter of Radius 30 crop_and_rearrange(complexI); cv::mulSpectrums(complexI,filter,complexI,0); //Multiplying original image with filter image to get final image crop_and_rearrange(complexI); showMagnitudeSpectrum(complexI,\"Filter Image Magnitude Spectrum\"); cv::Mat planes[2],imgOutput; cv::idft(complexI,complexI); //Reversing dft process to get our final image cv::split(complexI,planes); cv::normalize(planes[0], imgOutput, 0, 1, cv::NORM_MINMAX); showImg(imgOutput,\"Low Pass Filter\"); cv::waitKey(); cv::destroyAllWindows(); return 0; } cv::Mat fourier_transform(cv::Mat\u0026 img) { cv::Mat padded; expand_img_to_optimal(padded,img); // Since the result of Fourier Transformation is in complex form we make two planes to hold real and imaginary value cv::Mat planes[] = {cv::Mat_\u003cfloat\u003e(padded),cv::Mat::zeros(padded.size(),CV_32F)}; cv::Mat complexI; cv::merge(planes,2,complexI); cv::dft(complexI,complexI,cv::DFT_COMPLEX_OUTPUT); // Fourier Transform return complexI; } void expand_img_to_optimal(cv::Mat\u0026 padded,cv::Mat\u0026 img) { int row = cv::getOptimalDFTSize(img.rows); int col = cv::getOptimalDFTSize(img.cols); cv::copyMakeBorder(img,padded,0,row - img.rows,0,col - img.cols,cv::BORDER_CONSTANT,cv::Scalar::all(0)); } void lowpassFilter( cv::Mat\u0026 dft_filter,int distance) { cv::Mat tmp = cv::Mat(dft_filter.rows,dft_filter.cols,CV_32F); cv::Point center = cv::Point(dft_filter.rows/2,dft_filter.cols/2); double radius; for(int i = 0; i \u003c dft_filter.rows;++i) { for(int j=0; j \u003c dft_filter.cols;++j) { radius = (double)(sqrt(pow((i-center.x),2.0)+pow((j-center.y),2.0))); if(radius \u003e distance) tmp.at\u003cfloat\u003e(i,j) = 0.0; //if point is out of the radius make it zero else tmp.at\u003cfloat\u003e(i,j) = 1.0;// Else make it one } } cv::Mat toMerge[] = {tmp,tmp}; cv::merge(toMerge,2,dft_filter); //since we are dealing with Two Channel image which is greyscale } void crop_and_rearrange(cv::Mat\u0026 magI) { magI = magI(cv::Rect(0, 0, magI.cols \u0026 -2, magI.rows \u0026 -2)); int cx = magI.cols/2; int cy = magI.rows/2; cv::Mat q0(magI, cv::Rect(0, 0, cx, cy)); cv::Mat q1(magI, cv::Rect(cx, 0, cx, cy)); // Top-Right cv::Mat q2(magI, cv::Rect(0, cy, cx, cy)); // Bottom-Left cv::Mat q3(magI, cv::Rect(cx, cy, cx, cy)); // Bottom-Right cv::Mat tmp; // swap quadrants (Top-Left with Bottom-Right) q0.copyTo(tmp); q3.copyTo(q0); tmp.copyTo(q3); q1.copyTo(tmp); // swap quadrant (Top-Right with Bottom-Left) q2.copyTo(q1); tmp.copyTo(q2); } void showMagnitudeSpectrum(cv::Mat\u0026 complexI,const std::string\u0026 name) { cv::Mat p[2]; cv::split(complexI,p); cv::magnitude(p[0],p[1],p[0]); cv::Mat magI = p[0]; magI += cv::Scalar::all(1); cv::log(magI,magI); crop_and_rearrange(magI); cv::normalize(magI, magI, 0, 1, cv::NORM_MINMAX); showImg(magI,name.c_str()); } References OpenCV Computer Vision Application Programming Cookbook OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Modern C++ Programming Cookbook ","description":"In this article we will be looking at Low Pass and High Pass Filter and how to apply them using C++ and OpenCV","tags":["programming","cpp","opencv"],"title":"OpenCV - Understanding Low Pass And High Pass Filter using C++","uri":"/collections/programming/cpp/opencv/opencv-low-high-pass-filter/"},{"content":"OpenCV Fourier Transform using C++ In OpenCV, the idea of Fourier Transform is used in a lot of the processing you do on pictures and videos.\nJoseph Fourier was a French mathematician who worked in the 18th century, discovering and popularising several mathematical concepts. He specialised in investigating the principles of heat and, in mathematics, all things waveform.\nHe discovered that all waveforms are simply the sum of basic sinusoids of various frequency.\nWhat is Fourier Transformation The Fourier transform is a mathematical function that takes a time-based pattern and calculates the total cycle offset, rotation speed, and strength for each conceivable cycle in the pattern.\nThe Fourier Transform shows that any waveform can be re-written as the sum of sinusoidals.\nWaveforms, which are essentially functions of time, space, or any other variable, are subjected to the Fourier transform. A waveform is decomposed into a sinusoid via the Fourier transform, which gives another way to express a waveform.\nFourier Transform in Image Processing The Fourier Transform is a useful image processing method for breaking down an image into sine and cosine components.\nThe image in the Fourier or frequency domain is represented by the output of the transformation, whilst the spatial domain equivalent is represented by the input image. Each point in the Fourier domain image indicates a frequency contained in the spatial domain image.\nImage analysis, picture filtering, image reconstruction, and image compression are among applications that use the Fourier Transform.\nExample Now let us try how to apply Fourier Transform using OpenCV.\nThe Fourier Transform separates a picture into sinus and cosine components. In other words, it will change the spatial domain of an image to the frequency domain. The concept is that any function may be precisely approximated by adding infinite sinus and cosine functions. One method is to use the Fourier Transform. A two-dimensional image’s Fourier transform is defined mathematically as:\nThe image value in the spatial domain is f, and the image value in the frequency domain is F. Complex numbers are the outcome of the transformation. This can be displayed in two ways: as a real image and a complex image, or as a magnitude and a phase image. However, only the magnitude image is interesting throughout the image processing methods since it provides all of the information we require about the image’s geometric structure. However, if you want to make changes to the image in these forms and then retransform it, you’ll need to keep both of them.\nCode #include \u003copencv2/highgui.hpp\u003e #include \u003copencv2/imgcodecs.hpp\u003e #include \u003copencv2/imgproc.hpp\u003e #include \u003ciostream\u003e void showImg(cv::Mat\u0026, const std::string\u0026 ); void expand_img_to_optimal(cv::Mat\u0026 , cv::Mat\u0026 ); cv::Mat fourier_transform(cv::Mat\u0026 ); void crop_and_rearrange(cv::Mat\u0026 ); int main(int argc,char** argv) { cv::Mat input_img,fourier_img; input_img = cv::imread(argv[1],cv::IMREAD_GRAYSCALE); if(input_img.empty()) { fprintf(stderr,\"Could not Open image\\n\\n\"); return -1; } showImg(input_img,\"Input Image\"); fourier_img = fourier_transform(input_img); showImg(fourier_img,\"Fourier Image\"); cv::waitKey(); return 0; } void showImg(cv::Mat\u0026 img,const std::string\u0026 name) { cv::namedWindow(name.c_str()); cv::imshow(name.c_str(),img); } void expand_img_to_optimal(cv::Mat\u0026 padded,cv::Mat\u0026 img) { int row = cv::getOptimalDFTSize(img.rows); int col = cv::getOptimalDFTSize(img.cols); cv::copyMakeBorder(img,padded,0,row - img.rows,0,col - img.cols,cv::BORDER_CONSTANT,cv::Scalar::all(0)); } cv::Mat fourier_transform(cv::Mat\u0026 img) { cv::Mat padded; expand_img_to_optimal(padded,img); // Since the result of Fourier Transformation is in complex form we make two planes to hold real and imaginary value cv::Mat planes[] = {cv::Mat_\u003cfloat\u003e(padded),cv::Mat::zeros(padded.size(),CV_32F)}; cv::Mat complexI; cv::merge(planes,2,complexI); cv::dft(complexI,complexI,cv::DFT_COMPLEX_OUTPUT); // Fourier Transform cv::split(complexI,planes); cv::magnitude(planes[0],planes[1],planes[0]); cv::Mat magI = planes[0]; magI += cv::Scalar::all(1); cv::log(magI,magI); crop_and_rearrange(magI); cv::normalize(magI, magI, 0, 1, cv::NORM_MINMAX); // for visualization purposes return magI; } void crop_and_rearrange(cv::Mat\u0026 magI) { magI = magI(cv::Rect(0, 0, magI.cols \u0026 -2, magI.rows \u0026 -2)); int cx = magI.cols/2; int cy = magI.rows/2; cv::Mat q0(magI, cv::Rect(0, 0, cx, cy)); cv::Mat q1(magI, cv::Rect(cx, 0, cx, cy)); // Top-Right cv::Mat q2(magI, cv::Rect(0, cy, cx, cy)); // Bottom-Left cv::Mat q3(magI, cv::Rect(cx, cy, cx, cy)); // Bottom-Right cv::Mat tmp; // swap quadrants (Top-Left with Bottom-Right) q0.copyTo(tmp); q3.copyTo(q0); tmp.copyTo(q3) q1.copyTo(tmp); // swap quadrant (Top-Right with Bottom-Left) q2.copyTo(q1); tmp.copyTo(q2); } Explanation The above code shows the magnitude image of Fourier Transform.Digital Images are discret, this indicates they have the ability to take a value from a domain value. Values in a simple grey scale image, for example, are normally between zero and 255. As a result, the Fourier Transform must also be discrete, yielding a Discrete Fourier Transform (DFT).\nexpand_img_to_optimal() The size of the image has an impact on the DFT’s performance. For image sizes that are multiples of the digits two, three, and five, it is the fastest. As a result, padding border values to the image to create a size with such qualities is often a good idea to get maximum efficiency. We can use the copyMakeBorder() function to expand the borders of an image (the appended pixels are initialised with zero): getOptimalDFTSize() returns this optimal size, and we can use the copyMakeBorder() function to expand the borders of an image.\nCreating Two Planes for Complex and Real Values cv::Mat planes[] = {cv::Mat_\u003cfloat\u003e(padded),cv::Mat::zeros(padded.size(),CV_32F)}; cv::Mat complexI; cv::merge(planes,2,complexI); A Fourier Transform produces a complex output. This means that the output is two image values for each image value (one per component). Furthermore, the frequency domain’s range is substantially larger than that of the spatial domain. As a result, we normally save these in float format. As a result, we’ll are changing the type of our input image and add another channel to carry the complex values.\nTransform the real and complex values to magnitude There are two parts to a complex number: real (Re) and complex (imaginary - Im). A DFT yields complex numbers as a consequence. A DFT has the following magnitude:\ncv::split(complexI,planes); cv::magnitude(planes[0],planes[1],planes[0]); cv::Mat magI = planes[0]; The dynamic range of the Fourier coefficients turns out to be too large to display on the screen. We have certain little and large shifting variables that we are unable to see in this manner. As a result, the high values will all appear as white spots, while the low values will appear as black. We may convert our linear scale to a logarithmic one to use the grey scale values for visualisation:\nmagI += cv::Scalar::all(1); cv::log(magI,magI); crop_and_rearrange(cv::Mat\u0026 ) Remember how we extended the image in the first step? Now is the time to discard the newly inserted values. We can also rearrange the quadrants of the result for display purposes, so that the origin (zero, zero) corresponds to the image centre.\nResult References Building Computer Vision Projects with OpenCV 4 and C++\nLearning OpenCV 3: Computer Vision In C++ With The OpenCV Library\nOpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://pyimagesearch.com/\n","description":"In this article we will discuss What is Fourier Transform, and How to Use Fourier Transform in image processing","tags":["programming","cpp"],"title":"OpenCV dft() - Fourier Transform using C++","uri":"/collections/programming/cpp/opencv-fourier-transform-cpp/"},{"content":"MLOps: A Beginners Guid to MLOps We were all studying about the software development lifecycle (SDLC) until recently, and how it goes from requirement elicitation to designing, programming, testing, deployment, and maintenance. We studied (and continue to research) the waterfall, iterative, and agile software development models.\nAt this point, practically every other company is attempting to include AI/ML into their product. This new need for developing ML systems adds/reforms some SDLC principles, resulting in MLOps, a new engineering profession.\nMLOps – A new word has emerged, causing a stir and resulting in the creation of new job profiles. MLOps stands for Machine Learning Operations, which is also known as ModelOps.\nWhat is MLOps ? The discipline of delivering machine learning (ML) models through repeatable and efficient workflows is known as machine learning operations (MLOps).\nMLOps facilitates the continuous delivery of high-performing ML applications into production at scale, much like DevOps did for the software development lifecycle (SDLC). It considers ML’s specific requirements to create a new lifecycle that sits alongside traditional SDLC and CI/CD processes, resulting in a more efficient workflow and more effective results for ML.\nAll of the skills that data science, product teams, and IT require to deploy, manage, regulate, and protect models in production are included in MLOps. It consists of the following elements, which when combined, constitute an automated machine learning pipeline that maximises your ML performance and ROI:\nServing and pipelining models\nCatalog(s) of model service for all models currently in production\nVersion control for models\nInfrastructure administration\nMonitoring\nSecurity\nGovernance\nAll data sources are connected, and best-in-class tools for model training, development, infrastructure, and compliance are available.\nWhy MLOps ? We were dealing with reasonable amounts of data and a modest number of models on a small scale until recently. Now that we’re embedding decision automation in a wide range of applications, we’re facing a slew of technical obstacles in developing and deploying machine learning-based systems. To comprehend MLOps, we must first comprehend the lifespan of machine learning systems. A data-driven organization’s lifecycle involves multiple distinct teams. The following teams contribute from top to bottom:\nDefining business objectives with KPIs in the business development or product team\nData Engineering is the process of gathering and preparing data.\nArchitecting ML solutions and constructing models is what Data Science is all about.\nComplete deployment setup and monitoring alongside scientists – this is what IT or DevOps is all about.\nHow MLOps Works ? MLOps facilitates the ML lifecycle by connecting the ML code to all of the other elements required for ML success. This includes the following:\nData on training\nModel education\nValidation of models\nData on production\nServing and pipelining models\nCatalogue of sample services (s)\nVersion control for models\nInfrastructure administration\nMonitoring\nSecurity\nGovernance\nInterpretability and explainability\nMLOps includes all of the essential components, as well as the ability to integrate them all together—from data sources to compliance tools.\nWhat is ML Pipelines ? The data pipeline is a fundamental notion in data engineering. A data pipeline is a set of changes that we apply to data as it moves from one location to another. They’re commonly represented as a graph, with nodes representing transformations and edges representing dependencies or execution order. Many specialised tools are available to assist in the creation, management, and operation of these pipelines. ETL (extract, transform, and load) pipelines are another name for data pipelines.\nML models almost always involve data transformation, which is typically accomplished through scripts or even cells in a notebook, making them difficult to manage and run consistently. Switching to correct data pipelines has a number of benefits in terms of code reuse, runtime visibility, administration, and scalability.\nBecause ML training may also be thought of as a data transformation, it’s only logical to incorporate the exact ML processes in the data pipeline itself, transforming it into an ML pipeline. Most pipeline models will require two versions: one for training and one for serving. This is because, in most cases, data formats—and the methods for accessing them—are extremely diverse from one moment to the next, particularly for models that we provide in real-time queries (as opposed to batch prediction runs).\nThe machine learning pipeline is a pure code construct that is unaffected by specific data instances. This implies it’s easy to track its versions in source control and deploy them using a typical CI/CD pipeline, which is a DevOps best practise. This allows us to connect the code and data planes in a logical, automatic manner:\nIt’s important to note that there are two separate machine learning pipelines: the training pipeline and the serving pipeline. They all have one thing in common: the data transformations they execute must produce data in the same format, but their implementations can vary greatly.\nIt’s critical, though, to keep these two pipelines consistent, therefore code and data should be reused whenever possible.\nWhat is Model and Data Versioning ? Consistent version tracking is required to achieve reproducibility. In a conventional software world, versioning code is sufficient because it defines all behaviour. In machine learning, we must additionally keep track of model versions, as well as the data needed to train it and certain meta-data such as training hyperparameters.\nA traditional version control system like Git can track models and metadata, but data is often too massive and mutable for this to be efficient and useful. It’s also crucial to avoid linking the model lifecycle to the code lifecycle, because model training is generally done on a distinct schedule.\nVersioning data and tying each trained model to the specific versions of code, data, and hyperparameters utilised are also required. A purpose-built tool would be ideal, but there is currently no market consensus, and practitioners employ a variety of techniques, most of which are based on file/object storage protocols and metadata databases.\nMLOps tackles a number of major issues Managing such systems at scale is difficult, and several bottlenecks must be addressed. The following are some of the significant problems that teams have proposed:\nA scarcity of Data Scientists capable of creating and implementing scalable web applications exists. In today’s market, there is a new profile of ML Engineers that tries to meet this need. It’s a sweet place where Data Science and DevOps intersect.\nReflecting changing business objectives in the model — With data constantly changing, maintaining model performance requirements, and guaranteeing AI governance, there are many dependencies. It’s difficult to keep up with the constant model training and changing company goals.\nThe black-box nature of such ML/DL systems has sparked a lot of discussion. Models frequently deviate from their original purpose. Assessing the risk and expense of such failures is a crucial and time-consuming step.\nFor example, the cost of an incorrect YouTube video suggestion is far smaller than the cost of falsely accusing someone of fraud, blocking their account, and rejecting their loan applications.\nReferences https://predera.com/ https://mobcoder.com/ https://mobcoder.com/ https://medium.com/slalom-data-analytics/mlops-part-2-machine-learning-pipeline-automation-with-aws-1ca10348239e ","description":"ML Ops (or MLOps) is a set of methods that combines Machine Learning, DevOps, and Data Engineering with the goal of reliably and quickly deploying and maintaining machine learning systems.","tags":["programming","Machine learning"],"title":"MLOps - Beginners Guid to MLOps,What It Is, and How to Implement It","uri":"/collections/programming/machine-learning/mlops/what-is-mlops/"},{"content":"A Comprehansive Guid to React Native Elements React Native Elements (RNE) are a cross-platform user interface toolkit. These elements may now be utilised on the web, and your codebase can be shared between both React Native and React web projects. On the browser, the components of react-native elements are rendered flawlessly. After collaborating with React Native for Web and RNE, you may utilise these components to target Android, iOS, and the web.\nIt is a framework for styling React applications. It includes built-in components that replace React Native’s basic components. Elements’ styles are incredibly adaptable and can be adjusted to meet our specific requirements. Every component in Elements has a React Native \u003cView/\u003e component, and the style is done according to the properties specified.\nThe react-native elements’ objective is to provide a comprehensive UI kit for creating react native apps. There are various great UI components created by developers in open-source. Furthermore, react-native elements make the process of putting together React packages easier by providing a ready-made kit with a dependable API, look, and feel.\nSome of the most commenly used React Native Elements are as follows:-\nAvtars Avatars appear frequently in the UI design. You may find them everywhere in the UI design, from lists to profile panels. These elements are commonly used to represent a user. Icons, photographs, and text can all be included.\nBadges Badges are little components that are commonly used to communicate a numeric value to the user or to indicate the state of a certain object.\nThe small badge is one of the most common varieties of badge. When no value prop is specified, it enters the picture. Furthermore, it is quite excellent in representing different statuses. As can be seen in the diagram above, each hue represents a different badge.\nButton Buttons have a reputation for promoting screen interaction. They’re touchable elements that can show icons, text, or both at the same time. They may be styled with a variety of objects to make them stand out.\nButtonGroup ButtonGroup is a set of sections that runs in a straight line. Each ButtonGroup acts as a button that can show a different view. A ButtonGroup can be used to present choices that are closely related yet mutually incompatible.\nWith React and Tailwind CSS, how do you make a ButtonGroup Component? - Developers’ Community\nBottom Sheet A Bottom Sheet is an overlay modal that displays content from the bottom portion of the screen.\nCheckBox CheckBoxes allow users to complete tasks that need them to make decisions. The options can be selected or the settings can be turned on and off. Furthermore, it provides a clear visual representation of whether the choice is true or incorrect. Whichever React training method you choose, knowing these components is essential.\nCard Cards are a great method to display information. They usually consist of content and behaviours related to a specific topic. Cards can also incorporate text, buttons, photos, and other elements.\nDivider The usage of separators to visually separate the material is common. When you want to draw a separation between different portions of material, you can use a divider.\nFAB (Floating Action Button) Button with a Floating Action\nA floating action button (FAB) moves across the screen and performs the principal or most familiar action. This element appears in the foreground of all screen content. It is usually available in a circular shape with a symbol in the centre.\nHeader Headers are essentially navigation elements that display information and actions relevant to the current screen.\nThe heading in the graphic above is ‘My History,’ and it contains a menu underneath it that prompts you with appropriate options. The following are some examples of how headers are commonly used:\nDefault components in the header\nCustom components are passed in via props in the header.\nCustom components are sent in as children in the header.\nThe centre component is positioned to the left.\nMixture of components in the header\nCustomizable headers\nPriority of components\nHeadings in HTML format (Text) Characters and words of various sizes are featured in the text. In the React tutorial, a simple sample of HTML style heads can be seen, with each text readily identifiable.\nIcon Icons are the greatest choice for graphically indicating or describing purpose or activity.\nThe react-native-vector-icons package is used to create icon sets in React Native Elements.\nThe following are the icon sets that are accessible in React Native Elements:\nantdesign evilicon entypo font-awesome font-awesome-5 feather fontisto foundation material material-community ionicon octicon simple-line-icon zocial Image The images component is a drop-in replacement for the React Native Image component. They include graphics as well as a placeholder and smooth image transitions.\nInput These components, which look like forms or dialogues, allow users to enter text into a user interface.\nLabels and icons are used to input data.\nListItem ListItem is the appropriate component for displaying rows of information. They show rows of data such as a menu, a playlist, or a contact list. They can include symbols, badges, avatars, switches, and more, and are highly configurable.\nWhen you hover your mouse over each of the choices in the image above, you’ll see a drop-down menu appear.\nOverlay The Overlay is a view that floats above an app’s content. In general, overlays are simple methods of notifying or requesting information from the user.\nLinear Progress Users can see the status of ongoing processes thanks to these components. They employ progress markers to show how far they’ve come. They show states such as form submission, programme loading, and saving updates, among others. They also reflect accessible actions, such as whether a user may navigate away from the current screen, and express the status of an app.\nRating Ratings are a good way to get quantitative input from users. The user involvement is boosted by the clear pictures and rating indicators. Knowledge of such components is required when preparing for the React Interview Questions and Answers.\nPricing These components provide a great deal of ease to the user by displaying features and pricing. They present pricing information in a visually appealing manner.\nThe pricing component, for example, is depicted in the diagram above. It specifies the price as well as the features included in the specific bundle.\nSearchBar SearchBars can be used to filter or search for objects. When the amount of objects directly effects a user’s ability to find one among them, you can utilise a SearchBar.\nDigitalOcean takes a look at the React Native Elements UI Toolkit.\nThe following are examples of searchbars:\nDefault SearchBar SearchBar that is specific to a platform (iOS and Android) SocialIcon SocialIcons are graphic representations of internet and social media networks.\nNative React Example Tutorial: Create a Beautiful Social Icons Button Using React Native Elements\nSlider You may easily select a value from the available alternatives using this component. Furthermore, this component is regarded as a forked version of react-native-slider.\nSwitch It tells if a certain condition is active or not. This component is utilised when the need to indicate the user’s decision occurs. A switch is a controlled component that renders a boolean value and requires a onValueChange to update the value prop. React native elements provide you with an extra theme and colour support via the Switch Button.\nSpeed Dial A floating action button can show 3 to 6 actions as speed dial when Speed Dial is pressed. You can use something other than an FAB if you need more than six actions. When the FAB is pressed, it remains visible and generates a stack of activities. If the FAB is tapped in this condition, it must either perform its default action or disable the speed dial functions.\nTabs Tabs organise content across many displays, data sets, and a few other interactions in a logical way. The content of the’recent, favourite, and cart’ choices is neatly organised in the diagram below.\nTabView The following is a list of the Props available in this component:\nvalue onChange animationConfig animationType Tile Tiles, like cards, are a simple way to display pertinent content about a single topic. Tiles can be utilised in a variety of ways, including:\nTile of the Week Tile with an Icon as a Featured Item Tile with a symbol Tooltip When users hover their cursor over an element, tooltips appear.\n","description":"React Native Elements (RNE) are a cross-platform user interface toolkit. These elements may now be utilised on the web, and your codebase can be shared between both React Native and React web projects.","tags":["programming","react native"],"title":"A Comprehansive Guid to React Native Elements","uri":"/collections/programming/react-native/react-native-elements/"},{"content":"Python Class Method Decorator: A Comprehensive Guide Python is a powerful programming language that supports many features that make programming easier and more efficient. One of these features is the class method decorator. In this article, we will explore what class method decorators are, how to use them, and some practical examples.\nTable of Contents Introduction What is a Class Method? What is a Decorator? What is a Class Method Decorator? When should you use Python Decorators? Characteristics of Decorators classmethods() Syntax classmethod() Parameters classmethod() Return Value Practical Examples of Class Method Decorators Conclusion FAQs Introduction In Python, decorators are a simple and elegant way to add functionality to functions and methods. The use of decorators can be done in a variety of ways. Decorators can be used with methods declared in a class, which is a handy use-case. The functionality of the defined method can be extended by decorating methods in the classes we design. We could, for example, do a data integrity check or save the method call’s result to a file. What we choose to do is entirely up to us. Method decorators merely give us the ability to enhance functionality in a stylish manner.\nWhat is Class Method ? A class method is one that is attached to a class rather than its object. It, like staticmethod, does not necessitate the construction of a class instance.\nThe following is the distinction between a static method and a class method:\nThe static method has no knowledge of the class and only deals with the parameters.\nBecause the class method’s parameter is always the class itself, it works with it.\nWhat is a Decorator? In Python, a decorator is a function that takes another function as input and returns a modified version of the input function. A decorator is denoted by the @decorator_name syntax, where decorator_name is the name of the decorator function.\nDecorators can be used to modify the behavior of functions or classes without changing their code directly. This is a powerful feature of Python that allows for code reuse and simplification.\nWhat is a Class Method Decorator? A class method decorator is a decorator that is used to modify the behavior of a class method. Class method decorators are defined using the @classmethod and @decorator_name syntax.\nA class method decorator can be used to modify the behavior of a class method by adding functionality before or after the method is called, or by modifying the arguments passed to the method. Class method decorators can also be used to create alternative constructors, which allow creating instances of the class using different parameters or inputs.\nWhen Should You Use a Python Decorator? When you need to change the behaviour of a function without changing the function itself, you’ll use a decorator. When you want to add logging, test performance, do caching, validate rights, and so on, here are some nice examples.\nWhen you need to run the same code on many functions, you can utilise one. This prevents you from creating redundant code.\nCharacteristics of Decorators Declares a method for a class.\nThe first parameter must be cls, which is a class attribute accessor.\nOnly the class attributes, not the instance attributes, are accessible via the class method.\nClassName can be used to invoke the class method.\nIt has the ability to return a class object.\nThe @classmethod decorator is used to declare a class method that may be called using ClassName.MethodName(). A class object can also be used to call the class method.\nThe classmethod() function can be replaced with the @classmethod() function. Because the @classmethod decorator is only a syntactic sugar, it is advised to use it instead of the function.\nclassmethods() Syntax Syntax of classmethod methods is:\nclassmethod(func) ## un-Pythonic way #or @classmethod def func(cls,args...) classmethod() Parameters classmethod() method takes one argument:\nfunction - Function that need to be converted into class method. classmethod() Return Value It returns a class method for a giver function.\nExample Let’s have a look at an example of a class that deals with dates (this will be our boilerplate):\nclass Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year This class might certainly be used to hold information about specific dates.\nHere we have __init__, a typical initializer of Python class instances, which receives arguments as a typical instancemethod, having the first non-optional argument (self) that holds a reference to a newly created instance.\nLet’s say we want to construct a lot of Date class instances with date information encoded as a string with the format ‘dd-mm-yyyy’ from an external source. Assume we need to perform this in several locations across our project’s source code.\nSo here’s what we need to do:\nParse a string to get the day, month, and year as three integer variables or a three-item tuple that includes those variables.\nPass those variables to the initialization method to create Date.\nwhich look’s something like this:\nday, month, year = map(int, string_date.split('-')) date1 = Date(day, month, year) C++ can implement such a functionality with overloading for this reason, but Python lacks this overloading. We can instead use classmethod. Let’s create another “constructor”.\n@classmethod def from_string(cls, date_as_string): day, month, year = map(int, date_as_string.split('-')) date1 = cls(day, month, year) return date1 date_obj = Date.from_str(\"2021-11-12\") Let’s take a closer look at the above implementation and see what benefits we have:\nWe’ve made date string parsing reusable by putting it all in one place.\nEncapsulation works well in this case (if you think that you could implement string parsing as a single function elsewhere, this solution fits the OOP paradigm far better).\ncls is not a class instance, but rather an object that holds the class itself. It’s cool because if we inherit our Date class, all of our children will inherit `from_string() method.\nPractical Examples of Class Method Decorators Example 1: Alternative Constructor class Person: def __init__(self, name, age): self.name = name self.age = age @classmethod def from_birth_year(cls, name, birth_year): age = datetime.date.today().year - birth_year In this example, we define a class Person with an __init__ method that initializes the name and age attributes of the class. We also define an alternative constructor from_birth_year, which takes the name and birth_year as arguments and calculates the age of the person based on the current year.\nExample 2: Logging Decorator import logging def log_class_method(func): def wrapper(*args, **kwargs): logging.info(f\"Calling {func.__name__}\") return func(*args, **kwargs) return wrapper class MyClass: @classmethod @log_class_method def my_class_method(cls, arg1, arg2): # method code here In this example, we define a decorator function log_class_method that logs the name of the decorated method before it is called. We then decorate the my_class_method with this decorator, which adds logging functionality to the method.\nExample 3: Caching Decorator def memoize_class_method(func): cache = {} def wrapper(cls, arg): if arg not in cache: cache[arg] = func(cls, arg) return cache[arg] return wrapper class MyClass: @classmethod @memoize_class_method def my_class_method(cls, arg): # method code here In this example, we define a decorator function memoize_class_method that caches the results of the decorated method for future use. We then decorate the my_class_method with this decorator, which adds caching functionality to the method.\nConclusion In this article, we explored what class method decorators are, how to use them, and some practical examples. Class method decorators are a powerful tool in Python programming that allows modifying the behavior of a class method without changing its code directly. They can be used to add functionality before or after the method is called, or to modify the arguments passed to the method. Class method decorators can also be used to create alternative constructors or to add caching or logging functionality to a method.\nFAQs What is a decorator in Python? A decorator is a function that takes another function as input and returns a modified version of the input function. It is a powerful tool for modifying or enhancing the behavior of functions and classes without explicitly modifying their code.\nWhat is a class method in Python? A class method is a method that is bound to the class and not the instance of the class. It can be called on the class itself, not just on an instance of the class. Class methods are defined using the @classmethod decorator.\nWhat is a class method decorator in Python? A class method decorator is a decorator that is used to modify the behavior of a class method. Class method decorators are defined using the @classmethod and @decorator_name syntax.\nHow to use a class method decorator in Python? To use a class method decorator, you need to define the decorator function and use it with the @classmethod syntax. The decorator function modifies the behavior of the class method by adding functionality before or after the method is called, or by modifying the arguments passed to the method.\nWhat are some practical examples of using class method decorators in Python? Some practical examples of using class method decorators in Python are alternative constructors, logging decorators, and caching decorators. These examples demonstrate how class method decorators can be used to modify the behavior of a class method and add functionality to it.\nRefrences https://www.educba.com/python-classmethod-decorator/ https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner ","description":"Learn how to use decorators to modify and enhance the functionality of your Python class methods. This guide covers everything from the basics to advanced techniques, with practical examples and code snippets to help you get started.","tags":["programming","python"],"title":"Python Class Method Decorator- A Comprehensive Guide","uri":"/collections/programming/python/python-class-method-decorator/"},{"content":"Best Social Media Automation Tools In the previous two decades, marketing has changed dramatically.To achieve customer loyalty, businesses must now be able to think on their feet.According to social media statistics, automation of digital marketing techniques, particularly social media, is critical.\nThis will free up time for you to focus on what really matters: your business.In addition, you may choose from a variety of social networking platforms to make your marketing efforts more seamless.\nIn this piece, we’ll go through number of social media automation tools that will help you improve your marketing and grow your business.\nSEMrush SEMrush is a digital marketing software that specialises in SEO and PPC management. It does, however, provide a Social Media Toolkit, which allows for automated posting, tracking, promotion, and analytics across all major social media platforms.\nSocial Media Poster The Social Media Poster tool allows you to prepare and schedule your Facebook, Twitter, Instagram, Pinterest, and LinkedIn posts up to a week ahead of time, and it will automatically post them at the appropriate time.\nYou can check a post’s performance after it has been published, and if it seems good, you can reschedule it as a one-time post or add it to a repeating queue.\nYou may also add five RSS feeds to the Social Media Poster to automatically capture the most recent content from other websites.\nYou can go over the content ideas that have been gathered and add them to your queues for automatic publishing.\nSocial Media Tracker The Social Media Tracker tool works in the background, checking and aggregating all of your important interaction metrics into a single, easy-to-read table so you can determine which platforms and content are most effective.\nIn addition, the tracker finds your competitors’ social media accounts automatically (based on the domains you enter), allowing you to examine their campaigns, hashtags, special offers, and top-performing posts to improve your own social media strategy.\nSEMrush visualises the combined data in a graph, allowing you to compare yourself to your competitors based on three key metrics: audience, activity, and engagement.\nAdditionally, you can use customisable, easy-to-read PDF reports to report on the progress of your social media efforts to your teammates or clients.\nBrand Monitoring SEMrush’s Brand Monitoring Tool, which is part of its Content Marketing Toolkit, tracks any word or phrase you choose to track online.\nYou may monitor Twitter and Instagram feeds for certain keywords, hashtags, or identities, such as influencers or competitors, in addition to website mentions.\nYou may, for example, arrange the Instagram users who mention your brand name by the amount of mentions.\nPricing Starting at $99.95 a month, SEMrush offers a variety of membership levels that all include the Social Media Toolkit (paid annually).\nBrand Monitoring, on the other hand, is only available in the Guru and Business Plans, which start at $191.62 per month (paid annually).\nHootsuite ![Screenshot from 2021-10-28 23-53-31.png](../_resources/Screenshot from 2021-10-28 23-53-31.png)\nFor marketers, Hootsuite is a popular social media automation platform.Its versatile capabilities allow it to work with a wide range of social media networks. Because of the sophisticated features and cost range, it is primarily targeted at major enterprises and large agencies.\nWith Hootsuite’s social media streams, you may also watch many social media networks at simultaneously. Yes, with its real-time updates, you may use it to keep track of all your brands.Because it keeps track of your brands, you’ll be notified whenever your brand is mentioned, and you’ll be able to see what customers are saying about it.\nHoostsuite Core Features You can view social media streams in one place with automated social media posting. Hootsuite makes you understand how your social media strategy is performing by availing reports whenever you need them. Agorapulse ![Screenshot from 2021-10-28 23-49-08.png](../_resources/Screenshot from 2021-10-28 23-49-08.png)\nAgorapulse is a one-stop shop for social media management that saves you time.\nAgorapulse gathers all of your social discussions in one location for you to review, respond to, and classify. In chronological sequence, the inbox collects comments, mentions, messages, and reviews.\nYou can also use automated moderation rules to review, tag, hide, or allocate important messages, allowing you to spend less time in your inbox. You can also answer fast by using your template responses.\nThe scheduling capabilities in Agorapulse allow you to publish posts once, numerous times, or repeatedly with different queues. Queue categories can also be used to hold an unlimited number of queued posts on specific themes and topics.\nBased on user activity, the built-in Social CRM tool can automatically tag, label, and rank your most ardent fans, followers, and influencers for each social network.\nFurthermore, the monitoring tool may provide you automatic updates on trends, hashtags, and keywords.\nAgorapulse’s reporting tool collects data on your content performance in the background, so you know what content performs best on each social network.\nAgorapulse Core Features Each network’s chats are merged into a single mailbox.\nTo control crucial communications, it employs automated moderation rules.\nContent is scheduled and queued to be published once, multiple times, or on a regular basis.\nStores and categorises an unlimited number of queued postings on specific topics.\nTags your users based on their level of participation.\nAutomated notifications keep track on trends, hashtags, and keywords.\nProduces comprehensive analytical reports and insights\nPricing Starting at €99/month, Agorapulse offers a variety of membership rates based on the amount of users and social accounts you require (discount offered when purchased annually). Start with their free plan, which allows you to have three social profiles and one user.\nSocialPilot SocialPilot is a social media automation and analytics platform for organisations of all sizes, from small to large. You can expect to get a lot of features with this social media automation application, including social media scheduling, calendar management, client management, and more.\nFurthermore, it makes no difference whether social media outlet you utilise because SocialPilot has you covered. You can use SocialPilot with many social media sites at the same time, including LinkedIn, Facebook, Pinterest, Twitter, Instagram (Direct Publishing), and TikTok.\nThe best part is that SocialPilot has a social media calendar function that you can use to keep track of your social media initiatives. Filters may be added to your calendars, and you can track your social media campaigns by groups and profiles. Furthermore, the calendar feature allows users to drag and drop their soci* al media postings and reschedule them based on changes.\nFinally, SocialPilot * users can assess how engaged their audience, influencers, demographics, and competitors are in their social media post-performance.\nSocialPilot’s Key Features With advanced social media publishing capabilities and audience targeting, SocialPilot allows you to schedule your posts whenever you choose.\nWith downloadable PDF social media reports, you’ll obtain useful insights on your automatic social posts and know where to make adjustments.\nTo keep your company’s material flowing, provide access levels to your staff.You can add team members and assign accounts and access levels to them using the team collaboration tool.\nYou may utilise your Social Inbox to control Facebook page conversations.\nBy using Lead Ads or Boost Post, you can take control of your Facebook marketing campaigns.\nExcellent customer support is accessible 24 hours a day, 7 days a week to assist you by chat, email, or phone.\nPricing When you sign up, they provide you a 14-day free trial and a SocialPilot tour (no credit card required)\n$30/month for the Professional Plan\nPlan for Small Teams - $50 per month\n$100/month Studio Plan\n$150/month Agency Plan\nEnterprise Plan – you’ll need to get in touch with them for a price.\nBuzzSumo If you want a better knowledge of your content marketing strategy, BuzzSumo is a must-have social media search engine. Rather than wasting time guessing article subjects, utilise BuzzSumo to find some of the most popular terms in your market as well as the publications.\nBecause of its capacity to provide in-depth insights into what your competitors are doing, BuzzSumo has become a popular social media marketing tool over time.\nIf you want to become a clever content marketer and remain ahead of your competition, BuzzSumo can help you find the most popular material by using the search field to enter either a keyword or a domain name.\nYou can filter your results based on what you’re looking for, from duration to location. BuzzSumo may also gather data from popular social media platforms like Facebook, Pinterest, LinkedIn, and Twitter, to name a few.\nPromoRepublic PromoRepublic is a social media automation solution that allows organisations to manage hundreds or thousands of social media sites at once. They provide three different solutions for businesses of various sizes, ranging from tiny businesses to mid-sized agencies and enterprises.\nPromoRepublic includes a number of automation capabilities that can help social media marketing teams minimise their workload, including:\nAuto-reposting of high-performing content – If you have a really popular post, you can use PromoRepublic to automatically republish it at a later date to boost interaction.\nMaterial approval procedures — If you’re dealing with a variety of brands and agencies, you can set up automated workflows to guarantee that everyone is satisfied with the content before it goes live.\nSmart automated publishing — Schedule posts from a curated database to go live when it’s most convenient for your audience.\nThe variety of ready-to-use content offered for small businesses is one of PromoRepublic’s biggest features.\nIf you want to fill up your social media pages but don’t have time to create your own, you may choose from PromoRepublic’s extensive library of industry-relevant content to keep your followers engaged and build your reputation.\nOverall, it’s an excellent solution for small firms and larger corporations looking to streamline their operations.\nBuffer Buffer is a famous social media marketing automation application that allows users to schedule posts. LinkedIn, Twitter, Instagram, and Facebook users will benefit from this tool. Customers with the pro plan can now use the Pinterest function. Those on a cheaper plan, on the other hand, will only be able to link a restricted number of social media accounts.\nAs a new user, you have the option of using Buffer’s basic scheduling capabilities or customising your automation to meet your needs. By default, this social media automation service gives customers four slots per day. Depending on the amount of campaigns you have to post each day, you have the option to eliminate or add spaces. You’ll have to schedule additional posts if you have more campaign slots.\nFor Firefox, Safari, and Chrome, the Buffer browser extension is available. You can schedule your posts with these extensions by clicking the Buffer button to add them to your queue. There are also mobile applications available for Android and iOS.\nPallyy Pallyy is a social networking platform designed specifically for visual content promotions. Pallyy may be used to schedule content for a variety of social media platforms, but it’s perfect for Instagram marketers because it offers a lot of automation tools that can help you streamline your Instagram workflows.\nYou can utilise the client capabilities to send content to your clients before it is published, so they can give feedback. You may also search for user-generated material to repost using the Pallyy content planning tools to save time on content creation.\nPallyy, unlike many of the other solutions on our list, is mobile-friendly, allowing you to manage your Instagram marketing and social media scheduling while on the go, making it ideal for busy people.\nPallyy also has a bulk uploading tool that can make scheduling huge campaigns much easier and faster if you work with a variety of clients and have a lot of content to publish.\nPallyy is an excellent tool for Instagram marketing, and its visual editor and client capabilities make it an excellent alternative for freelance social media managers and small businesses.\nSendible Sendible is another excellent social media automation application that can assist you in managing and automating your social media networks. Sendible not only allows you to schedule individual campaigns on various social media accounts, but it also allows you to control how your campaigns are delivered directly from your dashboard.\nThe best feature is the ability to produce material and schedule it for a week, month, or as long as you need it. You may also customise your material for other social sites by using tools like emoticons, attachments, and more.\nStoryChief StoryChief is a multi-channel marketing platform with robust social media management and automation capabilities.\nEverything from social media campaigns to SEO copywriting and more can be managed with this platform. StoryChief has a number of handy automation capabilities, including automatic posting to all of your social media and CRMs, as well as content approval workflows.\nStoryChief also includes a content calendar that you can use to organise social media content, blog entries, and more from one centralised dashboard.\nOverall, StoryChief is a terrific solution for firms that want to use a variety of platforms in their content marketing strategy, including social media.\n","description":"With the top social media automation tools for 2021, you can automate your social media marketing channels.  This article contains reviews, advantages, disadvantages, and much more.","tags":["seo"],"title":"Best Social Media Automation Tools","uri":"/collections/seo/social-media-automation-tools/"},{"content":"WebViews in React Native allow access to any online site from within the mobile app.\nIn a React Native project, the WebView component is used to load webpages. It used to be included in React Native out of the box, but it has since been removed from the core and added to the React Native Community libraries; for additional information, see The Slimmening proposal.\nRequirements The requirements to follow this tutorial are:\nNodejs \u003e=8.x.x with npm or yarn installed as a package manager. react-native-cli. React Native WebView: Getting Started To get started with web view setup, we must first install the plugin itself. We’re going to use NPM(Node Package Manager) to install the plugin, although we could also use yarn. As a result, we must perform the following command in the command prompt of our project folder in order to install the plugin:\nnpm install --save react-native-webview #for npm yarn add react-native-webview # for yarn After that, connect the dependencies.Auto-linking will handle the linking procedure starting with react-native 0.60, but don’t forget to execute pod install beforehand.\nNative Objective-C, Swift, Java, or Kotlin code in React Native modules must be “linked” so that the compiler knows to include it in the app.\nRun the following command to link it:\nreact-native link react-native-webview For IOS Run the following command if you’re using CocoaPods in the ios/ directory:\npod install For Android Make sure AndroidX is enabled in your project if you’re using react-native-webview version 6.X.X by editing android/gradle.properties and adding the two lines below:\nandroid.useAndroidX=true android.enableJetifier=true I hope you were able to install it successfully. Please refer to the official installation guide if you get stuck.\nNow I’ll show you various relevant WebView examples, ranging from simple to complicated, as well as some how-tos.\nQuickstart with WebView in its most basic form import React, { Component } from \"react\"; import { WebView } from \"react-native-webview\"; class MyWebView extends Component { render() { return ( \u003cWebView style={{ marginTop: 35 }} source={{ html: \"\u003ch1\u003eAnother Techs,Welcomes you\u003c/h1\u003e\" }} /\u003e ); } } We’ve used the react-native-webview plugin to import the WebView component. Now, as demonstrated in the code sample above, we can utilise this component to load the HTML content.\nThe MyWebView class component has been defined. The render() function in this class component renders the WebView component.The HTML content for the WebView component is set to its source prop.\nLoading Remote URL import React, { Component } from \"react\"; import { WebView } from \"react-native-webview\"; class MyWebView extends Component { render() { return ( \u003cWebView source={{ uri: \"https://anothertechs.com/\" }} style={{ marginTop: 35 }} /\u003e ); } } The content is obtained using the source attribute, which might be a URL or HTML.You must pass an object with the field uri to load a webpage by its URL, as demonstrated below:\n\u003cWebView source={{ uri: \"https://anothertechs.com\" }} /\u003e originWhitelist property in WebView The origin where users can navigate in your WebView is controlled by the originWhitelist attribute. The default whitelisted origins are http:// and https://, and it accepts an array of strings.\nFor example, if you want to make sure that users can only go to URIs that start with https:// or git://, you’d do it this way:\n\u003cWebView originWhitelist={['https://*', 'git://*'] source={{ uri: 'https://anothertechs.com/' }} } /\u003e React Native Webview with a loading spinner It may take some time for the complete HTML content on the page to load when using the WebView component to browse the URL. As a result, until the website loads, we’ll show a loading indication to illustrate the delay. As demonstrated in the code sample below, we need to import the ActivityIndicator component from the react-native package for this:\nimport { Text, View, StyleSheet, ActivityIndicator } from \"react-native\"; import * as React from \"react\"; import { Text, View, StyleSheet, ActivityIndicator } from \"react-native\"; import { WebView } from \"react-native-webview\"; function Loading() { return \u003cActivityIndicator color=\"#009b88\" size=\"large\" /\u003e; } export default function App() { return ( \u003cWebView originWhitelist={[\"*\"]} source={{ uri: \"https://anothertechs.com/programming\" }} renderLoading={this.Loading} startInLoadingState={true} /\u003e ); } We’ve used the AcitivityIndicator with colour and size props in this example.Then we called the Loading function from the WebView component’s renderLoading prop.This enables us to keep the loading indication visible until the webpage is completely loaded. The startInLoadingState attribute is also used in this case. On the first load, this boolean value compels the WebView to show the loading view.In order for the renderLoading prop to work, this prop must be set to true.\nWrapping Up We learned about React Native’s web view property in this tutorial. Because React Native’s built-in web-view feature is being phased out, we learned how to use the react-native-webview third-party web view plugin.First, we learnt how to use the WebView component to render simple HTML content.Then we got a step-by-step tutorial on how to use the WebView component and its props to render the whole HTML content from the URL, including the loading indication.If you want to learn more about this web view plugin, head over to the main repository’s discussion thread.\n","description":"React Native WebView presents web information in a native view, and it comes with a variety of tricks and features to help you get the most out of it.","tags":["programming","react native"],"title":"React Native WebView Starter Guid","uri":"/collections/programming/react-native/react-native-webview/"},{"content":"Top HTML,CSS online courses for Beginner From beginner to advanced, the top online courses for learning HTML and CSS will help you enhance your website design and development skills.\nThe essential coding languages for web design and development are HTML and CSS. A working knowledge of these two is required if you need to execute any type of web modification activity.\nHTML, or HyperText Markup Language, tells the web browser what each section of a website is, while CSS, or Cascading Style Sheets, aids in the appearance and formatting of the online pages. Although there are many WYSIWYG (What You See Is What You Get) tools and services for creating websites, mastering HTML opens up a world of possibilities that no WYSIWYG editor can match.\nFurthermore, HTML is only the beginning of a much broader universe, and it is useful for people in a variety of different fields, such as marketing and app development.\nWeb-based business apps, such as HTML, CSS, and JavaScript, are in high demand since they can operate natively on any mobile device. It’s no surprise, then, that the Bureau of Labor Statistics expects a double-digit increase in web developer positions over the next five years.\nHere are some of the greatest courses to help you go from being an HTML noob to building your first web application.\nCoursera HTML allows you to do a lot more than just construct webpages, as I indicated previously. With the HTML, CSS, and Javascript for Web Developers course, you can take things a step further and learn how to construct web apps after you’ve put in some time and are comfortable writing HTML and designing it with CSS.\nJohns Hopkins University offers a fairly complete course in this area. Although it is part of the Ruby on Rails (RoR) specialisation, it may be taken alone and does not require any prior understanding of RoR. This beginning course, unlike the previous one, covers HTML5, CSS3, the Twitter Bootstrap CSS Framework, and JavaScript, which the presenter characterises as “the programming language for the web.”\nThe educator not only imparts practical knowledge, but also helps you understand why you’re doing what you’re doing. The course will last approximately 30 hours and will be carried out over four weeks. It contains video lessons, practise tests, and lists of suggested reading resources. The instructor sits down with a real client to create a website for his restaurant based on the client’s specifications, which is one of the wonderful features about the course that makes it stand out.\nCourse Link\nUdemy Udemy offers an HTML course in which you will learn everything you need to know about web design, professional website development, producing large-scale project codes, and more. Jonas Schmedtmann, a highly experienced and great web developer, designer, and teacher, will lead this course. He has the top Udemy ratings and reviews, as well as a Master’s degree in Engineering and a desire to educate. This course will provide you with the most up-to-date information by providing on-demand videos, articles, and other resources.\nThe course has the following features:\nYou’ll learn how to plan and code large-scale projects. It will be possible to create stylish and responsive websites in a professional manner. Methods for creating a quality website from the ground up that have been tried and tested. Learn how to use jQuery to create effects like sticky navigation, animations, and scroll effects. You can get a free eBook with all of the course material. Full access to 11.5 hours of on-demand videos, seven supplement sources, and eleven articles that will vastly improve your knowledge and skills. Course Link\nLinkdin Learning When you’ve mastered the art of building websites for desktop and mobile platforms, it’s time to include structured data to help improve the semantics of your site. You may give your website a greater degree of depth by using structured data to better define its content. The HTML: Structured Semantic Data course teaches you how to improve the readability of HTML code for search engines, web crawlers, and other user-engines.\nThe three-hour session looks at some of the most common structured data syntaxes and how to pick the correct one for you. This is a relatively new course that covers organised semantic data well and in a way that other courses on the subject do not.\nThe course is targeted for structured data beginners and assumes you have a decent working knowledge of HTML. It covers all of the commonly used structured data syntax, including Microformats, as well as how to interact with JSON-LD and gives extensive structured data examples.\nCourse Link\nUdacity HTML is frequently regarded as a good starting point for programming since it teaches you how to think and interact like a coder. If you’ve never programmed before, Udacity’s Intro to HTML and CSS is a great place to start. The Free course will not only teach you the fundamentals of HTML and CSS, but it will also expose you to broad programming principles and aid in the development of a programmer’s vocabulary.\nThe training is well-paced and geared for complete novices. It is structured into four lessons, the first two of which introduce HTML tags and the last two of which focus on CSS styling. The course employs a combination of videos, written lectures, interactive quizzes, and workspaces to not only introduce tags but also to practise them right within the course.\nInstead of introducing each and every HTML tag or CSS properties, the instructors encourage the use of reference resources like the HTML element reference on Mozilla’s Developer Network and the CSS-Tricks Almanac. They also introduce the developer tools built into the different browsers and give a primer on how you can use it to enhance your knowledge, which is a definite plus.\nCourse Link\nLynda( now linkdin ) Lynda gives a free lesson for those who are unsure about their ability to acquire knowledge and skills in computer language. Lynda allows newbies receive free access to hundreds of videos, course material, and articles. However, if you require additional support or tutoring, you can pay a fee to use the facility. This way, you’ll be able to study and practise HTML for free, and you’ll be able to choose whether or not to pay for more coaching if the need arises.\nThe program’s highlight\u003es include:\nThousands of course materials and articles are available to you. The entire course is available for free. Additional assistance is available at a low cost. You can also continue to learn other languages like CSS, HTML5, CSS3, JavaScript, and others. Everyone is welcome to sign up for a free subscription. Course Link\nSkill Share HTML is a simple language to learn, which explains why there are so many short courses available. However, the Hand-Code Your First Website course, in my opinion, is great for someone who needs a crash education in HTML. The instructor will not only provide you with the abilities to write your own website from scratch in under two hours, but he will do so utilising tools that are widely used in the development world.\nTo develop a simple one-page website for your favourite cartoon character, you’ll use Sublime text editor and Google Chrome, which the instructor claims can help you discover HTML mistakes and even perform some design work in the browser itself. He’ll also devote some time to getting you acquainted with Git, GitHub, and the GitHub Desktop programme, which you’ll use to host your code and publish your website.\nSure, the course will teach you how to construct CSS rulesets and expose you to a few key HTML components. The fact that it teaches you how to approach a programming project sets it distinct from other similar services. The instructor will emphasise and provide advice on how to plan a website, as well as a brief overview of debugging HTML code and some of the more prevalent sorts of errors.\nCourse link\nFinishing Up The courses listed above were chosen by our panel of experts as among the best HTML courses to assist registrants obtain the new knowledge and skills they need to become experts in HTML and other languages. These courses are available online, and you can access them at any time. The websites listed are also excellent resources for learning HTML.\n","description":"Want to learn HTML 5 and CSS 3 in 2021? Here are the 10 best online courses you can join to learn HTML and CSS in depth.","tags":["review"],"title":"Top HTML \u0026 CSS Courses - Learn HTML \u0026 CSS Online","uri":"/collections/reviews/best-html-online-course/"},{"content":"A Comprehensive Guide to React Native Navigation Stack: Best Practices and Tips React Native Navigation Stack is an essential component for building mobile applications using React Native. It provides a flexible and powerful navigation solution that can help you create intuitive and easy-to-use interfaces for your users.\nIn this comprehensive guide, we will cover everything you need to know about React Native Navigation Stack, including its best practices and tips. By the end of this article, you will have a solid understanding of how to use this navigation solution in your projects.\nReact Native Navigation React Native Navigation is a popular alternative to React Navigation. It’s a module that is dependent on and designed to be used with React Native. React Native Navigation differs slightly in that it directly uses native navigation APIs on iOS and Android, which allows for a more native look and feel.\nOne of the key benefits of React Native Navigation Stack is that it provides a consistent and predictable user experience. Users can easily navigate between screens, and the navigation is smooth and seamless. Additionally, it allows you to customize the navigation bar and add animations to make your app look and feel more polished.\nReact Native Stack Navigation Your best friend is Stack Navigation. This is how you may make stacks of pages that allow you to automatically return to the previous one. You’ll wind up with numerous layers throughout your project, and the ‘Back’ option is a lifesaver in terms of not having to programme it manually each time.\nGetting Started Before we begain let’s install the required dependencies for working with react native navigation\nnpm install @react-navigation/native @react-navigation/native-stack After install dependencies we will install react-native-screens and react-native-safe-area-context.\nexpo install react-native-screens react-native-safe-area-context # if you are using expo client npm install react-native-screens react-native-safe-area-context # if you are using react-native directly After install above packages we will start building our app.\nWe will create two screen Home Screen which the use will see whenever he/she opens the app and another Detail Screen which display information about the user.\nFirst let’s import our necessary libraries:\nimport React from \"react\"; import { TouchableOpacity, StyleSheet, Text, View } from \"react-native\"; import { NavigationContainer } from \"@react-navigation/native\"; import { createNativeStackNavigator } from \"@react-navigation/native-stack\"; import { Image } from \"react-native\"; Creating Native Stack Navigator createNativeStackNavigator is a function that returns an object with two properties: Screen and Navigator. Both are React components that are used to configure the navigation. To describe route configuration, the Navigator should have Screen components as its children.\nfunction Home() { // TODO } function Detail() { // TODO } const Stack = createNativeStackNavigator(); function App() { //TODO } After creating stack navigator object, let’s add different screen on top of our Stack:\nfunction Home() { // TODO } function Detail() { // TODO } const Stack = createNativeStackNavigator(); function App() { \u003cNavigationContainer\u003e \u003cStack.Navigator\u003e \u003cStack.Screen name=\"Home Screen\" component={Home} /\u003e \u003cStack.Screen name=\"Detail Screen\" component={Details} /\u003e \u003c/Stack.Navigator\u003e \u003c/NavigationContainer\u003e; } The NavigationContainer component handles and stores our navigation tree and navigation state. This component must encompass the structure of all navigators. This component is normally rendered at the root of our project, which is often the component exported from App.js.\nNow let’s create our Home Screen and Detail Screen:\nHome Screen function Home({ navigation }) { return ( \u003cView style={styles.container}\u003e \u003cImage source={require(\"./assets/images/cover1.png\")} style={{ paddingTop: 100, width: 300, height: 300, borderRadius: 300 / 2, }} /\u003e \u003cText style={{ fontSize: 25, paddingTop: 50 }}\u003eAnotherTechs\u003c/Text\u003e \u003cView style={{ paddingTop: 50 }}\u003e \u003cTouchableOpacity style={styles.Button} onPress={() =\u003e navigation.navigate(\"Detail Screen\")} \u003e \u003cText style={{ fontSize: 20, textAlign: \"center\" }}\u003e {\" \"} Go to Detail {\"\"} \u003c/Text\u003e \u003c/TouchableOpacity\u003e \u003c/View\u003e \u003c/View\u003e ); } const styles = StyleSheet.create({ container: { flex: 1, backgroundColor: \"#fff\", paddingTop: 100, alignItems: \"center\", }, Button: { marginTop: 10, paddingTop: 15, paddingBottom: 15, paddingLeft: 15, paddingRight: 15, marginLeft: 30, marginRight: 30, backgroundColor: \"#d4a56e\", borderRadius: 10, borderWidth: 1, borderColor: \"#fff\", }, }); navigation props: Every screen component in the native stack navigator receives the navigation prop. navigate(‘Details’): we then call navigate function with the name of the route we will like to move to. OUTPUT:\nDetail Screen function Details({ navigation }) { return ( \u003cView style={{ flex: 1, alignItems: \"center\", paddingTop: 50 }}\u003e \u003cImage source={require(\"./assets/images/pexels-mustafa-ezz-925043.jpg\")} style={{ width: 300, height: 300, borderRadius: 300 / 2 }} /\u003e \u003cView style={{ flex: 1, justifyContent: \"center\" }}\u003e \u003cText style={{ textAlign: \"center\", padding: 3, margin: 4 }}\u003e Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam laoreet metus faucibus orci vestibulum suscipit. Curabitur non bibendum ipsum, quis tristique sapien. Proin tempor tortor lectus, id dapibus nisi commodo vitae. Vestibulum lorem tortor, vulputate sit amet fermentum vehicula, porttitor vel libero. Praesent ipsum turpis, consequat non malesuada in, pretium a quam. Nunc fermentum a dolor in semper. Suspendisse venenatis nulla sed diam egestas rutrum. Suspendisse ut eleifend sapien. Suspendisse bibendum, quam in efficitur dapibus, dolor lectus consectetur arcu, vel efficitur sapien ipsum et augue. Vestibulum in tincidunt orci, a sagittis odio. Curabitur consectetur mi mauris, vel ornare arcu pellentesque vel. Quisque sagittis, erat nec vehicula facilisis, sem est aliquet dolor, sit amet iaculis nunc quam eu purus. Curabitur sit amet massa ultrices, ullamcorper risus vitae, accumsan enim. Sed et nisi eget nibh mattis auctor. \u003c/Text\u003e \u003c/View\u003e \u003c/View\u003e ); } When it is feasible to return from the active screen, the native stack navigator’s header will automatically include a back button (if there is only one screen in the navigation stack, there is nothing that you can go back to, and so there is no back button).\nOUTPUT:\nFinal Thoughts In conclusion, React Native Navigation Stack is a powerful and flexible navigation solution that can help you create intuitive and easy-to-use interfaces for your users. By following the best practices and tips outlined in this article, you can ensure that your app’s navigation is smooth, predictable, and visually appealing. So, what are you waiting for? Start implementing React Native Navigation Stack in your projects today!\nReferences React Native: The official website of React Native, a JavaScript framework used for building native mobile applications. React Navigation: A popular routing and navigation library for React Native apps. React Native Animations: A guide to using animations in React Native apps. React Native UI Components: A collection of customizable UI components for React Native. Expo: A free and open-source platform for building and deploying React Native apps. ","description":"Learn the best practices and tips for implementing React Native Navigation Stack in your projects. Our comprehensive guide covers everything you need to know about this powerful navigation solution.","tags":["programming","react native"],"title":"A Comprehensive Guide to React Native Navigation Stack- Best Practices and Tips","uri":"/collections/programming/react-native/react-native-navigation-stack/"},{"content":"Kaggle Data Science Projects for Beginners Introduction Kaggle has established itself as one of the most essential stepping stones for students and professionals interested in Data Science these days.\nKaggle features a wealth of online resources to aid in the learning of Data Science. Thousands of datasets, data science competitions, code submissions on datasets, community discussion, and even beginner-friendly courses are all available. In addition, the user receives a public user profile that monitors and displays all of the user’s efforts and accomplishments.\nThe user profile displays who the user follows, who follows the user, any code the user has written, any datasets the user has created, and other information. There are a variety of rating methods as well. The kaggle profile is a great method to develop shareable online projects and showcase your skills. Your kaggle profile serves as a tool to display your Data Science skills, similar to how your HackerEarth or Code Chef profile shows your competitive coding skills.\nTo create a solid kaggle profile, one must work with data and create high-quality Python or R notebooks in the form of projects, all while telling a storey with the data.\nOn Kaggle Notebooks, users can add various data visualisations, write markdown, and train models. There is a lot that can be done with them. The best part of Kaggle Notebooks is that they don’t require the user to install Python or R on their computer in order to use them. Almost any important library can be imported directly. TPUs are also available for free on Kaggle. Tensor Processing Units (TPUs) are deep learning-specific hardware accelerators. They’re available in Tensorflow 2.1 via the Keras high-level API and, at a lower level, in models with a custom training loop.\nWorking with Datasets on Kaggle is thus quite simple and convenient, and all beginners should give it a go in order to gain some expertise and information.\nHere are some datasets that any newbie can use to create amazing projects:\nSpotify Music Dataset This is a dataset that holds a lot of promise. This dataset is useful for a recommendation engine, trend analysis, popularity prediction, and unsupervised clustering, as indicated in the tasks.\nDiscover Influential Artists in a Variety of Genres.\nCreate an artist recommendation engine that is based on content.\nPredict the popularity of a song based on its other characteristics.\nExamine music trends over the course of a century.\nDifferent genres are grouped together based on auditory characteristics.\nDataset\nAirbnb By browsing at an Airbnb map of New York City, you may compare pricing by region.\nPredict the cost of an Airbnb rental in New York City.\nExamine whether there is a price difference between room types.\nDataset\nStudent Performance in Exam This information is based on demographics of the population. The data includes information such as the student’s food type, test preparation level, parental education level, and pupils’ math, reading, and writing results.\nVarious types of regression and classification issues can be solved using the data. It can also be used to determine which elements contribute to improved exam results. Overall, it should be fun to work on.\nDataset\nVaccine Tweets Examine tweets about the new Pfizer and BioNTech vaccine to learn more about the vaccine’s reception, subjects of discussion, and favourable and negative feelings.\nDataset\nDogs and Cats Images The classic dataset for categorising dogs and cats. There are several Dog and Cat photos available for training models and making predictions. For students interested in Image Processing or Computer Vision, this dataset is a must-have. You also get to see a lot of lovely cat and dog pictures.\nDataset\nNetflix Movies and TV Shows Is there anyone who doesn’t enjoy Netflix? Netflix TV episodes and movies are included in this kaggle dataset. This dataset can be used to develop a high-quality Exploratory Data Analysis project.\nThis dataset can be used to determine what type of material is created in which country, identify comparable content based on the description, and perform a variety of other activities.\nDataset\n","description":"Working with datasets on Kaggle is simple and convenient, and all beginners should give it a go to gain some experience and information.","tags":["programming","Machine learning"],"title":"Kaggle Data Science Projects for Beginners","uri":"/collections/programming/machine-learning/kaggle-data-science-projects/"},{"content":"Best Machine Learning Projects As Artificial Intelligence (AI) continues to advance at a breakneck pace in 2021, mastering Machine Learning (ML) will become increasingly vital for all players in the field. This is because AI and machine learning are mutually beneficial. As a result, if you’re a beginner, the best thing you can do is work on Machine Learning projects.\nWhile textbooks and study resources will provide you with all of the information you want about Machine Learning, you will never truly understand the subject until you devote your time in real-world practical experiments — Machine Learning projects.\nYou will not only be able to test your skills and shortcomings while you work on machine learning project ideas, but you will also receive exposure that will help you further your career.\nIn this blog, you’ll learn about 10 fun machine learning project ideas for novices and intermediate who want to gain some hands-on experience with the technology.\n1. Cartoonify Image Convert pictures into cartoons. Yes, CARTOONIFYING the pictures is the goal of this machine learning research.\nAs a result, you’ll create a Python programme that uses machine learning tools to turn an image into a cartoon.\nSource Code\n2. Emojify - Creating you own emoji using Machine Learning The goal of this machine learning research is to categorise and map human face emotions to emojis.\nTo identify face emotions, you’ll use a convolutional neural network.\nThen you’ll associate those feelings with the appropriate emojis or avatars.\nSource Code\n3. Fake News Detection Project This is one of the best machine learning project ideas for beginners, especially given how quickly bogus news is spreading. Fake news has an uncanny ability to spread like wildfire. And, with social media increasingly dominating our lives, it’s more important than ever to tell the difference between false news and actual news events. Machine Learning can aid in this situation. Facebook already employs artificial intelligence to remove false and spammy content from consumers’ news feeds.\nThis machine learning research seeks to use NLP (Natural Language Processing) techniques to identify false news and misleading articles that come from untrustworthy sources. You may also create a model that can distinguish between authentic and false news using the standard text classification technique. In the latter technique, you may collect datasets for both real and false news and use the Naive Bayes classifier to construct an ML model that can categorise a piece of news as fraudulent or legitimate based on the words and phrases it contains.\nSource Code\n4. Music Genre Classification Project The goal of this python machine learning project is to create a machine learning project that can automatically categorise various musical genres based on audio.\nYou must categorise these audio recordings based on their low-level frequency and temporal domain characteristics.\nSource Code\n5. Bitcoin Price Prediction Project The bitcoin price forecaster is a helpful tool. The use of blockchain technology is growing, and there are a growing number of digital currencies.\nThis project will assist you in predicting the price of bitcoin based on historical data.\nSource Code\n6. Sign Language Recognition Machine Learning Project There has been a lot of study done to aid deaf and dumb individuals.\nYou construct a sign detector that detects sign language in this sign language recognition project.\nThis may be extremely useful in talking with others for the deaf and dumb.\nSource Code\n7. Facial Recognition Machine Learning Project to dectect mood and recommend songs People listen to music based on their present mood and sentiments, which is a well-known truth. So, why not make an app that can identify a person’s mood based on their facial expressions and then propose music to them? You’ll utilise computer vision elements and techniques to do this.\nThe objective is to develop a model that can successfully use computer vision to assist computers interpret pictures and videos at a high level.\nSource Code\n8. Speech Emotion Recognition Machine Learning Project This is one of the most impressive machine learning projects I’ve ever seen. Audio data is used in the spoken emotion recognition system.\nIt takes a segment of speech as input and determines the speaker’s emotional state.\nYou can recognise many emotions such as happiness, sadness, surprise, anger, and so on. Identifying client emotions during a call to a call centre might be aided by this effort.\nSource Code\n9. Sentiment analysis Machine Learning Project This is an intriguing machine learning project concept. Although most of us use social media platforms to broadcast our personal thoughts and ideas to the world, comprehending the “sentiments” underlying social media posts is one of the most difficult tasks.\nThis is also a fantastic concept for your next machine learning project!\nUser-generated material abounds on social media platforms. It would be considerably easier for businesses to understand consumer behaviour if they could develop a machine learning system that could evaluate the emotion behind texts or posts. As a result, they would be able to improve their client service, allowing for maximum customer happiness.\nTo get started with your sentiment analysis machine learning project, you may try mining data from Twitter or Reddit. This might be one of the few deep learning projects that can benefit you in other ways as well.\nSource Code\n10. Object Detection Machine Learning Project This is one of the more intriguing machine learning projects I’ve ever worked on. Deep Neural Networks (DNNs) should be your first option for picture categorization. While DNNs are currently utilised in a variety of real-world picture categorization applications, this machine learning research seeks to take it to the next level.\nYou will use DNNs to address the problem of object detection in this machine learning project. You’ll need to create a model that can categorise items as well as properly locate objects of various types. Object detection will be approached as a regression issue using object bounding box masks. You’ll also create a multi-scale inference process that can produce high-resolution item detections at a low cost.\nSource Code\nConclusion A complete collection of machine learning project ideas may be found here. Machine learning is still in its infancy throughout the world. There are many tasks to complete and many areas to enhance. Systems that assist business grow better, quicker, and more lucrative with clever minds and keen ideas. You must have hands-on experience with such machine learning projects if you want to thrive in Machine Learning.\n","description":"Check out beginner, fresher, and expert machine learning projects with source code to acquire practical experience and prepare for a career.","tags":["programming","Machine learning"],"title":"Best Machine Learning Projects For Beginners with Source Code [2021]","uri":"/collections/programming/machine-learning/machine-learning-projects/"},{"content":"Nodejs- forEach loop cheatsheet You can use looping to run through each item in an array and customise and output each one as you see fit. Loops are a crucial tool for rendering arrays in JavaScript, just as they are in any other programming language.\nLet’s dive deeper into the many ways you can use foreach loops in JavaScript/Nodejs with the help of some practical examples.\nforEach Loops in Nodejs/Javascript It’s a sophisticated looping structure for traversing elements in a collection. It runs till the collection set is finished.\nThe difference between a for loop and a foreach loop is that a for loop works with variables, whereas a foreach loop works with an object.\nIn most cases, foreach loops do not have an explicit counter.\nAlthough forEach in Nodejs/JavaScript cannot decrement, it is typically less verbose than the raw for loop. It operates by selecting items one by one without remembering the previous one.\nThe general syntax of JavaScript forEach is as follows:\n... array.forEach((element) =\u003e { action; }); ... For Example:\n... let array = [1, 3, 5, 6]; array.forEach(elem =\u003e { console.log(elem) }); ... Output: 1 3 5 6 Applying External function supplied as parameter to forEach on an array of items In this example, we’ll use forEach to apply to each element of the array. We also define the function separately and pass it as an input to forEach.\n... let elements = [1,2,3,4,5]; let myFunc = function (elem) { console.log(elem * 2); }; elements.forEach(myFunc); ... Output: 1 4 6 8 10 Using Lambda fucntion in forEach loop In the below example we will show how to use Nodejs/Javascript Lambda function in forEach loop.\n... let elements = [1,2,3,4,5,6] elements.forEach(elem =\u003e (console.log(elem*2))) ... Output: 1 4 6 8 10 Using forEach on Array with access to Element, Index and Array In each iteration of this example, we will access index, array, and element.\n... let elements = [\"dog\",\"cat\",\"fish\",\"horse\"] let myFunc = function(elem, index, array) { console.log(index + ' : ' + elem + ' - ' + array[index]) } elements.forEach(myFunc) ... Output: 0 : dog - dog 1 : cat - cat 2 : fish - fish 3 : horse - horse Although we’ve covered the JavaScript/Nodejs forEach looping methods here, understanding the fundamentals of iteration in programming allows you to employ them in your applications with confidence and flexibility. However, the majority of these JavaScript/Nodejs loops operate in the same way, with just minor changes in their overall structure and syntax.\nMost client-side array rendering, on the other hand, is based on loops. So feel free to experiment with various looping strategies. Using them with more complex arrays, for example, will help you comprehend JavaScript/Nodejs loops better.\n","description":"Node.js forEach executes the supplied function for each element using Node.js forEach.  With the help of examples, we will learn how to use the forEach statement in this article.","tags":["programming","javascript"],"title":"Nodejs- forEach loop cheatsheet","uri":"/collections/programming/javascript/nodejs-foreach-loop/"},{"content":"A Comprehensive Guide to Auto-Encoders in Neural Networks\" In our last section we have seen what is ResNet and how to implement it. In this article we will look at AutoEncoders and how to implement it in PyTorch.\nIntroduction Auto-encoders are a type of nepytorch autoencoder tutorial,ural network that have gained popularity in recent years due to their ability to learn efficient representations of data. They are used in a variety of applications such as image and speech recognition, and anomaly detection.\nWhat are AutoEncoder ? Well according to wikipedia “It is an artificial neural network used to learn efficient data encoding”. Basically autoencoder compreses the data or tu put it in other word it transform the data of heigher dimenssion to lower dimenssion by learninh how to ignore noises. The encoder part in an autoencoder learns how to compress the data into lower domenssion, while the decoder part learns how to reconstruct original data from the encoded data. Autoencoder are used in deepfake where the idea is we train two autoencoder both on different kind of datasets and then we use first autoencoder’s encoder to encode the image and second autoencoder’s decoder to decode the encoded image. Here is an example of deepfake. Beside this autoencoder are also used in GAN-Network for generating image,image compression,image dignosing etc.\nHow do Auto-Encoders Work? Auto-encoders work by learning a compressed representation of the input data that captures its essential features. This compressed representation can be used for tasks such as data visualization, feature extraction, and anomaly detection.\nThe training of an auto-encoder involves minimizing a loss function that measures the difference between the input and the reconstructed output. The most common loss function used for auto-encoders is the mean squared error (MSE) loss.\nApplications of Auto-Encoders Auto-encoders have a wide range of applications in various fields. They are used for image and speech recognition, natural language processing, and anomaly detection.\nIn image and speech recognition, auto-encoders are used to learn compact representations of images and audio signals that can be used for classification and other tasks. In natural language processing, auto-encoders are used to learn representations of words and sentences that can be used for tasks such as sentiment analysis and machine translation.\nAutoEncoder Components Encoder: Here the model learn how to compress or reduce the input dimenssions of the input data to the encoded representation or lower representation.\nDecode: Here the model learn how to reconstruct the encoded representaion to it’s original form or close to it’s original form.\nBottleneck: It is the compressed representation of the input data. This is the lowest possible dimenssion of the input data.\nReconstruction Loss: This is the method which tell us how well the decoder perfromed in reconstructing data and how close the ouput is to the original data\nArchitecture of AutoEncoder The network architecture for autoencoders can vary between a simple FeedForward network, LSTM network or Convolutional Neural Network depending on the use case.\nImplementation AutoEncoder Let’s now implement a basic autoencoder. For the dataset we will be using STL10. First let’s import necessary modules. Create a new file name main.py and write the following code :\n#main.py #! /usr/bin/env python import torch import numpy as np import torchvision import torch.nn as nn from tqdm import tqdm from AutoEncoder import AutoEncoder ## Our AutoEncoder Model import matplotlib.pyplot as plt import torchvision.transforms as transforms from torchvision.utils import save_image __DEBUG__ = True LOAD = True PATH = \"./autoencoder.pth\" DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") TRANSFORM = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]) EPOCHS = 50 BATCH_SIZE = 4 Downloading and transforming dataset #main.py def get_dataset(train = True): trainset = torchvision.datasets.STL10(root = '../dataset', download = True,transform= TRANSFORM) trainLoader = torch.utils.data.DataLoader(trainset,batch_size = BATCH_SIZE, shuffle = True,num_workers = 8) return trainLoader def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() def showRandomImaged(train): # get some random training images dataiter = iter(train) images, labels = dataiter.next() # show images print(images.shape) imshow(torchvision.utils.make_grid(images)) The get_dataset method will download and tranform our data for our model. It take one argument train is set to true it will give us trainning dataset and if it is false it will give us testing dataset. This method return an DataLoader object which is used in trainning.\nNow let’s write our AutoEncoder. Open new file name AutoEncoder.py and write the following code:\nAutoEncoder #AutoEncoder.py # Encoder class Encoder(nn.Module): def __init__(self): super(Encoder,self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(3, 16, kernel_size=5,stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(16), nn.Conv2d(16,32,kernel_size = 5, stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(32)) self.layer2 = nn.Sequential( nn.Conv2d(32, 64, kernel_size=5,stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(64), nn.Conv2d(64,128,kernel_size = 5, stride = 1, padding = 2), nn.ReLU()) self.fc1 = nn.Linear(32*32*128,1000) self.fc2 = nn.Linear(1000,100) def forward(self,x): x = self.layer1(x) x = self.layer2(x) x = x.view(x.size(0),-1) x = self.fc1(x) x = self.fc2(x) return x In my previous article I have explained why we import nn.Module and use super method. Now let jump to our layer1 which consists of two conv2d layers followed by ReLU activation function and BatchNormalization. self.layer1 takes 3 channel as input and gives out 32 channel as output.\nSimilarly self.layer2 takes 32 channel as input and give out 128 channel as ouput.\nNote: Here dimenssion of image is not being changed\nNext we create two fully connected layer layers self.fc1 and self.fc2.\nIn forward method we define who our data is followed first we pass the data to layer1 followed by layer2. After that we flatten our 2D data to 1D vector using x.view method. Now our data is ready to pass through fully connected layer fc1 and fc2\nNow let’s write our Decoder:\nclass Decoder(nn.Module): def __init__(self): super(Decoder,self).__init__() self.fc1 = nn.Linear(100,1000) self.fc2 = nn.Linear(1000,32*32*128) self.layer1 = nn.Sequential( nn.ConvTranspose2d(128, 64, kernel_size=5,stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(64), nn.ConvTranspose2d(64,32,kernel_size = 5, stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(32)) self.layer2 = nn.Sequential( nn.ConvTranspose2d(32,16, kernel_size=5,stride = 1, padding = 2), nn.ReLU(), nn.BatchNorm2d(16), nn.ConvTranspose2d(16,3,kernel_size = 5, stride = 1, padding = 2), nn.ReLU()) def forward(self,x): x = self.fc1(x) x = self.fc2(x) x = x.view(x.size(0), 128, 32, 32) x = self.layer1(x) x = self.layer2(x) return x As you can clearly see our Decoder is opposite to the Encoder. Here first we have two fully connected layer fc1 and fc2. The output of fc2 is feeded to layer1 followed by layer2 which recontruct our original image of 32x32x3.\nLet’s now combine this to model:\n#AutoEncoder.py #AutoEncoder class AutoEncoder(nn.Module): def __init__(self): super(AutoEncoder,self).__init__() self.encoder = Encoder() self.decoder = Decoder() def forward(self,x): x = self.encoder(x) x = self.decoder(x) return x Training #main.py if __name__ == '__main__': if __DEBUG__ == True: print(DEVICE) train = get_dataset() if __DEBUG__ == True: print(\"Showing Random images from dataset\") showRandomImaged(train) model = AutoEncoder().cuda() if torch.cuda.is_available() else AutoEncoder() if __DEBUG__ == True: print(model) criterian = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5) if LOAD == True: model.load_state_dict(torch.load(PATH)) for epoch in range(EPOCHS): for i,(images,_) in enumerate(train): images = images.to(DEVICE) out = model(images) loss = criterian(out,images) optimizer.zero_grad() loss.backward() optimizer.step() ## LOG print(f\"epoch {epoch}/{EPOCHS}\\nLoss : {loss.data}\") if __DEBUG__ == True: if i % 10 == 0: out = out / 2 + 0.5 # unnormalize img_path = \"debug_img\" + str(i) + \".png\" save_image(out,img_path) #SAVING torch.save(model.state_dict(),PATH) For trainning we have use MSELoss() and Adam optimizer. Next we train our model till 50 epochs. Then we iterate to each of the trainning batches and pass this batches to our model. Then we calculate MSELoss(). Now before backpropagation we make our gradient to be zero using optimzer.zero_grad() method. Then we call backword method on our loss variable to perfrom back-propogation. After gradient has been cacluate we optimze our model with optimizer.step() method.\nImage compression using AutoEncoder One popular application of autoencoders is image compression. In this example, we will train an autoencoder to compress and decompress images from the CIFAR-10 dataset.\nimport torch import torch.nn as nn import torch.optim as optim import torchvision.transforms as transforms import torchvision.datasets as datasets # define the autoencoder model class Autoencoder(nn.Module): def __init__(self): super(Autoencoder, self).__init__() # encoder self.enc1 = nn.Linear(in_features=28*28, out_features=128) self.enc2 = nn.Linear(in_features=128, out_features=64) self.enc3 = nn.Linear(in_features=64, out_features=32) # decoder self.dec1 = nn.Linear(in_features=32, out_features=64) self.dec2 = nn.Linear(in_features=64, out_features=128) self.dec3 = nn.Linear(in_features=128, out_features=28*28) def forward(self, x): x = x.view(x.size(0), -1) # flatten the input x = torch.relu(self.enc1(x)) x = torch.relu(self.enc2(x)) x = torch.relu(self.enc3(x)) x = torch.relu(self.dec1(x)) x = torch.relu(self.dec2(x)) x = torch.sigmoid(self.dec3(x)) # use sigmoid to restrict output to [0, 1] x = x.view(x.size(0), 1, 28, 28) # reshape the output back to image shape return x # load the MNIST dataset train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True) # instantiate the model and optimizer autoencoder = Autoencoder() optimizer = optim.Adam(autoencoder.parameters(), lr=0.001) # define the loss function criterion = nn.MSELoss() # train the model num_epochs = 10 for epoch in range(num_epochs): for data in train_loader: img, _ = data recon = autoencoder(img) loss = criterion(recon, img) optimizer.zero_grad() loss.backward() optimizer.step() print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # test the model by encoding and decoding a test image test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) with torch.no_grad(): for data in test_loader: img, _ = data recon = autoencoder(img) break # visualize the original image and the reconstructed image import matplotlib.pyplot as plt fig, axs = plt.subplots(1, 2, figsize=(10, 5)) axs[0].imshow(img.squeeze().numpy(), cmap='gray') axs[0].set_title('Original Image') axs[1].imshow(recon.squeeze().numpy(), cmap='gray') axs[1].set_title('Reconstructed Image') plt.show() In this example, we define a simple autoencoder model with three encoding layers and three decoding layers. We use the MNIST dataset and transform the images to tensors. We then train the autoencoder using the mean squared error loss function and the Adam optimizer. Finally, we test the model by encoding and decoding a test image and visualizing the original and reconstructed images.\nConclusion Auto-encoders are a powerful tool in the field of machine learning and neural networks. They can be used for a variety of applications, from image and speech recognition to anomaly detection. Implementing auto-encoders in your own projects may seem intimidating at first, but with the right approach, it can be a straightforward process. With this comprehensive guide, you should now have a good understanding of what auto-encoders are, how they work, and how to implement them in your own projects.\n","description":"Learn all about auto-encoders in neural networks with this comprehensive guide. Discover how they work, their applications, and how to implement them in your own projects.","tags":["programming","Neural Network"],"title":"A Comprehensive Guide to Auto-Encoders in Neural Networks","uri":"/collections/programming/neural-network/auto-encoders/"},{"content":"What is Pandas Data Profiling ? The Pandas library has a large number of functions. It aids in data manipulation and offers a diverse set of features for practically any activity. The pandas describe() function is a popular Pandas function. It provides a descriptive statistical overview of all the dataset’s features to the user. Even though it is useful for understanding data, it lacks numerous capabilities.\nPandas profiling is the answer to this problem. It let’s you create reports for your dataset that include a range of features and adjustments. In this post, we’ll examine at the functionality of this library, as well as some of the more advanced use cases and connectors that may help you create stunning data frame reports!\nInstalling Pandas Data Profiling in Jupyter notebook Pandas profiling, like all other python packages, is quickly installed using the pip or conda package manager:\npip:\npip install pandas-profiling conda:\nconda install -c conda-forge pandas-profiling Setting up Pandas Data Profiling in jupyter notebook Now we’ll look at how to use the pandas profiling library and create a report from the data frames. Let’s start by importing a dataset for which we’ll be generating reports. In our example we will be using iris dataset from sklearn.\nfrom sklearn.datasets import load_iris import pandas as pd import numpy as np data = load_iris() df = pd.DataFrame(np.column_stack((data.data, data.target)), columns = data.feature_names+['target']) df.describe() Now let’s start with pandas profiling library:\nfrom pandas_profiling import ProfileReport To begin profiling a dataframe, you can use one of two methods:\nYou can use the .profile_report() method on a pandas dataframe. This function is not part of the pandas API, but it is automatically added to dataframe objects after the profiling library is loaded.\nOr by giving the dataframe object to the profiling function and then executing the function object created.\nYou will get the same outcome report regardless of the method you use. To build the report for the iris dataset imported, I’m using the second technique.\nprofile = ProfileReport(df) profile Profiling Report Now let’s explore each section of profiling one by one:\n1. Overview Overview, Warnings, and Reproduction are the three tabs in this section.\nOverall statistics are shown in the Overview. This contains the number of variables (dataframe features or columns), the number of observations (dataframe rows), the number of missing cells, the percentage of missing cells, the number of duplicate rows, and the total size in memory.\nThe warnings tab offers a variety of warnings, including those about cardinality, correlation with other variables, missing values, zeroes, skewness of the variables, and more.\nThe reproduction tab simply displays information related to the report generation. It shows the start and ends the time of the analysis, the time taken to generate the report, the software version of pandas profiling, and a configuration download option.\nThe reproduction tab just displays details on the report creation process. It shows the start and end times of the analysis, the time it took to construct the report, the version of the pandas profiling software, and a configuration download option.\n2. Variable This section of the report examines all of the dataset’s variables, columns, and characteristics in detail. Depending on the data type of variable, the information given differs. Let’s have a look at it in more detail.\nNumeric Variables The unique values, missing values, min-max, mean, and negative values count are all included in the numeric data type features. In the form of a Histogram, you can also receive small representation values.\nWhen the toggle button is hit, the Statistics, Histogram, Common values, and Extreme values tab expands.\nThe following information can be found on the statistics tab:\nQuantile statistics include min-max, percentiles, median, range, and IQR (Inter Quartile range)\nDescriptive statistics include standard deviation, coefficient of variance, kurtosis, mean, skewness, variance, and monotonicity.\nThe histogram tab displays the frequency of variables or the distribution of numeric data. On the common values page, the variable value_counts are displayed as both counts and percent frequency.\nString Variables String type variables provide distinct (unique) values, distinct percentages, missing, missing percentages, memory size, and a horizontal bar presentation of all the unique values with count presentation.\nWhen the toggle button is hit, the Overview, Categories, Words, and Characters tabs are expanded.\nThe Overview tab displays the maximum and minimum median mean lengths for string type values, as well as total characters, different characters, separate categories, unique, and a sample from the collection.\nThe categories tab shows a histogram and, on occasion, a pie chart of the feature’s value counts. The value, count, and % frequency are all listed in the table.\nIn terms of how the data is presented in tabular and histogram format, the words and characters tab is similar to the categories tab, but it may go much further into the lower case, higher case, punctuation, and special characters categories.\n3. Correlations The term “correlation” refers to the degree to which two variables move in lockstep with one another. Pearson’s r, Spearman’s ρ, Kendall’s τ, Phik (φk), and Cramér’s V (φc) are the five forms of correlation coefficients available in the pandas profiling report.\n3. Missing values The visualisations for the missing variables in the dataset are also included in the report. There are three sorts of plots available: count, matrix, and dendrogram. The count plot is a simple bar graph with column names on the x-axis and the length of the bar representing the number of values present (without null values). The matrix and the dendrogram are similar.\n5. Sample Display’s first and last 10 rows of the dataset\nSaving Pandas Profiling Report You’ve shown how to create dataframe reports using just one line of code or function, as well as all of the report’s capabilities. You might want to save this analysis as an external file so that you can combine it with other programmes or publish it online.\nThis report can be saved in –\nJSON format or HTML format For any of the formats, the save function remains the same; simply change the file extension when saving. To save the report, use the profile object’s “.to file()” function:\nprofile.to_file(\"iris_profile_report.html\") profile.to_file(\"iris_profile_report.json\") Widget in Jupyter Notebooko The HTML will only be rendered in the code cell when you run panda profiling in your Jupyter notebooks. This causes the user’s experience to be disrupted. You may make it act as a simple widget that presents information in a condensed style. To do so, simply call .to_widgets() on your profile object:\n","description":"Pandas profiling allows you to build a report for a dataset with a variety of features and modifications.","tags":["programming","python"],"title":"Everything you need to know about Python Pandas Data Profiling","uri":"/collections/programming/python/pandas-data-profiling/"},{"content":"Best SQL online courses To enroll Whether you’re a beginner or an established user, the top online SQL courses make it straightforward and uncomplicated to increase your understanding in Structured Query Language (SQL).\nThe Structured Query Language, or SQL as it is more often known, is a database query language. While opinions differ on how to pronounce SQL, everyone seems to agree on its value, particularly in this age of big data and corporate analytics.\nWith businesses of all kinds churning out massive amounts of data, there’s a growing demand for talents that can not only collect and store data, but also analyse it so that educated decisions can be made. SQL is a route to data science positions and one of the most important tools in a data scientist’s toolbox.\nSQL also works with a variety of relational database management systems, including MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and others. And this is true regardless of whether your database is hosted in the cloud or on-premises.\nSo, whether you’re a seasoned developer eager to learn something new or a SQL newbie, we’ve compiled a list of the top online courses to help you grasp SQL.\nSkillShare Check out SkillShare’s Master SQL Database Queries in Just 90 Minutes if you don’t have much time and want a quick crash course on SQL. The presentation is just over an hour long, and the presenter makes the most of it by covering everything from single table queries to joins and subqueries. He gives a cursory look at how to create tables and update data within them before moving on to getting information. The courses are well-explained, and the problems at the end of each session are fairly useful, especially because he takes the time to explain all of the solutions, which is quite helpful.\nThe instructor executes the SQL queries using Firefox and the SQLite Manager add-on during the lesson. However, because the add-on is no longer compatible with current Firefox versions, the instructor now recommends DB Browser for SQLite, which is very comparable.\nReasons to purchase\nSuccinct fast-paced Disadvantages\nThere are no subtitles or transcripts available. Udemy The Ultimate MySQL Bootcamp course on Udemy is a great place to start if you’re new to databases. The course is designed for total novices and includes over 20 hours of video. The course is divided into approximately 300 lectures that cover a wide range of topics in a reasonable amount of time. The instructor gently guides you through the process of installing the MySQL database and creating an Instagram-like database, which you’ll then use to perform some real-world data queries.\nPreviously, the instructor recommended and used the online Cloud9 IDE, but now advises students to use the Goorm.io IDE, which is very comparable. He also demonstrates how to install MySQL on Windows and Mac OS X, but he advises that you do so at the end of the course.\nAfter teaching you how to write SQL, the instructor guides you through the creation of a small web application. He uses Node.JS and the Express web development platform instead of the standard MySQL and PHP combination. While he does provide a crash course for individuals who have never worked with Node.JS before, keep in mind that this isn’t the course’s main focus.\nReasons to purchase\nAssumes no prior knowledge Covers a wide range of topics Is easily accessible Coursera edX Data is all around us in business, and it has a lot of economic worth. Take a look at the Introduction to Data Analytics for Managers course if you want to develop skills that will help you do this. The course is for managers who see the value of data analytics in the workplace but lack the skills to put it into practise.\nThe course, taught by the University of Michigan’s Ross School of Business, combines a combination of lectures, business case studies, and hands-on exercises to introduce students to data analytics techniques and their applications in business.\nYou’ll get a general understanding of data analytics and management tools and methodologies, as well as enough hands-on examples and activities to help you prepare for further in-depth courses on the subject. The course leverages the Azure ML Studio IDE, which is browser-based, and includes a dataset that you can load into the platform.\nReasons to buy\nExtremely detailed Well-structured Disadvantages\nVery theoretical Linkdin Learning There are a few SQL classes that focus on query optimization, despite the fact that there are many SQL classes accessible. When writing SQL queries in the real world, you want to make sure they’re not just correct, but also efficient. There are a variety of ways to perform a query, but some are more efficient than others. You’ll learn everything you need to know about query tuning and performance optimization in the Advanced SQL for Query Tuning and Performance Optimization course.\nThe course starts with an overview of indexing, one of the most significant tools for query tailoring. It will then educate you about query plans, including how to translate a declarative query to a procedural execution plan, as well as data structuring and query crafting strategies for producing efficient execution plans.\nNeedless to mention, the course presupposes prior knowledge of SQL and does not cover the fundamentals. Also, the trainer covers installation on Mac OS X and uses the PostgreSQL database in the training.\nReasons to buy\nGood concept coverage Query improvement plus Helpful hints Disadvantages\nNot for novioces Coursera If you’re already comfortable with SQL and want to apply your knowledge to big data, the Analyzing Big Data with SQL course is for you. Cloudera provides the course, which can be audited for free.\nThe course assumes you have a basic understanding of SQL and builds on that foundation to teach you how to design SQL SELECT queries for data analysis. Although the instructor concentrates on two distributed big data SQL engines, Apache Hive and Apache Impala, the knowledge can be applied to regular RDBMS as well.\nThe course contains a virtual machine (VM) with everything you’ll need to complete it. Make sure you read over the VM’s hardware requirements before enrolling in the programme.\nThis is the second of three specialised courses. If you need to brush up on big data topics and vocabulary, you can attend the previous course, which can also be audited for free.\nReasons to purchase\nReal-life examples Practical exercises Transcripts and Captions Disadvantages\nVMs hardware requirement. ","description":"Build your SQL skills with these top SQL online courses.","tags":["review"],"title":"Best SQL online Courses To Enroll in 2022","uri":"/collections/reviews/best-sql-online-courses/"},{"content":"What is Machine Learning ? According to Wikipedia:\nMachine learning is a scientific study of algorithm and statistical model of computer system.\nThat is machine learning is used to perform certain tasks without using any human interaction. Machine learning has the ability to learn and recognize pattern to perform particular tasks. It can also improve it’s own performance based on how well it learned to perform a that task.\nIt is nothing just a computer program which takes “data” as input, learn from that data, recognize underlying patterns between the data, and based on this it produced the output(which is usual case are just predictions).\nNote: The data here can be of any form such as video, images, files etc\nThe way machine learning is implemented is by using Machine Learning Algorithm. This algorithm are based on sample data known as “training data” from which it build it’s own mathematical model in order to make predictions or decisions without explicitly programmed to perform tasks. This mathematical model are just statistics which focuses on making prediction using computers.\nMachine learning is seen as a subset of Artificial Intelligence (AI). Here AI is ultimate intelligence demonstrated by machine to perform all the task which a human or animal could do, on the other hand Machine Learning are capable of learning one tasks well. They cannot be used to perform different tasks which it has not trained for.\nNow a days Machine Learning is the most powerful tool which are being used in wide variety of applications. From email filtering which detect weather an email if spam or real, to medical diagnosis where they are being used to detect weather a person is having diseases such as cancer or it is being used to make new drugs to cure viruses.\nHistory of Machine Learning Back than computer had very small amount of memory to run program. Arthur Samuel who was than working at IBM developed a game of checker. The way he developed the game was with something called as alph-tunning which uses a scoring function based on the position of pieces in the board. This scoring function chooses the next step using minimax strategy that is the program chooses it’s next step in which it has the highest chance of winning. His algorithm also recorded each and every move he has taken so far and combined this with the reword function. He was the first man who came up with the idea “Machine Learning” in 1952.\nThen in 1957 Frank Rosenblatt designed the first neural network for computers, which simulate the thought processes of the human brain by combining model of brain cell by Donald Hebb’s and Arthur Samuel’s Machine Learning efforts.\nSince then we are seeing new algorithm emerging out in the field of machine learning like Nearest Neighbour algorithm in 1967, in 1970’s scientist begin to create program to analyze large amount of data and draw a conclusion from it.\nProcess Involved in Machine Learning Machine Learning process involves number of step. Let’s get to them one by one:\nData Gathering: This is one of the most important set in Machine Learning. Here quantity and quality of data both are very important. Data could be of any form images,videos,audio,csv file or any text file. The reason quantity and quality both are important because this result machine learning model will directly depend of data you feed to them. If the data is large but is unclean or in other words it consists of unnecessary information then the outcome of machine learning model will show unnecessary result or if the data is clean but amount of data is less the model will not predict will enough. Therefore, gathering large and clean amount of data is very important.\nChoosing Model: In this phase we have to select a Machine Learning Model. This model consists of number of algorithm which use mathematics, statistics, and probability. Choosing a right machine learning usually required experience because this model will learn and recognize pattern in your data.\nTraining: Here you train your model with the data you have gather. In training process usually 70-80% of your data is feed to your model. In this process model tries to learn your data.\nEvaluating or Validation: The remaining 20-30% of your data is used for validation that is it is used to check how well did the model perform.\nTunning: After validating the model they are tunned. Tunning step usually consists of increasing number of training steps, adjusting learning rate, or change the values of default parameters.\nPrediction: After nicely tunning the model, the model is ready for making prediction on new datasets.\nMachine learning Approches There are many ways from which machine learning can be implemented but this are grouped into 4 basic category:\nSupervised Learning Unsupervised Learning Reinforcement Learning Semi-Supervised Learning Let’s understand each of them one by one:\nSupervised Machine Learning Algorithm Supervised Machine Learning Algorithm build a mapping function between the set of data and the desired output.\nThis algorithm learns the relation between the input and the output. For example let’s say you have dataset which consists of images of apples and bananas and you want to classify them here model will learn mapping function as bananas are long, apples are curved and small, bananas are yellow apples are red etc. Once it have learned to recognize this pattern it will classify new data set given to them.\nSupervised Learning are further classified into two type:\nClassification and Regression Classification Classification Machine Learning algorithm are used when you have to classify different group of data. This algorithm learn how to separate the data just like apple and bananas. This algorithm consists of categorical output.\nRegression This is another form of supervised learning. The output of regression are numerical rather than a class. This algorithm are used when you have to predict number for example the stock market price,temperature of a given day, or probability of an event occurring\nUnsupervised Machine Learning Algorithm This algorithm are bit harder than Supervised Learning Algorithm. This algorithm only consists of input set and no output label for mapping it. This algorithm learn by itself and find pattern between the datasets.\nThis algorithm start from scratch and tries to find new or sometimes even better way to solve problem then Supervised Learning Machine Learning Algorithm. Therefore sometime they are also called Knowledge Discovery algorithms.\nUnsupervised Learning Algorithm takes a set of input data and tries to find structure in the data by grouping or clustering. Instead of responding to the feedback like Supervised Learning does it finds common pattern between the data and react between the presence and absence of this common pattern of each new pieces of data.\nThey are often used in clustering where same pieces of data are cluster together based on their commonalities.\nReinforcement Machine Learning Algorithm Reinforcement Machine Learning Algorithm are by far most complex and most accurate machine learning algorithm. Instead of labels these algorithms uses reward or feedback to learn patterns.\nThis algorithm are very much similar to how human learns. We perform some action based on that we receive some feedback. This feedback can either be positive of negative through this feedback we improve our actions.\nSimilar to this Reinforcement Learning algorithm take set of input from the environment. Then it do it’s computation then it perform some action. This action is passed through an agent which will give a feedback to algorithm either positive or negative based on the feedback this algorithm will adjust itself.\nGames are very good examples of Reinforcement Learning. They provide ideal data-rich environments. The scores in the game acts as as reword single to train the model. Reinforcement model tries to maximise it reward by playing game over and over again.\nSemi-Supervised Machine Learning Algorithm This is by far my most favorite machine learning algorithm. This algorithms combines both the world: Supervised Learning and Unsupervised Learning.\nIt reduces the burden of having large amount of labelled data. It takes only small number of labelled data and large amount of unlabelled data.\nA very good example of these learning is GAN(Generative Adversial Network). It uses two type of Neural Network Generator and Discriminator. The Generator is trainned to produced new data. This data is feed to discriminator which decides weather the data is real or fake. Based on the output of discriminator the generator network is optimized. While on the other hand Discriminator is trainned to decide between real or fake using small set of labelled data.\nBy using the network both for generating input and another one to generate outputs there is no need for us to provide explicit labels every single time and so it can be classed as Semi-Supervised Machine Learning Algorithm\n","description":"This review of machine learning covers its history, key definitions, applications, and current issues in the business world.","tags":["programming","Machine learning"],"title":"What is Machine Learning ?","uri":"/collections/programming/machine-learning/what-is-machine-learning/"},{"content":"The Three Mountain Top Candlestick Pattern The three mountain top/Buddha Top is a technical analysis chart pattern that predicts a price reversal in an asset. A three mountain top, signals that the asset is no longer rallying and that lower prices are approaching. This pattern can exist on any time period, but they must follow an upswing to be classified as such.\nA reversal of three mountain top, on the other hand, indicates that the price of the asset is no longer falling and may be rising.\nNotes:\nThree mountain top is formed when three peaks move into the same place, with pullbacks in between.\nThree mountain top is formed when the price falls below pattern support, indicating that the price will continue to fall.\nA trader exits long holdings and enters short positions after three mountain top pattern completes.\nIf trading the pattern, a stop loss could be put above resistance (peaks).\nThe expected downward aim for the pattern is equal to the pattern’s height minus the breakout point.\nHow does three mountain top works ? The price rises to a new high, then falls, showing that buyers are in charge.\nThe first sign of selling pressure appears when the price fails to break out of the prior high. At this time, the market is pulling down and forming a consolidation.\nThe market tries and fails once more to break out higher. The three “spikes” are visible after three futile attempts to escape.\nThe Three Mountain Top Chart Pattern is completed when the price breaks below the consolidation lows.\nImportance of Three Mountain Top Candlestick Pattern A three mountain top pattern shows that the price is difficult to penetrate the area of the peaks from a technical standpoint. In real-life terms, this signifies that the asset has been unable to find many purchasers in that price range despite many attempts.\nTraders who bought at the beginning of the pattern are under pressure to sell as the price falls. Investing in a price that is unable to move over resistance has limited profit potential. As past buyers quit losing long positions and new traders establish short positions, selling may become more aggressive when the price falls below the pattern’s swing lows. This is the psychology of the pattern, and it’s what drives the selloff after it’s completed.\nThere isn’t such a thing as a flawless pattern. A three mountain top creation and completion can lead traders to believe that the asset will continue to fall. However, the price may recover and break through the barrier.\nFor further safety, a trader could place a stop loss on short bets above the most recent peak or above a previous swing high inside the pattern.\nIf the price does not fall but instead rises, the risk of the deal is reduced.\nHow to Trade using Three Mountain Top Pattern Once the price of the asset falls below pattern support, some traders will establish a short position or quit a long position. The most recent swing low following the second peak serves as the pattern’s support level, or a trader might use a trendline to connect the swing lows between the peaks. When the price falls below the trendline, the pattern is considered complete, and the price is predicted to fall further.\nTraders will look for significant volume when the price falls through support to add confirmation to the pattern. Volume should increase, indicating a strong desire to sell. The pattern is more likely to fail if the volume does not rise (price rallying or not falling as expected).\nThe pattern specifies a downside goal equal to the pattern’s height minus the breakout point. This goal is only a guess. The price may drop significantly lower than the aim, or it may not reach the target at all.\nIn addition to the triple top, other technical indicators and chart patterns can be used. To help confirm the price drop, a trader can look for a bearish MACD crossover after the third peak, or the RSI to drop out of overbought area.\nMistakes to Avoid while trading Three Mountain Top Pattern 1. It’s too late to enter once the pattern is obvious: It’s too late to short the markets when the Three Mountain Top pattern appears on the charts.\nWhy?\nBecause you’ll most likely be shorting into an area of Support, where purchasing pressure could push the price higher.\nThis indicates you’re selling just as buyers are getting ready to enter the market, which isn’t a good idea.\n2. Don’t Chase Breakdown Okay, so you’re thinking:\n“Well, I’ll hold off on shorting the markets until Support breaks.” That sounds fantastic in theory.\nBut…\nBy the time Support is broken, the market has already moved lower and is set to make a higher decline.\nIf you try to follow the markets lower, you’ll almost certainly get stopped out on a pullback.\nReferences https://www.cryptostationchannel.com/2020/01/3-mountain-3-buddha-top-3-river.html\nhttp://www.dothefinancial.info/white-candlestick/three-mountains-and-three-rivers.html\n","description":"A Three Mountain Top/Buddha Top is a technical chart pattern that indicates an asset is no longer rallying and is headed towards lower pricing.","tags":["crypto"],"title":"The Three Mountain Top Candlestick Pattern(Buddha Top)","uri":"/collections/crypto/three-mountain-top/"},{"content":"ResNet: Understanding the Revolutionary Neural Network Architecture for Deep Learning ResNet (Residual Network) is a powerful convolutional neural network architecture that has revolutionized the field of deep learning. It was introduced in a research paper by Microsoft Research Asia in 2015 and has since become one of the most popular deep learning models due to its high accuracy and efficiency.\nResNet addresses the problem of vanishing gradients that often occur in very deep neural networks. When a neural network has many layers, the gradients can become very small, which makes it difficult for the network to learn. ResNet solves this problem by introducing shortcut connections, also known as skip connections, that allow the network to skip over certain layers and directly propagate the input to deeper layers. This helps to preserve the gradient and make it easier for the network to learn.\nDeep Convolutional Neural Network In our last article we have seen how a simple Convolutional neural network works.A Deep Convolution Neural Network are the network which consists of many hidden layer for examples AlexNet which consist of 8 layer where first 5 were convlutional layer and last 3 were full connected layer or VGGNet which consists of 16 Convolutional layer.\nThe problem with these deep neural network were as you increase the layer we start seeing degradation problem. Or to put it in another word as we increase depth of the network the accuracy gets saturated and starts degrading rapidly. In a deep neural network as we perform back-propogation, repeated mulitplication for finding optimal solution makes gradient very small which result in degradation. This problem is often called vanishing gradient/exploding gradient.\nResNet(or Residual Network) ResNet solve this degradation problem, is by skipping connection or layer. Skipping connection means, consider input x and this input is passed through stack of neural network layers and produce f(x) and this f(x) is then added to original input x.So our ouput will be:\nH(x) = f(x) + x\nSo, instead of mapping direct function of x -\u003e y with a function f(x), here we define a residual function using f(x) = H(x) - x. Which can be reframed to H(x) = f(x) + x, where f(x) represent stack of non-linear layers and x represent identity function. From this if the identity mapping is optimal we can easily put f(x) = 0 simply by putting value of weight to 0. So the f(x) is what authors call residual function.\nThis mapping ensure that higher layer will perfrom at least as good as lower layer, and not worse.\nThe ResNet architecture consists of several blocks, each of which contains multiple convolutional layers, batch normalization layers, and ReLU activation functions. The shortcut connections are added between these blocks, allowing the network to learn more complex features and improve its accuracy.\nOne of the major benefits of ResNet is its ability to train very deep neural networks with high accuracy. It has been used in a variety of applications, including image classification, object detection, and segmentation. In fact, ResNet is the backbone architecture for many state-of-the-art computer vision models, such as Faster R-CNN and Mask R-CNN.\nImplimenting ResNet Now let’s impliment ResNet model. Here I will be using ResNet18 model which consists of 18 layers. The dataset I will be using is dog-vs-cat which i have downloaded form kaggle websites. Our model will classify images of dogs and cats.\nArchitecture In the above diagram first we take input image which consists 3 channel(RGB) passed it to Convolutional layer of kernel_size = 3 and get 64 channel ouput. The Convolutional block between the curved arrow represent a Residual Block which will consists of: Convolutional layer -\u003e Batch Normalization -\u003e ReLU activation -\u003e Convolutional layer.\nOuput of these rediual block is than added to the initial input(i.e x) of residual block.After adding the ouput is than passed to ReLU activation function for next layer.\nThe dotted arrow represent that the output dimenssion of residual has changed so we also have to change the dimenssion of the input which is passed to that rediual block(i.e x) for adding it because adding is only possbile if the dimenssion are same.\nThe last layer of this architecture is a Linear Layer which will take the input and gives us ouput i.e wheater it is dog or cat.\nCode Let’s first our import necessay libraries:\nfrom PIL import Image import torch.optim as optim from tqdm import tqdm from torchvision import transforms import torch.nn.functional as F import torch.nn as nn import torchvision.datasets as dt import torch import os device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") PREPROCESS = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean = [0.485,0.456,0.406],std = [0.229,0.224,0.225])]) PyTorch provides very good class transforms which are use for modifying and transfoming image.transforms.Compose is use to combine or chained different transformation of image. This is used to build transformation pipeline.\nNow let’s get out dataset:\ndef get_dataset(train = True): if train: trainset = dt.ImageFolder(root = \"./train/\",transform = PREPROCESS) train_loader = torch.utils.data.DataLoader(trainset,batch_size = 8,shuffle=True) return train_loader else: testset = dt.ImageFolder(root = \"./test/\",transform = PREPROCESS) test_loader = torch.utils.data.DataLoader(trainset,batch_size = 8,shuffle=True) return test_loader Next let’s write our Residual Block:\nclass ResidualBlock(nn.Module): expansion = 1 def __init__(self, inchannel, outchannel, stride=1): super(ResidualBlock, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False), nn.BatchNorm2d(outchannel), ) self.conv2 = nn.Sequential( nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(outchannel) ) self.skip = nn.Sequential() if stride != 1 or inchannel != self.expansion * outchannel: self.skip = nn.Sequential( nn.Conv2d(inchannel, self.expansion * outchannel, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * outchannel) ) def forward(self, X): out = F.relu(self.conv1(X)) out = self.conv2(out) out += self.skip(X) out = F.relu(out) return out In the last article I have explain why we use nn.Module in our class so, I am going to skip that part. We have created two Convolutional layer self.conv1 and self.conv2 just like in diagram. The self.skip is our shortcut layer which will be added to the output of self.conv2. The “if” part in __init__() method checks weather the dimenssion of self.conv2 will change or not. If it changes than we have to change the ouput dimenssion of input by passing it to nn.Conv2d layer. In forward() method it is straight forward that how our data will flow.\nNow let’s write our Model class or ResNet class:\nclass Model(nn.Module): def __init__(self, ResidualBlock, num_classes): super(Model, self).__init__() self.inchannel = 64 self.conv1 = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(64), ) self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1) self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2) self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2) self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2) self.fc = nn.Linear(512*ResidualBlock.expansion, num_classes) def make_layer(self, block, channels, num_blocks, stride): strides = [stride] + [1] * (num_blocks - 1) layers = [] for stride in strides: layers.append(block(self.inchannel, channels, stride)) self.inchannel = channels * block.expansion return nn.Sequential(*layers) def forward(self, x): out = F.relu(self.conv1(x)) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.layer4(out) out = F.avg_pool2d(out, out.size()[3]) out = torch.flatten(out,1 ) out = self.fc(out) return out In __init__() method self.conv1 is the layer where we will take our input image of channel 3 (RGB) and will produce 64 output channel.Then we create 4 layer using make_layer method and each layer consists of 2 ResidualBlock. And the last layer(self.fc) is our Linear layer which will give us ouput weather it is dog or cat.\nIn forward method before passing it to self.fc layer we firt flatten or reshape our matrics to 1D.\nNow let’s define our loss function and optimizer:\nif __name__ == '__main__': resnet = Model(ResidualBlock,num_classes = 2) if torch.cuda.is_available(): resnet.cuda() print(resnet) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(resnet.parameters(),lr = 0.01) I have used here CrossEntropyLoss() and SGD``` optimizer.\nTrainning Let’s train our model:\ntrain = get_dataset(train = True) for epoch in tqdm(range(10)): for i,(images,target) in enumerate(train): images = images.to(device) target = target.to(device) out = resnet(images) loss = criterion(out,target) print(loss) # Back-propogation optimizer.zero_grad() loss.backward() optimizer.step() _,pred = torch.max(out.data,1) correct = (pred == target).sum().item() if i % 100 == 0: torch.save(resnet.state_dict(),\"model\") print(f\" epoch: {epoch}\\tloss: {loss.data}\\tAccuracy: {(correct/target.size(0)) * 100}%\") I have used 10 epochs to train the model. optimizer.zero_grad() method is used to make gradient to 0. Next we call backword() on our loss variable to perfrom back-propogation. After the gradient has been calculated we optimize our model by using optimizer.step() method.\nTesting test = get_data(train = False) with torch.no_grad(): correct = 0 total = 0 for i,(images,target) in tqdm(enumerate(test)): images = images.to(device) target = target.to(device) out = resnet(images) _,pred = torch.max(out.data,1) total += target.size(0) correct += (pred == target).sum().item() print(f\"Accuracy: {(correct/total) * 100}\") Since we don’t need to calculate weight during back-propogation while testing our model we use torch.no_grad method. Rest part is same as training.\nAfter 10 epochs I got accuracy of 93.23%.\nConclusion In conclusion, ResNet is a powerful deep learning architecture that has become an essential tool for computer vision applications. By understanding its working, benefits, and potential applications, you can leverage ResNet to improve the accuracy and efficiency of your neural network models.\nReferences ResNet Overview of Resnet PyTorch Docs Deep Learning ResNet Architecture ","description":"Explore the latest advancements in neural network technology with ResNet, a revolutionary deep learning architecture. Understand how it works, its benefits, and potential applications.","tags":["programming","Neural Network"],"title":"ResNet -  Understanding the Revolutionary Neural Network Architecture for Deep Learning","uri":"/collections/programming/neural-network/resnet/"},{"content":"Javascript Regex Cheatsheet A Regular Expression (RegEx) is a string of characters that defines a search pattern. Regex can save you hundreds of lines of code and can be used with Javascript or any programming language (and even CLI tools)\nBasic of Regular Expression or Regex In a RegEx, you always start and end the string with “/”. It is your responsibility to write the code in between the slashes.\"/apple/\" RegEx is the easiest way to match the term “apple”. Due to RegEx’s case-sensitive nature, this will not match “aPpLe” or “Apple”.\nA search for apple using the “/apple/i” keyword will match the words “apple” and “aPpLe”. Use “/apple|nut/” RegEx to match both “apple” and “nut.”\nIsn’t it simple, eh?\nRegex in Javascript In this lesson, we’ll learn how to use RegEx in JS.\nstr.match(regex): Returns an array of matches. It turns out that there’s a catch here. As an example, try saying \"apple apple\".match(/apple/) In the case of a match(/apple/), you would expect to get [“apple” and “apple”] but simple [‘apple’] is returned. Add g flag to receive a full array with multiple matches. regex.test(str): This method returns either true if the string is matched or false if not. let regex = /javascript|programming/i; const str = \"javascript is awesome programming language\"; regex.test(str); // true str.match(regex); // [\"javascript\", index: 0, input: \"javascript is awesome programming language\"] regex = /javascript|programming/gi; str.match(regex); // [\"javascript\", \"programming\"] Wildcard Period in Regex Let’s use the word ‘hug’ (/hug/) as an example of how we learned to statically match a word. But what if we want to match ‘huh’, ‘hug,’ and ‘hum’ all at once? It’s the wildcard round! That is the correct response. /hu./ This will match all three-letter long words that begin with the letter ‘hu.’\nMatching string which starts or ends with particular character or word or line using Regex To find a character or word at the beginning of a string use ^. let regex = /^123/; regex.test(\"__123_123_\"); // false - 123 is not exactly at the beginning! regex.test(\"123___123___\"); // true To search for a character or word at the end of string use $ like so regex = /123$/; regex.test(\"__123__123_\"); // false - 123 has to be at the end regex.test(\"__123\"); // true Matching character which occur one or more than one time We can use + operator to match the occurance of a string one or more the one time\n// With + let regex = /1+/g; \"123\".match(regex); //[\"1\"] \"1123\".match(regex); //[\"11\"] \"11213\".match(regex); //[\"11\", \"1\"] \"234\".match(regex); //null //without + regex = /a/g; \"abc\".match(regex); //[\"1\"] \"aabc\".match(regex); //[\"1\",\"1\"] \"aabac\".match(regex); //[\"1\",\"1\", \"1\"] \"bbc\".match(regex); //null Matching character which occur zero or more We can use * operator to match the occurance of a string zero or more .\nlet str = \"111\"; str.match(/1*/g); // [\"111\", \"\"] str.match(/1/g); // [\"1\", \"1\", \"1\"] Optional Character The ? operator makes a character or string optinal while mathcing regex pattern.\nlet regex = /1234?5/; // makes 'u' capital let american = \"1235\"; let british = \"12345\"; regex.test(\"1235\"); // true regex.test(\"12345678\"); // true regex.test(\"12365\"); // false Matching character at certain limit in Regex What if you wish to match a set of characters that repeats X times, such as a set of 5 letters that spells out “a”? Let’s get this party started, a{5} This would only match ‘aaaaa’, not ‘aa’ or “aaaaaaaaaa”.\nlet str = \"ama baalo maaaaamal aaaaaa\"; console.log(str.match(/a{5}/g)); //prints [\"aaaaa\". \"aaaaa\"] //to match 'm' letter followed by 5 x 'a' console.log(str.match(/ma{5}/)); // prints [\"maaaaa\", indes: 10, ...] //which means we have a match at index 10 You learned how to match a specific amount of repeating letters, such as a{5} matching “aaah.” But what if you want to match from one to three recurring characters rather than exactly five? Here’s a{1,3}: it matches “a,” “aa,” and “aaa,” but not “aaaa.”\nPosative and Negative Lookahead in Regex This is one of the more esoteric aspects of regex. But let’s try to understand:\nx(?=y) - This expression will match all the occurence of “x” that if followed by “y”, without making “y” part of the match.\nx(?!y) - This expression will match all the occurence of “x” that is NOT followed by “y”, without making “y” part of the match.\nlet str = \"worldheyworldhello\"; str.match(/world(?=hello)/); //[\"world\", index: 8, input: \"worldheyworldhello\"] str.match(/world(?=hey)/); //[\"world\", index: 0, input: \"worldheyworldhello\"] str.match(/world(?!hey)/); //[\"world\", index: 8, input: \"worldheyworldhello\"] Capture groups in Regex We’ve all heard of the DRY (Don’t Repeat Yourself) programming principle. Capture groups assist us in achieving this goal.The string to match is put between /(string)/\n/(123)\\1\\1/g; //Here \\1 will be evaluated as \"123\" so the regex expression will be: // /123123123/g You can alse use multiple capture group as:\nlet regex = /(123+)(567+)\\1\\2/g; //Will be same as /1233567712335677/g Comman shorthands For matching only alphbate you can use /[a-z]/ or /[A-Z]/. Or you can also use /D/.\nFor matching only numeric you can use /[0-9]/. Alternatively you can also use /d/.\nFor matching a non-word character such as “!@#%^*\u0026” you can use /\\W/.\nFor matching alphanumeric character you can use /\\w/.\nFlags in Regex Along with /regex/g in Javascript we have six flags which are used as needed. They are:\n/regex/i: The match is now case-insensitive. There is no difference between the letters ‘C’ and ‘c’.\n/regex/s: Allow wildcard period . to match with newline character \\n.\n/regex/u : For Unicode Support.\n/regex/y : Enable full Unicode support.\n/regex/m: Multiline more; only affects the behavior of ^ and $\n/regex/g: Only the first match will be returned if this flag is not set.\nReferences https://dev.to/carter/regular-expressions-regex-in-javascript-4m9h\nhttp://www.vitoshacademy.com/vba-regex-in-excel/\n","description":"This short guide outlines the most important principles and illustrates them with examples to teach you everything you need to know about JavaScript Regular Expressions.","tags":["programming","javascript"],"title":"Javascript Regex cheatsheet 2021","uri":"/collections/programming/javascript/javascript-regex-cheat-sheat-2021/"},{"content":"Javascript Regex Cheatsheet A Regular Expression (RegEx) is a string of characters that defines a search pattern. Regex can save you hundreds of lines of code and can be used with Javascript or any programming language (and even CLI tools)\nBasic of Regular Expression or Regex In a RegEx, you always start and end the string with “/”. It is your responsibility to write the code in between the slashes.\"/apple/\" RegEx is the easiest way to match the term “apple”. Due to RegEx’s case-sensitive nature, this will not match “aPpLe” or “Apple”.\nA search for apple using the “/apple/i” keyword will match the words “apple” and “aPpLe”. Use “/apple|nut/” RegEx to match both “apple” and “nut.”\nIsn’t it simple, eh?\nRegex in Javascript In this lesson, we’ll learn how to use RegEx in JS.\nstr.match(regex): Returns an array of matches. It turns out that there’s a catch here. As an example, try saying \"apple apple\".match(/apple/) In the case of a match(/apple/), you would expect to get [“apple” and “apple”] but simple [‘apple’] is returned. Add g flag to receive a full array with multiple matches. regex.test(str): This method returns either true if the string is matched or false if not. let regex = /javascript|programming/i; const str = \"javascript is awesome programming language\"; regex.test(str); // true str.match(regex); // [\"javascript\", index: 0, input: \"javascript is awesome programming language\"] regex = /javascript|programming/gi; str.match(regex); // [\"javascript\", \"programming\"] Wildcard Period in Regex Let’s use the word ‘hug’ (/hug/) as an example of how we learned to statically match a word. But what if we want to match ‘huh’, ‘hug,’ and ‘hum’ all at once? It’s the wildcard round! That is the correct response. /hu./ This will match all three-letter long words that begin with the letter ‘hu.’\nMatching string which starts or ends with particular character or word or line using Regex To find a character or word at the beginning of a string use ^. let regex = /^123/; regex.test(\"__123_123_\"); // false - 123 is not exactly at the beginning! regex.test(\"123___123___\"); // true To search for a character or word at the end of string use $ like so regex = /123$/; regex.test(\"__123__123_\"); // false - 123 has to be at the end regex.test(\"__123\"); // true Matching character which occur one or more than one time We can use + operator to match the occurance of a string one or more the one time\n// With + let regex = /1+/g; \"123\".match(regex); //[\"1\"] \"1123\".match(regex); //[\"11\"] \"11213\".match(regex); //[\"11\", \"1\"] \"234\".match(regex); //null //without + regex = /a/g; \"abc\".match(regex); //[\"1\"] \"aabc\".match(regex); //[\"1\",\"1\"] \"aabac\".match(regex); //[\"1\",\"1\", \"1\"] \"bbc\".match(regex); //null Matching character which occur zero or more We can use * operator to match the occurance of a string zero or more .\nlet str = \"111\"; str.match(/1*/g); // [\"111\", \"\"] str.match(/1/g); // [\"1\", \"1\", \"1\"] Optional Character The ? operator makes a character or string optinal while mathcing regex pattern.\nlet regex = /1234?5/; // makes 'u' capital let american = \"1235\"; let british = \"12345\"; regex.test(\"1235\"); // true regex.test(\"12345678\"); // true regex.test(\"12365\"); // false Matching character at certain limit in Regex What if you wish to match a set of characters that repeats X times, such as a set of 5 letters that spells out “a”? Let’s get this party started, a{5} This would only match ‘aaaaa’, not ‘aa’ or “aaaaaaaaaa”.\nlet str = \"ama baalo maaaaamal aaaaaa\"; console.log(str.match(/a{5}/g)); //prints [\"aaaaa\". \"aaaaa\"] //to match 'm' letter followed by 5 x 'a' console.log(str.match(/ma{5}/)); // prints [\"maaaaa\", indes: 10, ...] //which means we have a match at index 10 You learned how to match a specific amount of repeating letters, such as a{5} matching “aaah.” But what if you want to match from one to three recurring characters rather than exactly five? Here’s a{1,3}: it matches “a,” “aa,” and “aaa,” but not “aaaa.”\nPosative and Negative Lookahead in Regex This is one of the more esoteric aspects of regex. But let’s try to understand:\nx(?=y) - This expression will match all the occurence of “x” that if followed by “y”, without making “y” part of the match.\nx(?!y) - This expression will match all the occurence of “x” that is NOT followed by “y”, without making “y” part of the match.\nlet str = \"worldheyworldhello\"; str.match(/world(?=hello)/); //[\"world\", index: 8, input: \"worldheyworldhello\"] str.match(/world(?=hey)/); //[\"world\", index: 0, input: \"worldheyworldhello\"] str.match(/world(?!hey)/); //[\"world\", index: 8, input: \"worldheyworldhello\"] Capture groups in Regex We’ve all heard of the DRY (Don’t Repeat Yourself) programming principle. Capture groups assist us in achieving this goal.The string to match is put between /(string)/\n/(123)\\1\\1/g; //Here \\1 will be evaluated as \"123\" so the regex expression will be: // /123123123/g You can alse use multiple capture group as:\nlet regex = /(123+)(567+)\\1\\2/g; //Will be same as /1233567712335677/g Comman shorthands For matching only alphbate you can use /[a-z]/ or /[A-Z]/. Or you can also use /D/.\nFor matching only numeric you can use /[0-9]/. Alternatively you can also use /d/.\nFor matching a non-word character such as “!@#%^*\u0026” you can use /\\W/.\nFor matching alphanumeric character you can use /\\w/.\nFlags in Regex Along with /regex/g in Javascript we have six flags which are used as needed. They are:\n/regex/i: The match is now case-insensitive. There is no difference between the letters ‘C’ and ‘c’.\n/regex/s: Allow wildcard period . to match with newline character \\n.\n/regex/u : For Unicode Support.\n/regex/y : Enable full Unicode support.\n/regex/m: Multiline more; only affects the behavior of ^ and $\n/regex/g: Only the first match will be returned if this flag is not set.\nReferences https://dev.to/carter/regular-expressions-regex-in-javascript-4m9h\nhttp://www.vitoshacademy.com/vba-regex-in-excel/\n","description":"This short guide outlines the most important principles and illustrates them with examples to teach you everything you need to know about JavaScript Regular Expressions.","tags":["programming","javascript"],"title":"Javascript Regex cheatsheet 2021","uri":"/post/javascript-regex-cheat-sheat-2021/"},{"content":"What is Convolutional Neural Network (CNN) In our previous article we have discussed how a simple neural network works.The problem with fully conntected neural network is that they are computationally expensive. Also, by adding lots of layer we come across some problem:\nWe run into problem of vanashing gradient problem. Another issue is that number of trainable parameter grows rapidly. Because of this trainning slows down or became practically impossible. Fully connected neural network can be easily exposed to overfitting Convolutional Neural Network (or CNN) can solve this problem by finding correlations between adjacent input between dataset (eg. image or time series). This means that not every node in the network is connected to every other node in the next layer and this cut down number of weight parameter required to be trained in the model.\nHow Convolutional Neural Network (CNN) works According to wikipedia a convolution is a operation between to function the produce third function expression how the shape of one modified by the other. In convolutional neural network this means that we move a window or filter across the image being studied. We can imagine this as a 2x2(or 3x3) filter sliding accross all the available nodes/ pixels in the input image.\nNow let’s look at some of the important term used in convolutional nerual network (CNN)\nComponents of Convolutional Neural Network (CNN) Feature mapping (or activation map) Whenever we use a filter matrix(or slidding window) accross the image to compute the convolution operation the resultant matrix is called Feature Map.Because of this mapping our network learn to recognize common geometrical object such as edge,line etc.\nConvolution layers need multiple filters(For example an image consists of 3 channel Red,Green and Blue).This multiple filters will produce their own 2D output, then this outputs is then combined to produced a single output (Feature map or activation map). Each channel will be trained to detect certain key feature.The ouptut dimenssion of the feature map will be 1 more than the input channel.\nPolling Polling is a type of sliding window technique, but instead of applying weights, we apply some statistical function of some type over content window. The most common type of polling is max polling. And the output of max polling is the maximum value in that content window.\nStrides Strides is the number of pixels which shits over the input image.For example if strides is 1 then we move the window 1 pixels at a time, when it is 2 we move the windows 2 pixels at a time and so on.\nIn polling if strides is greater than 1 then the ouput size will reduce. For example if the input image is of size 5x5 and if we apply strides of 2 than our output will be of 3x3 this process is called down sampling and it reduce the number of trainning parmeter\nPadding Sometimes our sliding window doest not fit perfactly with the input image. So we have two options:\nPad the image with zero(also called zero-padding). Drop or clip the part of the image where our filter window did not fit. This is also known as valid padding which keeps only valid part of the image. Implementing Convolutional Nerual Network Firt let’s import our necessary libraries:\nimport torch import torch.nn as nn from torch.autograd import Variable import torch.nn.functional as F import torch.optim as optim from torchvision import datasets,transforms from tqdm import tqdm device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") def get_data(batch_size = 200,train = True): data = torch.utils.data.DataLoader( datasets.MNIST('../data', train=train, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True) return data Now let’s create our model class\nclass Model(nn.Module): def __init__(self): super(Model,self).__init__() self.l1 = nn.Sequential( nn.Conv2d(1,32,kernel_size = 5,stride = 1,padding = 2), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2) ) self.l2 = nn.Sequential( nn.Conv2d(32,64,kernel_size = 5,stride = 1,padding = 2), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2) ) self.drop_out = nn.Dropout() self.fc1 = nn.Linear(7*7*64,1000) self.fc2 = nn.Linear(1000,10) As we have discussed in our last section to create a nerual network all you need is to inherit the nn.Module class in your model.\nNext we create sequential object. The nn.Sequential() method allow us to create sequential ordered layer in our network. As in our case we have create convolution-\u003eReLU-\u003epolling.sequence.\nThe nn.Conv2d() method is used the create set of convolution filters and we have passed to parameters first is our input channel which is 1 for gray scale image and the next is number of output channel.The kernel_size argument is the size of our sliding window of our image and the last two argument are strides and padding.\nTo know the dimenssion of our output image we have formula:\nWout = ((Win - F + 2P)/S) + 1 (Same formula is calculated for Height)\nWout = Width of ouput F = Filter size(or window size) P = Padding S = Stride\nIf we wish to keep our input and output dimenssion same we have to take kernel_size = 5 and stride = 1. If we put all the value in above formula we get padding = 2.\nnn.ReLU() is our activation function. The last element on our sequential object of self.l1 is nn.MaxPool2d. First argument given to this method is polling size which we have given as 2x2. Next we want to down sample our date by giving the stride 2. So our 28x28 pixel image will now be 14x14 pixel(You can also put the value in above formula, you will get Wout = 14 if you put padding = 0).\nSimilaryly, we have define our second layer i.e self.l2.After this operation our ouput will be of dimensssion 7x7 pixels.\nNext we use nn.Dropout to avoid overfitting.Finally we create two fully connected layer self.fc1 and self.fc2 by using nn.Linear().The input size of first layer will be 7x7x64 which is dimenssion of image and number of channel from previous Sequential layer and this is connected to 1000 nodes.Finally this 1000 node is than connected to 10 nodes.\nNow let’s write our froward() method to tell our network how our data should flow.\ndef forward(self,x): x = self.l1(x) x = self.l2(x) x = x.reshape(x.size(0),-1) x = self.drop_out(x) x = self.fc1(x) x = self.fc2(x) x = x.reshape(x.size(0),-1) return x The first two line of forward is straight forward. After line self.l2(x) we apply reshape method which flatten our data of dimession 7x7x64 to 3136x1. Next dropout is applied followed by two fully connected layer,with finaly output being returned.\nNow we create our main function:\nif __name__ == '__main__': cnn = Model() if torch.cuda.is_available(): cnn.cuda() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(cnn.parameters(),lr = 0.01) First of all we create instance of our Model class called cnn. Next we define our loss function which in our case is nn.CrossEntropyLoss(). Then we define our optimize,here I have used Adam optimizer, which takes model parameter as it’s firt input and we have given learning rate to be 0.01.\nNow let’s train our model:\ntrain = get_data() for epoch in tqdm(range(10)): for i,(images,target) in enumerate(train): images = images.to(device) target = target.to(device) out = cnn(images) loss = criterion(out,target) # Back-propogation optimizer.zero_grad() loss.backward(https://miro.medium.com/max/3944/1*YejW73f36BGhNGhrtbz67g.png) optimizer.step() _,pred = torch.max(out.data,1) correct = (pred == target).sum().item() if i % 100 == 0: print(f\" epoch: {epoch}\\tloss: {loss.data}\\tAccuracy: {(correct/target.size(0)) * 100}%\") In trainning I have used 10 epochs. Next we iterate to our DataLoader object(i.e train).Then we pass our datasets(i.e images) to our model.Using criterion we calculate our loss. Next step is to perform back propogation, first we make gradient to be zero which is done by using optimizer.zero_grad(). Next we call backword() on our loss variable to perfrom back-propogation. After the gradient has been calculated we optimize our model by using optimizer.step() method.\nNow to test model:\n# Testing test = get_data(train = False) with torch.no_grad(): correct = 0 total = 0 for i,(images,target) in tqdm(enumerate(train)): images = images.to(device) target = target.to(device) out = cnn(images) _,pred = torch.max(out.data,1) total += target.size(0) correct += (pred == target).sum().item() print(f\"Accuracy: {(correct/total) * 100}\") Testing model is pretty much similar to trainning model except, in testing model we don’t want to update weights of our model. For this we use torch.no_grad() which tell pytorch not to update weight.\nOUTPUT epoch: 6\tloss: 0.35762882232666016\tAccuracy: 86.0% epoch: 6\tloss: 0.345419317483902\tAccuracy: 88.5% epoch: 6\tloss: 0.3479294180870056\tAccuracy: 90.0% 70%|█████████████████████████████████████████████████████████████████▊ | 7/10 [08:18\u003c03:34, 71.46s/it] epoch: 7\tloss: 0.5705909729003906\tAccuracy: 85.0% epoch: 7\tloss: 0.36838656663894653\tAccuracy: 89.5% epoch: 7\tloss: 0.2633790671825409\tAccuracy: 91.5% 80%|███████████████████████████████████████████████████████████████████████████▏ | 8/10 [09:30\u003c02:23, 71.59s/it] epoch: 8\tloss: 0.31612157821655273\tAccuracy: 89.0% epoch: 8\tloss: 0.2783728539943695\tAccuracy: 91.0% epoch: 8\tloss: 0.38499078154563904\tAccuracy: 90.5% 90%|████████████████████████████████████████████████████████████████████████████████████▌ | 9/10 [10:42\u003c01:11, 71.68s/it] epoch: 9\tloss: 0.2609010934829712\tAccuracy: 91.5% epoch: 9\tloss: 0.32276371121406555\tAccuracy: 88.5% epoch: 9\tloss: 0.4422188997268677\tAccuracy: 87.0% 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [11:54\u003c00:00, 71.45s/it] 300it [00:30, 9.96it/s] Accuracy: 89.43333333333334 References CNN https://medium.com/swlh/convolutional-neural-networks-mathematics-1beb3e6447c0 Feature Mapping PyTorch torch.nn PyTorch Convolution Layer PyTorch Normalization Layer ","description":"In this tutorial, we'll show you how to design convolutional neural network (CNN) models Create CNN models to solve a classification problem.","tags":["programming","Neural Network"],"title":"Implementing Convolutional Neural Network (CNN) using Python","uri":"/collections/programming/neural-network/cn/"},{"content":"Implementing Convolutional Neural Network (CNN) in Pytorch In this turorial, we’ll go through the basic ideas of PyTorch starting at tensors and computational graphs and finishing at the Variable class and the PyTorch autograd functionality.\nTensor Tensor are just multi-dimessional array.Tesnors play a vital role in deep learning libraries for efficient computation.Graphical Processing Unit(GPUs) work effectivly at computing operations between different tensor.In Pytourch there are number of way we can declare tensors:\nimport torch a = torch.Tensor(2,3) #create tensor of size(2,3)(or 2x3 matrix) filled with zero b = torch.ones(4,5) #create tensor of size(4,5)(or 2x3 matrix) filled with ones c = torch.rand(5,5) #create tensor of size(5,5)(or 2x3 matrix) filled with random values Autograd In neural network we calculate errors between predicted ouput and the actual ouput and based on this two we calculate error gradient and do a back-propogation through our computational graph. This mechanisum in PyTorch is called autograd. Autograd allows automatic gradient computation on the tensor when the .backword()method is called.\nVariable class is the main component autograd system.This class wraps around tensor. The object of this class contains the value of tensor, the gradient of tensor and also contains reference to whatever function created.In PyTorch variable is declared as:\nx = Variable(torch.onses(2,3)+5,requires_grad = True) Creating Neural Network Here we are going to create a neural network of 4 layer which will consists of 1 input layer,1 ouput layer, and 2 hidden layer. The input consits of 28x28(784) gray scale pixels which are MNIST hand written data set.This input data is passed through 2 hidden layers with ReLU activation function. Finally ouput layer is of 10 nodes corresponding to 10 possibledigit(i.e 0 to 9).At the ouput layer we will be using softmax activation function.\nFirst let import our necessay libraries\nimport torch import torch.nn as nn from torch.autograd import Variable import torch.nn.functional as F import torch.optim as optim from torchvision import datasets,transforms from tqdm import tqdm When creating neural network we have to include nn.Module class from PyTorch. The nn.Module is the base class of all neural network. Inheriting this class allows us to use the functionality of nn.Module base class but have the capabilites of overwritting of the base class for model construction/forword pass through our network.\nclass Model(nn.Module): def __init__(self): super(Model,self).__init__() self.input_layer = nn.Linear(28*28,200) self.hidde_layer1 = nn.Linear(200,200) self.hidde_layer2 = nn.Linear(200,10) In our __init__() method we also define our neural net architecture which will contain input layer of nodes 28x28(784), two hidden layer of node 200, and one ouput layer of 10 nodes.\nNext lets define how our data will flow in our network. For this we will be using forword() in our class. This method will overwrite a dummy method in the base class.and this is needed for definning each network.\ndef forword(self,x): x = F.relu(self.input_layer(x)) x = F.relu(self.hidde_layer1(x)) x = self.hidde_layer1(x) return F.log_softmax(x) Now let’s write our main function\ndef get_data(batch_size = 200,train = True): data = torch.utils.data.DataLoader( datasets.MNIST('../data', train=train, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True) return data if __name__ == '__main__': net = Model() net.to(device) optimizer = optim.SGD(net.parameters(),lr = 0.01,momentum = 0.9) criterion = nn.NLLLoss() ## Trainning Model train = get_data() for epoch in tqdm(range(10)): for idx,(x_train,target) in enumerate(train): x_train,target = Variable(x_train).to(device),Variable(target).to(device) x_train = x_train.view(-1,28*28) optimizer.zero_grad() net_out = net(x_train) loss = criterion(net_out,target) loss.backward() optimizer.step() if idx % 10 == 0: print(f\"Train Loss {loss.data}\") ## Testing our model test = get_data(train = False) test_loss = 0 correct = 0 with torch.no_grad(): for epoch in tqdm(range(10)): for idx,(x_test,target) in enumerate(test): x_test,target = Variable(x_test).to(device),Variable(target).to(device) x_test = x_test.view(-1,28*28) optimizer.zero_grad() net_out = net(x_test) test_loss += criterion(net_out,target).data pred = net_out.data.max(1)[1] correct += pred.eq(target.data).sum() test_loss /= len(test.dataset) print(f\"Average loss: {test_loss}\\n correct:{correct}\\n Accuracy: {(correct/len(test.dataset)) * 100}%\") In the main fucntion first we create instance of our model class. Then we define our loss function, here I have used SGD loss function. Next we set our log criterion which is negative log likelihood.\nNext in trainnig part i have extracted data from train object which is included in PyTourch utility module.This object supplies batched of input. In the outer loop is the number of epochs while the inner loop runs through entire trainning set. The view function operates on PyTorch variable to reshape them.optimizer.zero_grad() reset all the gradient in this model. loss.backward() runs back-propogation operation from loss Variable and optimizer.step() method execute gradient descent step based on gradient calculated during the .backward() operation.\nSimilarly we do this operation on test data but now we dont need to update gradient on our network for this we use torch.no_grad() to tell PyTorch not to update weight while back-propgating.\nAfter running for 10 epochs I got the following ouput:\nAverage loss: 0.0033838008530437946 correct:97720 Accuracy: 98% References CNN https://medium.com/swlh/convolutional-neural-networks-mathematics-1beb3e6447c0 Feature Mapping PyTorch torch.nn PyTorch Convolution Layer PyTorch Normalization Layer ","description":"In this tutorial we are going to learn how to implement Convolutional Neural Network (CNN) using pytorch","tags":["programming","Neural Network"],"title":"Implementing Convolutional Neural Network (CNN) using PyTorch","uri":"/collections/programming/neural-network/cnn/"},{"content":"Stochastic Gradient Descent (SGD) with Python Stochastic gradient descent or SGD is very popular and powerful algorithm used in machine learning. Stochastic Gradient Descent (SGD) is an iterative algorithm which is used to compare various solution till the optimum solution is not found.They are widely used in trainning Neural Network.\nLet’s now understand what Stochastic Gradient Descent (SGD) is, but before that first let’s understand what Gradient Descent is?\nWhat is Gradient Descent Gradient mean a slope either upward or downwords and Descent means stepping downword in a scale.Hence, gradient descent simply means stepping upwoard or downword of a slope to reach the lowest or highest point of that slope. In machine learning the objective of gradient descent is such that it find the minimum value of the objective function such that the final result is optimum or satisfactory.\nLet’s take an exmaple imagine you are blindfolded and want to climb to the top of the hill with fewest steps along the way as possible.Initially what you will do is first take big-big steps in steepest direction of the hill but as you come close to the top of the hill your steps will be smaller and smaller to avoid skipping it.Now instead of climbing up think of gradient descent as hiking down to the bottom valley.This is a better way to visualize this algorithm as it is a minimizing alogrithm.\nGradient descent initially will take bigger step then as it get’s closer to the minimum slope of the objective function it’s steps gets smaller and smaller.Gradient descent always tries to find minimum value of the slope which is close to zero but not zero because if it gets zero the model will stop learning.\nGradient descent start by picking random initial value for the parameter.(It is mostly partial derivative with respect to its input).\nThen we update gradient descent function by giving this parameter values.\nThe output of gradient descent function is used in calculating step size: step size = gradient x learning rate\nHere deciding the value “learning rate” is very important because if the value of “learning rate” is larger it may skip the optimum point and if it is smaller it will take more time in finding minimum point.\nNow calculate new parameter by:\nnew params = old params - step size\nThis process is repeated until the optimum value or satisfacotry value of the objective fucntion is not found.\nThe downside of gradient descent algorithum is the amount of computation it takes in each iteration.Suppose we have 50,000 data points and 50 features.Then we calculate derivative of the function with respect to it’s feature,so total will be 50000 x 50 = 2500000 computaion per iteration. It is common to take atleast 1000 iteration so 2500000 x 1000 = 2500000000 computaion to complelte the alogrithm. Which conclude that gradient descent is very slow on huge data.\nWhat is Stochastic Gradient Descent (SGD) ‘Stochastic’ means involving random variable.Instead of selecting whole data and calculates it’s derivative in each iteration, SGD selects few samples from the data points called ‘batches’ and compute it’s derivatives.This step reduce the computaion steps enormusly.This samples or batches are randomly shuffle on each iteration of the alogrithum.\nThe path taken by Stochastic Gradient Descent (SGD) algorithm to find the optimum value is usually nosier than gradient descent as only one sample from the datasets is taken on each iteration. SGD usually taken heigher number of iteration to find optimum value than gradient descent but computation cost is much lesser than gradient descent since we are taking only samples instead of whole datastes.\nNow let’s look at example on how to implement Stochastic Gradient Descent in Python:\nExample of Stochastic Gradient Descent using Python First let’s import our necessary libraries:-\nimport numpy as np from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.datasets import make_blobs from sklearn import metrics import matplotlib.pyplot as plt Here we will be using SGDClassifier which is linear models but uses stochastic gradient descent (SGD) learning.\nNow let’s get our datasets.We have use here make_blobs to create our own datasets which contain 500000 data points with 2 features and 2 centers.\ndef get_datasets(): X,y = make_blobs(n_samples = 500000,n_features = 2, centers = 2,cluster_std = 2.5,random_state = 40) return X,y Let us now implement SGDClassifier on the above datasets:\nif __name__ == '__main__': X,y = get_datasets() X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 40) model = SGDClassifier(loss = \"log\",max_iter = 1000, n_jobs = -1) model.fit(X_train,y_train) y_pred = model.predict(X_test) print(f\"Accuracy : {metrics.accuracy_score(y_test,y_pred)}\") Output: References https://pantelis.github.io/cs634/docs/common/lectures/optimization/sgd/\nhttps://deepai.org/machine-learning-glossary-and-terms/stochastic-gradient-descent\n","description":"In this blog we understand what is Stochastic Gradient Descent How to implement it using python","tags":["programming","Machine learning"],"title":"Understanding Stochastic Gradient Descent Descent (SGD) with python","uri":"/collections/programming/machine-learning/sgd/"},{"content":"Understanding Three White Soldiers Candlestick Pattern After a downtrend, the bullish three white soldiers chart pattern can be useful in predicting a price reversal. Understand this candlestick pattern and how to trade it.\nWhat is Three White Soldiers Candlstick Pattern ? There have been a number of different names for the Three White Soldiers pattern throughout history. Because they used a red candle instead of a white one, the Japanese called it “The Three Red Soldiers.” During World War II, the design was known as the Three Marching Soldiers. Finally, the design is now known as the Three White Soldiers and is widely recognised.\nIn a downtrend, this pattern is classified as a bullish reversal. Any candle with a white body can create all three lines, which appear as lengthy lines. As a result, the following candles may appear: Long White Candle, White Candle, White Marubozu, Opening White Marubozu, and Closing White Marubozu, among others. No doji candles or spinning tops!\nWhen a downtrend occurs, the first line is drawn. The second and third lines open and close above the prior candle’s initial price.\nPrior to this change, certain authors argued that both the second and third lines’ starting prices should be located at least halfway up the prior candle’s body height. In the opinion of other people, closing prices should be near the candle’s peak, resulting in very brief shadows. The Three White Soldiers candlestick pattern is a wonderful example of this trend.\nWhen three long white candles in a row have each closing price higher than the preceding, the market is controlled by the bulls.\nNotes\nIt is considered a reliable reversal pattern when validated by other technical indicators such as the relative strength index (RSI) .\nA retracement risk can be determined by measuring the size of the candles and their shadows.\nWhen three white soldiers are reversed by three black crows, it implies that an upward trend has reversed.\nWhat does Three White Soldiers Candlestick Pattern tells traders ? An upward-trending candlestick pattern with three white soldiers is a bullish reversal candlestick. Because of this, bears are losing ground and the market is expected to reverse its course in the near future.\" In this design, each candle must be stacked on top of the preceding one, so that each step is taller than the one before it A new trend is set to begin as a result of the pattern’s rising tendency.\nThis is a really effective and trustworthy pattern, as we’ve already described. When used in conjunction with other indicators, such as the Relative Strength Index, it’s much more powerful. As a result, it’s a terrific tool for developing trading strategies. As well as the price, dealers need also consider other elements such as volume.\nThe three white soldiers pattern is useful for starting new trade or ending existing ones. Three white soldiers emerge on the chart of a trader, followed by an opening gap up. Five-minute intervals or 15-minute intervals are available for searching the three white troops on intraday charts. A stop-loss should be established at the latest low, and profits should be taken anytime there is any indication that the trend is turning direction. This candlestick pattern is extremely rare, yet traders shouldn’t ignore it.\nIdentifying Three White Soldiers Candlestick Pattern ? The Three White Soldier Design, as its name suggests, has three candlesticks, making it a difficult pattern to recognise. As a result, you’ll need the following traits to be able to recognise it:\nThree consecutive bullish candlesticks are required for the three white soldiers candlestick pattern.\nIt is essential that all three candles open and close higher than the one before.\nThe second candle’s body must be larger than the first.\nThe wicks of all three candles must be either nonexistent or very small. It’s a sign that the buyers were able to gain control. In this case, the prices were able to be closed out at the highest point of a candle.\nAs the name suggests, the three white soldiers pattern appears at a support zone when three consecutive lengthy bullish candles follow a downtrend and forecast an impending rally.\nLimitation of Three White Soldier Candlestick pattern When three white soldiers appear, it’s easy to fall into the trap of believing that the current trend will continue rather than reverse itself. One of the most crucial things to keep an eye on is the volume supporting the creation of three white soldiers. It is more likely that the market behaviour of a few is suspicious when the volume is low.\nIn order to circumvent these constraints, traders use three white soldiers and other candlestick patterns in conjunction with technical indicators such as trendlines, moving averages, and bands. Before initiating a long position, traders might check for areas of upcoming resistance to confirm that there was a big quantity of dollar volume transacted. Traders may wait for further confirmation of a breakthrough before opening long positions, assuming the pattern occurred on low volume and near-term resistance.\nReferences https://blockoney.com/three-white-soldiers-candlestick/ https://www.adigitalblogger.com/chart-patterns/three-white-soldiers/ https://www.alphaexcapital.com/candlestick-patterns/ ","description":"Three white soldiers is a bullish candlestick pattern that is used to signal the reversal of a downward trend.","tags":["crypto"],"title":"Understanding Three White Soldiers Candlestick Pattern","uri":"/collections/crypto/three-white-soliders/"},{"content":"React Tutorial : Creating responsive Drawer using Material-UI In this article we are going to build a responsive Drawer which adjust itself according to screen size using material-ui framework. If you are new visit here for installing and getting started with Material-UI.\nIn the code below first we will set up a simple drawer then we will add feature of hiding the drawer according to the screen size.\nLet’s get started, open up you text editor and write the following code:\nDrawer import React from \"react\"; import AppBar from \"@material-ui/core/AppBar\"; import Drawer from \"@material-ui/core/Drawer\"; import Toolbar from \"@material-ui/core/Toolbar\"; import Divider from \"@material-ui/core/Divider\"; import Hidden from \"@material-ui/core/Hidden\"; import List from \"@material-ui/core/List\"; import IconButton from \"@material-ui/core/IconButton\"; import MenuIcon from \"@material-ui/icons/Menu\"; import ListItem from \"@material-ui/core/ListItem\"; import ListItemText from \"@material-ui/core/ListItemText\"; import Typography from \"@material-ui/core/Typography\"; import { makeStyles } from \"@material-ui/core/styles\"; import img1 from \"./img1.jpg\"; import img2 from \"./img2.jpg\"; const drawerWidth = 240; const useStyles = makeStyles((theme) =\u003e ({ drawer: { width: drawerWidth, flexShrink: 0, }, toolbar: theme.mixins.toolbar, content: { flexGrow: 1, padding: theme.spacing(3), marginLeft: 300, }, })); export default function App() { const classes = useStyles(); const drawer = ( \u003cdiv\u003e \u003cdiv\u003e \u003cDivider /\u003e \u003cList\u003e {[\"Search\", \"Browse\", \"Category\"].map((anchor, text) =\u003e ( \u003cListItem button key={anchor}\u003e \u003cListItemText primary={anchor} /\u003e \u003c/ListItem\u003e ))} \u003c/List\u003e \u003cDivider /\u003e \u003cList\u003e {[\"Services\", \"About Us\", \"Contact Us\"].map((anchor, text) =\u003e ( \u003cListItem button key={anchor}\u003e \u003cListItemText primary={anchor} /\u003e \u003c/ListItem\u003e ))} \u003c/List\u003e \u003c/div\u003e \u003c/div\u003e ); return ( \u003cdiv\u003e \u003cAppBar position=\"fixed\"\u003e \u003cToolbar\u003e \u003cDrawer classes={{ paper: classes.drawer, }} open variant=\"permanent\" \u003e {drawer} \u003c/Drawer\u003e \u003c/Toolbar\u003e \u003c/AppBar\u003e \u003cdiv className={classes.content}\u003e \u003cdiv className={classes.toolbar} /\u003e \u003cimg src={img1} style={{ height: \"auto\", width: \"80%\" }} alt=\"img1\" /\u003e \u003cTypography paragraph\u003e I inadvertently went to See's Candy last week (I was in the mall looking for phone repair), and as it turns out, See's Candy now charges a dollar -- a full dollar -- for even the simplest of their wee confection offerings. I bought two chocolate lollipops and two chocolate-caramel-almond things. The total cost was four-something. I mean, the candies were tasty and all, but let's be real: A Snickers bar is fifty cents. After this dollar-per-candy revelation, I may not find myself wandering dreamily back into a See's Candy any time soon. \u003c/Typography\u003e \u003cimg src={img2} style={{ height: \"auto\", width: \"80%\" }} alt=\"img1\" /\u003e \u003cTypography paragraph\u003e Spending time at national parks can be an exciting adventure, but this wasn't the type of excitement she was hoping to experience. As she contemplated the situation she found herself in, she knew she'd gotten herself in a little more than she bargained for. It wasn't often that she found herself in a tree staring down at a pack of wolves that were looking to make her their next meal. \u003c/Typography\u003e \u003c/div\u003e \u003c/div\u003e ); } In the above code first we have define the width of our drawer which is 240px. Then in the App() function we have defined our drawer which contains the list of item such as Browser,Category etc. To make our app visual we have added a AppBar on top of this we will have our drawer. The way we create drawer is using material-ui Drawer tag. Since our drawer should be visible we pass open props to our Drawer tag. Also we have passed variant to permanent so that our other content on the page does not get block out.\nNext we write out App content. The classes.toolbar property in the div tag tells that our content should be below the AppBar. Also our content should have left margin of atleast 240px so it does not hide behind our drawer. The output of the following App we something look like this:\nBut if you make your screen smaller you will notice that our Drawer sticks their and content get’s smaller and smaller. We don’t want that we want our drawer to automatically hide if the display screen is smaller so that user can read our content. So to make drawer hide we will be using material-ui Hidden tag.\nModify you code as follows:\nResponsive Drawer import React from \"react\"; import AppBar from \"@material-ui/core/AppBar\"; import Drawer from \"@material-ui/core/Drawer\"; import Toolbar from \"@material-ui/core/Toolbar\"; import Divider from \"@material-ui/core/Divider\"; import Hidden from \"@material-ui/core/Hidden\"; import List from \"@material-ui/core/List\"; import IconButton from \"@material-ui/core/IconButton\"; import MenuIcon from \"@material-ui/icons/Menu\"; import ListItem from \"@material-ui/core/ListItem\"; import ListItemText from \"@material-ui/core/ListItemText\"; import Typography from \"@material-ui/core/Typography\"; import { makeStyles } from \"@material-ui/core/styles\"; import img1 from \"./img1.jpg\"; import img2 from \"./img2.jpg\"; const drawerWidth = 240; const useStyles = makeStyles((theme) =\u003e ({ drawer: { width: drawerWidth, flexShrink: 0, }, toolbar: theme.mixins.toolbar, content: { flexGrow: 1, padding: theme.spacing(3), [theme.breakpoints.up(\"sm\")]: { marginLeft: 300, }, }, })); export default function App() { const classes = useStyles(); const [state, setSmallDevice] = React.useState(false); const handleSmallDevice = () =\u003e { setSmallDevice(!state); }; const drawer = ( \u003cdiv\u003e \u003cdiv\u003e \u003cDivider /\u003e \u003cList\u003e {[\"Search\", \"Browse\", \"Category\"].map((anchor, text) =\u003e ( \u003cListItem button key={anchor}\u003e \u003cListItemText primary={anchor} /\u003e \u003c/ListItem\u003e ))} \u003c/List\u003e \u003cDivider /\u003e \u003cList\u003e {[\"Services\", \"About Us\", \"Contact Us\"].map((anchor, text) =\u003e ( \u003cListItem button key={anchor}\u003e \u003cListItemText primary={anchor} /\u003e \u003c/ListItem\u003e ))} \u003c/List\u003e \u003c/div\u003e \u003c/div\u003e ); return ( \u003cdiv\u003e \u003cAppBar position=\"fixed\"\u003e \u003cToolbar\u003e \u003cHidden smUp implementation=\"css\"\u003e \u003cIconButton color=\"inherit\" aria-label=\"open drawer\" edge=\"start\" onClick={handleSmallDevice} \u003e {\" \"} \u003cMenuIcon /\u003e \u003c/IconButton\u003e \u003cDrawer classes={{ paper: classes.drawer, }} open={state} onClose={handleSmallDevice} \u003e {drawer} \u003c/Drawer\u003e \u003c/Hidden\u003e \u003cHidden xsDown implementation=\"css\"\u003e {\" \"} \u003cDrawer classes={{ paper: classes.drawer, }} open variant=\"permanent\" \u003e {drawer} \u003c/Drawer\u003e \u003c/Hidden\u003e \u003c/Toolbar\u003e \u003c/AppBar\u003e \u003cdiv className={classes.content}\u003e \u003cdiv className={classes.toolbar} /\u003e \u003cimg src={img1} style={{ height: \"auto\", width: \"80%\" }} alt=\"img1\" /\u003e \u003cTypography paragraph\u003e I inadvertently went to See's Candy last week (I was in the mall looking for phone repair), and as it turns out, See's Candy now charges a dollar -- a full dollar -- for even the simplest of their wee confection offerings. I bought two chocolate lollipops and two chocolate-caramel-almond things. The total cost was four-something. I mean, the candies were tasty and all, but let's be real: A Snickers bar is fifty cents. After this dollar-per-candy revelation, I may not find myself wandering dreamily back into a See's Candy any time soon. \u003c/Typography\u003e \u003cimg src={img2} style={{ height: \"auto\", width: \"80%\" }} alt=\"img1\" /\u003e \u003cTypography paragraph\u003e Spending time at national parks can be an exciting adventure, but this wasn't the type of excitement she was hoping to experience. As she contemplated the situation she found herself in, she knew she'd gotten herself in a little more than she bargained for. It wasn't often that she found herself in a tree staring down at a pack of wolves that were looking to make her their next meal. \u003c/Typography\u003e \u003c/div\u003e \u003c/div\u003e ); } The Hide tag hides the content under it up certain certain breakpoints it has reach. In the above code we have used to hide tag. So if the screen is smaller we want our drawer to hide . Also we want to display some kind of button so that the user can click on it to open the drawer. So for toggling the drawer we need some kind of state which tell that weather the drawer is open or close and based on this we open or close the drawer. We are using React.useState() hook to define our drawer state.\nSo basically we are having to dawer wrapped inside the Hide tag. One of them will be shown and other one will be hidden based on the screen size. The smUp props in the Hide tag say that if the screen size if 600px or more hide the content which is inside the tag or in other word if the screen is large enough make the drawer visible and permanent. Also in our classe.content property we have specified:\n[theme.breakpoints.up('sm')]: { marginLeft: 300, }, which will leave 300px margin from left if the drawer on open and permanent so our content does not get hidden under the drawer. The xsDown props in the second Hide tag hides the our permanent drawer or in other word it will show our toggling drawer.\n","description":"In this blog we will look at how to create a responsive drawer using material-ui and react.","tags":["programming","react"],"title":"Creating Responsive Drawer using Material-UI","uri":"/collections/programming/react/material-ui-responsive-drawer/"},{"content":"React Tutorial - Creating a social card using Material UI In this article we will create a simple social card using react’s materail UI framework.\nThe idea is to create 3 statless component namely SocialCard,CardFront,CardBack. I will also be using flip animation so that whenever someone hover the mouse on my front component it get’s flipped and show user the back side of the card.\nSocialCard : Main component which will render other components\nCardFront: Front side of our card which will consists of person detail.\nCardBack: Back side of our card which will consist of form which will have detail for contacting that person.\nNow let’s start writing the component:\nCardFront import React from \"react\"; import { makeStyles } from \"@material-ui/core/styles\"; import CardContent from \"@material-ui/core/CardContent\"; import Typography from \"@material-ui/core/Typography\"; import Card from \"@material-ui/core/Card\"; import CardMedia from \"@material-ui/core/CardMedia\"; const style = makeStyles((theme) =\u003e ({ root1: { display: \"flex\", width: 550, height: 250, }, details: { display: \"flex\", flexDirection: \"column\", }, content: { flex: \"1 0 auto\", }, photo: { height: 200, width: 200, }, })); const CardFront = () =\u003e { const classes = style(); const photo = require(\"../photo.jpg\"); return ( \u003cCard className={classes.root1}\u003e \u003cdiv className={classes.detail}\u003e \u003cCardContent className={classes.content}\u003e \u003cTypography variant=\"h2\"\u003eHatim Master\u003c/Typography\u003e \u003c/CardContent\u003e \u003cCardContent\u003e \u003cTypography variant=\"caption\" align=\"justify\"\u003e \u003ci\u003e Computer Science Engineer \u003c/i\u003e \u003c/Typography\u003e \u003c/CardContent\u003e \u003cCardContent\u003e \u003cTypography variant=\"body2\"\u003e Full Stack developer, Machine learning expert. Have work expieriance of more than 6 year \u003c/Typography\u003e \u003c/CardContent\u003e \u003c/div\u003e \u003cCardMedia className={classes.photo} image={photo} title=\"Hatim\" /\u003e \u003c/Card\u003e ); }; export default CardFront; Here we used hook API(``makeStyle) to change some changes of materil UI style.Then we have used Cardcomponent provided by material UI to display the content. For our fonts to good and presentable we will be using material UITypographycomponent. Finally to display the image in our card we useCardMedia``` component.\nOUTPUT: CardBack import React from \"react\"; import { makeStyles } from \"@material-ui/core/styles\"; import CardContent from \"@material-ui/core/CardContent\"; import Card from \"@material-ui/core/Card\"; import TextField from \"@material-ui/core/TextField\"; import TextareaAutosize from \"@material-ui/core/TextareaAutosize\"; import Button from \"@material-ui/core/Button\"; const style = makeStyles((theme) =\u003e ({ root1: { display: \"flex\", width: 550, height: 250, }, content: { flex: \"1 0 auto\", }, })); const CardBack = () =\u003e { const classes = style(); return ( \u003cCard className={classes.root1}\u003e \u003cdiv\u003e \u003cCardContent className={classes.content}\u003e \u003cTextField label=\"First Name\" /\u003e \u003c/CardContent\u003e \u003cCardContent className={classes.content}\u003e \u003cTextField label=\"Last Name\" /\u003e \u003c/CardContent\u003e \u003cCardContent className={classes.content}\u003e \u003cTextareaAutosize rowsMin={4} placeholder=\"Message\" /\u003e \u003c/CardContent\u003e \u003c/div\u003e \u003cdiv\u003e \u003cCardContent className={classes.content}\u003e \u003cTextField label=\"Email\" /\u003e \u003c/CardContent\u003e \u003cCardContent className={classes.content}\u003e \u003cTextField label=\"Subject\" /\u003e \u003c/CardContent\u003e \u003cCardContent className={classes.content}\u003e \u003cButton variant=\"contained\" color=\"primary\"\u003e {\" \"} Submit!{\" \"} \u003c/Button\u003e \u003c/CardContent\u003e \u003c/div\u003e \u003c/Card\u003e ); }; export default CardBack; The back side of our card will content a form which will consist textfields in which user will write their information and a submit button. Again we have used React’s [hook API] to change certain value of material UI. For input field we have used material’s ``TextField` wrapper component. It is complete form control component including input,label and helper text.\nOUTPUT: SocialCard ** Card.css **:\n.card { transform: translate(-50%, -50%); position: absolute; } .front, .back { overflow: hidden; backface-visibility: hidden; position: absolute; transition: transform 0.6s linear; } .front { transform: perspective(500px) rotateY(0deg); } .back { transform: perspective(500px) rotateY(180deg); } .card:hover \u003e .front { transform: perspective(500px) rotateY(-180deg); } .card:hover \u003e .back { transform: perspective(500px) rotateY(0deg); } import React from \"react\"; import { makeStyles } from \"@material-ui/core/styles\"; import CardContent from \"@material-ui/core/CardContent\"; import Grid from \"@material-ui/core/Grid\"; import CardFront from \"./CardFront\"; import CardBack from \"./CardBack\"; import \"./Card.css\"; const style = makeStyles((theme) =\u003e ({ root: { marginTop: 75, display: \"flex\", width: 550, height: 250, }, content: { flex: \"1 0 auto\", }, })); const SocialCard = () =\u003e { const classes = style(); return ( \u003cGrid container justify=\"center\" alignItem=\"cetner\"\u003e \u003cdiv className={classes.root}\u003e \u003cdiv className=\"card\"\u003e \u003cdiv className=\"front\"\u003e \u003cCardContent className={classes.content}\u003e \u003cCardFront /\u003e \u003c/CardContent\u003e \u003c/div\u003e \u003cdiv className=\"back\"\u003e \u003cCardContent className={classes.content}\u003e \u003cCardBack /\u003e \u003c/CardContent\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/Grid\u003e ); }; export default SocialCard; This component bind the CardBack and CardFront component together. The Grid component of material UI is used to place the card at the center of the screen. For flipping animation we have used css transition and transform attributes.\n","description":"In this article we will create social card using Martrial-UI framework and react.","tags":["programming","react"],"title":"Creating Social Card using React and Material UI","uri":"/collections/programming/react/react-social-card/"},{"content":"React js components : Stateful vs stateless What is state ? State is an object which is declared inside the constructor method of the class.They are mutable and are used to interact between different component of the class.\nStatefull Components Statefull components are the component which have state. They are created in constuctor method of the class. They are refered as class component and are extended by React.Component. Statefull component or container deal with the data which are frequently updated. Statefull component recieve both state and props. The state are mutable and are updated with this.setState() method of the class.\nStateless Components Statless component are static and often act like container in an app. These component only recieve props. They act upon the data which are pass to them as props.Stateless component are written as pure javascript function whoes parameter value does not change.Statless component are sometimes also referred as ‘presentational component’. They are used when you know that the information is static and will never change.Whenever writting component we should always use statless component whenever possible.\nStatefull vs Stateless The main difference between these state are that one has state and other does not. That means stateful component always used to track data which are frequently changing while stateless component prints out data which are passed to them as props or they will always render the samething.\nNow let’s look at example of statefull and stateless component.\nFirst let’s look at the data to be rendered:\nconst data = [ { type: \"Flowers\", kind: \"Lotus\", }, { type: \"Animals\", kind: \"Cat\", }, { type: \"Animals\", kind: \"Dog\", }, { type: \"Flowers\", kind: \"Balsam\", }, { type: \"Animals\", kind: \"Snake\", }, { type: \"Flowers\", kind: \"Sunflower\", }, { type: \"Flowers\", kind: \"Rose\", }, ]; Now let’s create our parent class or statefull class which will contain the state of this data.\nclass Statefull extends React.Component { constructor(props) { super(props); this.state = { data: data }; } clickToFileter(kind) { let types = []; if (kind === \"all\") { types = data; } else { types = data.filter((value) =\u003e { if (value.type === kind) return true; }); } this.setState({ data: types }); } render() { return ( \u003cdiv className=\"App\"\u003e \u003ch3\u003e Types Lists: \u003c/h3\u003e \u003cbutton onClick={(e) =\u003e { this.clickToFileter(\"all\"); }} \u003e {\" \"} All{\" \"} \u003c/button\u003e \u003cbutton onClick={(e) =\u003e { this.clickToFileter(\"Animals\"); }} \u003e {\" \"} Animals{\" \"} \u003c/button\u003e \u003cbutton onClick={(e) =\u003e { this.clickToFileter(\"Flowers\"); }} \u003e {\" \"} Flowers{\" \"} \u003c/button\u003e {this.state.data.map((value, idx) =\u003e { return \u003cStateless key={idx} kind={value.kind} /\u003e; })} \u003c/div\u003e ); } } In above code I have given three button which will change the state of the component. And this changed state is pass through Statless component as props which than display the content of the state.\nconst Stateless = ({ kind }) =\u003e { return \u003ch3\u003e{kind}\u003c/h3\u003e; }; OUTPUT: References https://yukcoding.id/stateful-stateless-component-react/ ","description":"It this blog we will look at difference between react stateful and stateless components","tags":["programming","react"],"title":"Understand React Stateful and Stateless components","uri":"/collections/programming/react/react-statefull-stateless/"},{"content":"What are state and props in Reactjs In React js we develop different component to develop complex UI.To create dataflow between different component React uses state and porps. Because of state and props it is able to render component with dynamic data.\nProps Props stands for properties. It is an object which look like HTML attributes and also works similar to the HTML attrubute.Props are immutable i.e. we cant not change the value of props. Props are used for sending the data from one component to other component.They are treated as pure javascript function whoes parameter value cannot be changed\nLet’s have a quick look at how props works:\nimport React from \"react\"; import \"./state_prop.css\"; class Demo extends React.Component { constructor(props) { super(props); } render() { return ( \u003cdiv className=\"App\"\u003e \u003cimg src={this.props.img_url} /\u003e \u003cp className=\"text-primary\"\u003e {this.props.time} \u003c/p\u003e \u003c/div\u003e ); } } export default Demo; The above code is simple. We have a class Demo that render the value of props passed to it which is nothing but and image file and the current time. Now let’s look at our index.js file where we have initialize our props value.\nimport React from \"react\"; import ReactDOM from \"react-dom\"; import \"./index.css\"; import Demo from \"./state_prop\"; import * as serviceWorker from \"./serviceWorker\"; var date = new Date(); var t = date.getHours() + \":\" + date.getMinutes() + \":\" + date.getSeconds(); ReactDOM.render( \u003cDemo img_url={require(\"./img1.jpg\")} time={t} /\u003e, document.getElementById(\"root\") ); // If you want your app to work offline and load faster, you can change // unregister() to register() below. Note this comes with some pitfalls. // Learn more about service workers: https://bit.ly/CRA-PWA serviceWorker.unregister(); As you can see we have passed to props img_url and time.As we have disccused earlier props are immutable so if you try to change the value of the props it will give you error.\nIf you have done everything write the output component will some what look like this:\nState State are used to store the data of that component.They are used to update compnent whenever some event occur for example clicking button,pressing some key etc.Whenever some event occur the component will re-render itself. State are mutable i.e. we can change the value of state by using setState() function. Whenever a class inherits the class React.Component it’s constructor will automatically assigns attribute state to the class with intial value is set to null.\nLet’s see how state works with an example. We will try to modify our last component but here instead of displaying time once we will use state to update the clock on ever second.\nimport React from \"react\"; import \"./state_prop.css\"; class Demo extends React.Component { constructor(props) { super(props); this.state = { time: null }; } componentDidMount() { setInterval(() =\u003e { var date = new Date(); this.setState({ time: date.getHours() + \":\" + date.getMinutes() + \":\" + date.getSeconds(), }); }, 1000); } render() { return ( \u003cdiv className=\"App\"\u003e \u003cimg src={this.props.img_url} /\u003e \u003cp className=\"text-primary\"\u003e {this.state.time} \u003c/p\u003e \u003c/div\u003e ); } } export default Demo; As you can see the initial value of the state is null. We have used here setInterval() function to trigger the event in every second.As soon as the event is triggred we change to value of the state by using setState() function and the component re-renders itself.\nTry this, if everything went fine you will see component updating its value on every second.\n","description":"We will look at what are states and props in react","tags":["programming","react"],"title":"Understanding State and Props in React","uri":"/collections/programming/react/react-state-props/"},{"content":"OpenCV - Detecting corner location in subpixel(cornerSubPix()) using C++ A digital photo’s smallest component is called a “pixel”. Between pixel information is just inaccessible to the human sight. There are some applications that require more accuracy than a camera can provide. Exact measurements are needed, for example, when recreating a 3D object from an image. Mathematical strategies were developed to improve the accuracy of corner recognition.\nWhy there is need of subpixel accuracy ? Take a look at this square! It is zoomed in quite a bit, so you can see the pixel-by-pixel breakdown. Look at this image and see if you can spot the corner.\nAs you can see, there isn’t a single pixel in the corner. In reality, it’s nearly impossible to get corners to line up perfectly with pixels.\nThe Shi-Tomasi and Harris methods, for example, will give you something like this (53, 786). Scientists and others want a place like this (53.786, 786.110).\nHere we have absolute pixel-perfect precision on display. Despite the fact that 53.786, 786.110 is not available, you’ve confirmed that the corner is exactly at this address.\nWhat’s the point of going through the hassle of figuring out the decimal? They are one of the most crucial aspects of an image. Also, you’ll need the extra precision in a variety of circumstances:\nStereo vision 3D reconstruction Camera calibration Tracking As a result, a lot of effort has gone into improving accuracy and speed.\nSubpixel corners in OpenCV Subpixel corners can be found using OpenCV’s built-in feature. Using the dot product methodology, it refines corners identified by previous systems, such as the corner harris detector. Iterative refining occurs once a termination criteria is met.\nOpenCV cornerSubPixel function void cv::cornerSubPix( InputArray image, InputOutputArray corners, Size winSize, Size zeroZone, TermCriteria criteria )\nParameter:\nimage: Input image. corners: As the name suggests, this array stores the approximate corners at the start of the process. As a result of the function, this array is modified with revised corner positions. winSize: This function relies on a number of equations to perform its job. Several pixels around the corner are used to get this effect, as well. If you use winSize, you may control how many pixels are extracted from a certain window. zeroZone: This function also solves numerous equations using the same way. When it comes to “solving” problems, a matrix is used. This matrix is inverted in order to find a solution to this problem. But some matrices cannot be inverted. Some pixels surrounding the corner are ignored to prevent this. That area is zeroZone. criteria: Criteria for stopping the iterative corner refinement procedure. To put it another way, the process of refining the angle of the corner comes to an end either once a set of conditions is met.(CV_TERMCRIT_ITER or CV_TERMCRIT_EPS or both) OpenCV - TermCriteria TermCriteria() is commonly used to generate the structure we need. To terminate the method, we can specify the number of iterations to be performed or the convergence metric to achieve a certain value in the CV_TERMCRIT_ITER or CV_TERMCRIT_EPS arguments (respectively). They specify whether one or both of these criteria should be used to terminate the algorithm based on their values.\nIn order to be able to halt when either limit is reached, we have both types of termination available: CV_TERMCRIT_ITER | CV_TERMCRIT_EPS\nC++ code for calculating sub-pixel corner #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e cv::Mat src, src_gray; int maxCorners = 10; const int MAXTRACKBAR = 25; const std::string source_window = \"Source Image\"; void trackFeatures( int, void* ); int main( int argc, char** argv ) { if(argc \u003c 2 ) { std::cout \u003c\u003c \"Usage: \" \u003c\u003c argv[0] \u003c\u003c \" \u003cInput image\u003e\\n\"; return -1; } src = cv::imread(argv[1]) ; if( src.empty() ) { std::cout \u003c\u003c \"Could not open or find the image!\\n\\n\" ; std::cout \u003c\u003c \"Usage: \" \u003c\u003c argv[0] \u003c\u003c \" \u003cInput image\u003e\\n\"; return -1; } cv::cvtColor( src, src_gray, cv::COLOR_BGR2GRAY ); cv::namedWindow( source_window ); cv::createTrackbar( \"Max corners:\", source_window, \u0026maxCorners, MAXTRACKBAR, trackFeatures ); trackFeatures( 0, 0 ); cv::waitKey(); return 0; } void trackFeatures( int, void* ) { maxCorners = MAX(maxCorners, 1); std::vector\u003ccv::Point2f\u003e corners; double qualityLevel = 0.01; double minDistance = 10; int blockSize = 3, gradientSize = 3; bool useHarrisDetector = true; double k = 0.04; cv::Mat copy = src.clone(); cv::goodFeaturesToTrack( src_gray, corners, maxCorners, qualityLevel, minDistance, cv::Mat(), blockSize, gradientSize, useHarrisDetector, k ); std::cout \u003c\u003c \"** Number of corners detected: \" \u003c\u003c corners.size() \u003c\u003c \"\\n\"; int radius = 8; for( size_t i = 0; i \u003c corners.size(); i++ ) { std::cout \u003c\u003c \" -- Original Corner Detected [\" \u003c\u003c i \u003c\u003c \"] (\" \u003c\u003c corners[i].x \u003c\u003c \",\" \u003c\u003c corners[i].y \u003c\u003c \")\\n\" ; cv::circle( copy, corners[i], radius, cv::Scalar(0,255,0), cv::FILLED ); } cv::Size winSize = cv::Size( 5, 5 ); cv::Size zeroZone = cv::Size( -1, -1 ); cv::TermCriteria criteria = cv::TermCriteria( cv::TermCriteria::EPS + cv::TermCriteria::COUNT, 40, 0.001 ); std::vector\u003ccv::Point2f\u003e refinedCorners(corners); cv::cornerSubPix( src_gray, refinedCorners, winSize, zeroZone, criteria ); for( size_t i = 0; i \u003c refinedCorners.size(); i++ ) { std::cout \u003c\u003c \" -- Refined Corner [\" \u003c\u003c i \u003c\u003c \"] (\" \u003c\u003c refinedCorners[i].x \u003c\u003c \",\" \u003c\u003c refinedCorners[i].y \u003c\u003c \")\\n\" ; cv::circle( copy, refinedCorners[i], radius, cv::Scalar(0,0,255), cv::FILLED ); } cv::namedWindow( source_window.c_str() ); cv::imshow( source_window, copy ); } Code Explanation We start by reading our input image using cv::imread() and converting it into gray scale using cv::cvtColor().\nThen we use goodFeaturesToTrack() function to find corners in the image. In this function we have used corner Harris detection algorithm to detect corner in the image. The coordinate of image corners are stored in corners variable.\nOut of this coordinates in corner variable we find corners at sub-pixel level using cv::cornerSubPix() function.\nOutput In this output green circle represent corner detected using Harris Corner detection algorithm and red circle represent the refined corner which is calculated using cv::cornerSubPix() function.\nReferences Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://stackoverflow.com/questions/18955760/how-does-cvtermcriteria-work-in-opencv https://vovkos.github.io/doxyrest-showcase/opencv/sphinx_rtd_theme/page_tutorial_corner_subpixeles.html https://aishack.in/tutorials/subpixel-corners-increasing-accuracy/ ","description":"In this tutorial we will look cornerSubPix() function which is use to find corner at sub-pixel level","tags":["programming","cpp","opencv"],"title":"OpenCV - Detecting corner location in subpixel(cornerSubPix()) using C++","uri":"/collections/programming/cpp/opencv/corernsubpixel/"},{"content":"Best Coding Apps for iOS and ipadOS The code has led to some of the world’s most innovative technologies. While all coders strive to enhance their skills, those who can code on the go will have an advantage over their competitors in this competitive field.\nIf you want to boost your chances of becoming a successful coder, use these apps for iOS and iPadOS.\nProgramming Hub (Free, subscription available) If you’re interested in learning more than just coding, the Programming Hub app is a great choice. You can take lessons in artificial intelligence, IT principles, and coding, for example.\nDescriptions of each lesson are included in the guide, as well as an estimate of the amount of money you may expect to earn after following the course. After each course, you can use a compiler to test your coding skills.\nSwift Playgrounds (Free) Despite being exclusive to the iPad, Swift Playgrounds is a fun and easy way to learn Apple’s programming language. Students and developers can learn how to create their own apps on the platform by using the app.\nWhile you’re solving riddles using coding, the app also gives you access to an editor where you may create your own apps and games! If your app is accepted into the App Store, you’ll be the first to know about it thanks to Swift Playgrounds. Using this application, which is absolutely free, you may learn how to develop better apps with Swift.\nKoder (Free) Koder is a great tool for those who are already familiar with coding but wish to enhance their skills. This is one of the most comprehensive coding programmes, with more than 80 various programming languages.\nAdditionally, the software will help you write code that is more efficient and less likely to contain errors, in addition to providing syntax highlighting and auto-complete functionality.\nThis free programme for iOS and iPadOS makes file sharing for local and remote connections simple. As a result, you’ll always be able to find the specific files you need when travelling.\nSololearn (Free, subscription available) Sololearn, which claims to have the largest library of online coding programmes, offers a large number of free coding classes. Examples include Python and C++. Java and jQuery\nEvery course you complete on the Sololearn platform comes with a certification that you can add to your LinkedIn page or CV. In addition to learning how to code, the app allows you to improve your skills by competing with other coders.\nYou’ll need to upgrade to the premium edition of the app to access all courses and remove the ads.\nBuffer ($4.99) A powerful editor will be needed when you’re ready to code on the move, and Buffer may be able to supply it. With this native iOS app, you can customise your UI with a variety of themes.\nImprove the accessibility of your resources by connecting Buffer to services such as GitHub, DropBox, and many others. Syntax highlighting and auto-complete are available, exactly like in the other code editors on this list. It is possible to test your code on Safari when you are ready.\nTextastic ($9.99) One of the industry’s greatest full iOS development tools is Textastic. Koder’s code editor allows you to write in more than 80 different computer languages, and this one is no exception. TextMate and Sublime Text 3 are also supported.\nBased on native iOS and iPadOS APIs, the app has an advantage over other similar apps in terms of speed and responsiveness. It’s possible to see the results of complex functions without any delays or problems. Textastic’s suggested features include importing and exporting files, as well as cursor navigation for easy selection.\nPythonista 3 ($9.99) It’s hard to find a better tool than Pythonista 3 for learning Python. Python coders will know many of the libraries, but it also contains native iOS capabilities like location data, contacts, reminders and images.\nPythonista 3 delivers desktop-like capabilities such as syntax highlighting, code completion, outline views, and multiple tab support on your mobile devices, so you won’t have any trouble learning on the go.\nGrasshopper (Free) A basic and easy-to-use software that runs in two ways, Grasshopper will appeal to beginners who want to learn Javascript. Learn Javascript by answering multiple-choice questions or by writing code through exercises.\nIt starts off with simple missions, but as the game progresses they grow increasingly difficult. As you advance through the lessons, you’ll receive real-time feedback and accolades for your work.\nEnki (Free, subscription available) By employing flashcards, Enki can help you recall specific coding facts if you use it in conjunction with other iOS coding tools. Before asking you a question, Enki will offer you an informational card with code facts.\nDespite the fact that Enki won’t teach you how to code, it will allow you to contextualise your coding talents and support others in their learning\nMimo (Free, subscription available) Whatever your level of expertise, Mimo will keep you on track. It’s a language-learning programme with a similar interface to Duolingo. You can get points based on how much you learn in a given day.\nMimo will create a collection of lessons that are specifically targeted to your needs based on your goals.\nThe app maintains track of how many consecutive days you’ve completed your classes in order to measure your progress. The most common programming languages will be used to teach you how to code, and you’ll gain new abilities by using different learning approaches.\n","description":"HTML \u0026 CSS courses from top universities and industry leaders. Learn HTML \u0026 CSS online with courses like HTML, CSS, and Javascript for Web Developers and Web Design for Everybody Basics of Web Development \u0026 Coding.","tags":["review"],"title":"Best Coding Apps for iOS and ipad0S","uri":"/collections/reviews/best-coding-app-ios-ipados/"},{"content":"Customize Your GNOME Desktop A lot of people prefer GNOME’s minimalist design, which is quite straightforward and streamlined for a Linux GUI. In spite of its simplicity, GNOME may be customised to suit your needs. It is possible to customise the top bar, window title bars, icons, cursors, and many other UI features with GNOME Tweaks and the User Themes Extension.\nNote: It is necessary to install Tweaks and enable the user themes extension in your browser before you may change your GNOME theme and customise your desktop.\nInstalling GNOME Tweaks Fire up your terminal and write the following command to install gnome-tweaks:\nFor Fedora based distros:\nsudo dnf install gnome-tweaks For Debian distros:\nsudo apt install gnome-tweaks For Ubuntu based distros:\nsudo add-apt-repository universe \u0026\u0026 sudo apt install gnome-tweaks For Arch/Manjaro:\nsudo pacman -S gnome-tweaks Customize GNOME Themes Enabling user themes Launch Tweaks and click Extensions to enable the user themes extension. User themes can be enabled by clicking on the slider next to it.\nSelect Theme Having completed the criteria, you’re ready to search for and download themes. gnome-looks is an excellent place to find new themes.\nLeft of the page is a list of topic categories. Downloading a theme is the next step after you’ve found one you like. You may need to create the directory first. I downloaded the.tar file directly to the ~/.themes directory in my home directory.\nmkdir ~/.themes Having downloaded the file, extract the archive to ~/.themes location on your computer. Save some disc space by deleting the.tar.xz file.\ntar xvf theme_name.tar.xz Applying Theme Tweaks’ Appearance section is where you’ll find your new theme to apply. You can choose from a variety of choices for each part of your desktop in this section.\nCustomize GNOME icons Select and download icons from gnome-looks.After you have downloaded icons next set is to create ~/.icons directory and extract *.tar.xz file in ~/.icons directory.\nmkdir ~/.icons Next step is to open your gnome-tweak application got to appereance section and there you will find icons. When you open the drow down list of the icons you will see your icon theme listed.\nJust select it and you icons theme will change instantly.\nEnabling Extensions You could get variety of extension at GNOME Extension website\nSome of the famous extension are:\nDash to panel User Themes Tray Icons Arch menu ShellTile Animation Tweaks Install all of the extensions listed below. Activate the appropriate extensions by clicking on the “OFF” button located on your browser’s right sidebar.\nNote: There are certain GNOME Extensions that do not require setups to work. But some extension require little bit of configuration for eg. Dash to panel or Arch menu\nCustomize GNOME Fonts The default typefaces on your desktop are making you grumpy. The GNOME Tweaks Tool lets you download new fonts and apply them to your system. You can change the font for the interface text, document text, monospace text, and legacy window titles, as shown in the screenshot below.\nUsing GNOME Tweaks, you may also adjust font hinting and initialiasing as well as scaling.\nTweaking your Keyboard \u0026 Touchpad You can adjust the keyboard settings and set up extra layout options here, as well. My favourite feature is the ability to disable your laptop’s touchpad when you’re typing.\nThis is especially handy if you are typing quickly and your palm accidently contacts the touchpad, causing the cursor to jump to a random spot, slowing down your process and increasing the likelihood of making an error.\nCustomzie GNOME Windows and Titlebars In the Tweak Tool, you may change the settings for the programme window by selecting this item from the menu. Options in the titlebar can be maximised or minimised here. These options can also be moved from the top-right corner to the top-left corner of the programme window.\nA double-click, middle-click, and secondary-click can also be configured here to change the behaviour of the programme window when they are performed.\n","description":"A lot of people prefer GNOME's minimalist design, which is quite straightforward and streamlined for a Linux GUI.  In spite of its simplicity, GNOME may be customised to meet your needs.  It's possible to customise the look of GNOME Tweaks, window title bars, icons, and cursors with the user themes extension.","tags":["linux"],"title":"Customize Your GNOME Desktop","uri":"/collections/linux/gnome-customize/"},{"content":"calHist() - Calculate histogram using openCV and C++ In practically every element of computer vision, histograms are used. For threshold, we employ gray-scale histograms. For white balance, we employ histograms. For object tracking in photos, such as with the CamShift technique, we use colour histograms. Color histograms are used as features, and colour histograms in several dimensions are included.\nIn a more abstract sense, they form the HOG and SIFT descriptors from histograms of visual gradients.\nA histogram is also a bag-of-visual-words representation, which is widely employed in image search engines and machine learning. And, more than likely, this isn’t the first time you’ve seen histograms in your studies.\nSo, why do histograms come in handy?\nBecause histograms depict a set of data frequency distribution. And it turns out that looking at these frequency distributions is a dominant method to develop simple image processing techniques… as well as really powerful machine learning algorithms.\nThis blog post will summarize image histograms, as well as how to calculate colour histograms from video using openCV and C++.\nWhat is Histogram ? You might think of a histogram as a graph or plot that shows how an image’s intensity distribution is distributed. It’s a graph with pixel values (usually ranging from 0 to 255) on the X-axis and the number of pixels in the picture on the Y-axis.\nIt’s just a different way of looking at the image. When you look at the histogram of an image, you may get a sense of the image’s contrast, brightness, intensity distribution, and so on.\nAlmost all image processing software today includes a histogram feature.\ncalHist() function in openCV cv.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])\nSo now we use calcHist() function to find the histogram. Let’s familiarize with the function and its parameters :\nimages : this is the uint8 or float32 source image. “[img]” should be written in square brackets.\nchannels: It is the channel index for which the histogram is calculated. If the input is a gray-scale image, the value is [0].\nTo calculate the histogram of the blue, green, or red channel in a colour image, pass [0], [1], or [2].\nmask: It is given as “None” to find the histogram of the entire image. But if you want to find histogram of a particular region of image, create a mask image for that and give it as a mask.\nhistSize: Our BIN count is represented by histSize. Must be enclosed in square brackets. We pass [256] for full scale.\nrange: It’s usually [0,256].\nCode #include \"opencv2/highgui.hpp\" #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e const int histSize = 256; void drawHistogram(cv::Mat\u0026 b_hist,cv::Mat\u0026 g_hist,cv::Mat\u0026 r_hist) { int hist_w = 512; int hist_h = 400; int bin_w = cvRound((double)hist_w / histSize); cv::Mat histImage(hist_h, hist_w, CV_8UC3, cv::Scalar(0, 0, 0)); cv::normalize(b_hist, b_hist, 0, histImage.rows, cv::NORM_MINMAX, -1, cv::Mat()); cv::normalize(g_hist, g_hist, 0, histImage.rows, cv::NORM_MINMAX, -1, cv::Mat()); cv::normalize(r_hist, r_hist, 0, histImage.rows, cv::NORM_MINMAX, -1, cv::Mat()); for (int i = 1; i \u003c histSize; i++) { cv::line( histImage, cv::Point(bin_w * (i - 1), hist_h - cvRound(b_hist.at\u003cfloat\u003e(i - 1))), cv::Point(bin_w * (i), hist_h - cvRound(b_hist.at\u003cfloat\u003e(i))), cv::Scalar(255, 0, 0), 2, 8, 0); cv::line( histImage, cv::Point(bin_w * (i - 1), hist_h - cvRound(g_hist.at\u003cfloat\u003e(i - 1))), cv::Point(bin_w * (i), hist_h - cvRound(g_hist.at\u003cfloat\u003e(i))), cv::Scalar(0, 255, 0), 2, 8, 0); cv::line( histImage, cv::Point(bin_w * (i - 1), hist_h - cvRound(r_hist.at\u003cfloat\u003e(i - 1))), cv::Point(bin_w * (i), hist_h - cvRound(r_hist.at\u003cfloat\u003e(i))), cv::Scalar(0, 0, 255), 2, 8, 0); } cv::namedWindow(\"calcHist Demo\", cv::WINDOW_AUTOSIZE); cv::imshow(\"calcHist Demo\", histImage); } int main(int argc, char **argv) { cv::Mat src, dst; cv::VideoCapture cap; if (argc != 2) cap.open(0); else cap.open(argv[1]); if (!cap.isOpened()) { std::cerr \u003c\u003c \"Failed to load webcam/Video ...\\n\"; return -1; } for (;;) { if(!cap.read(src)) { std::cerr \u003c\u003c \"Cannot read file\\n\"; break; } cv::imshow(\"Src\", src); std::vector\u003ccv::Mat\u003e bgr_planes; cv::split(src, bgr_planes); float range[] = {0, 256}; const float *histRange = {range}; bool uniform = true; bool accumulate = false; cv::Mat b_hist, g_hist, r_hist; cv::calcHist(\u0026bgr_planes[0], 1, 0, cv::Mat(), b_hist, 1, \u0026histSize, \u0026histRange, uniform, accumulate); cv::calcHist(\u0026bgr_planes[1], 1, 0, cv::Mat(), g_hist, 1, \u0026histSize, \u0026histRange, uniform, accumulate); cv::calcHist(\u0026bgr_planes[2], 1, 0, cv::Mat(), r_hist, 1, \u0026histSize, \u0026histRange, uniform, accumulate); drawHistogram(b_hist,g_hist,r_hist); if (cv::waitKey(30) == 27) break; } return 0; } Explanation We start the code by first reading our input file, which is a video frame by frame using cap.read() method.\nUsing split() function we divide multi-channel array (i.e RGB) into separate single-channel array which we store in bgr_planes.\nThen we calculate histogram of each plane and stores value in the variable b_hist,g_hist,r_hist.\nIn our histogram we want our bins to have same size and we want to clear our histogram at the beginning therefore, we set uniform and accumulate to `true.\nAfter calculating histogram we create an image histImage to display our histogram.\nThen we simply draw the line using cv::line at each pixel for each channel i.e b_hist,g_hist,r_hist.\nOutput Refrences Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://www.picturecorrect.com/tips/the-histogram-explained/ https://docs.opencv.org/3.4/d8/dbc/tutorial_histogram_calculation.html ","description":"How to compute picture histograms using OpenCV and the \"cv::calcHist()\" function will be covered in this lesson.","tags":["programming","cpp","opencv"],"title":"calHist() - Calculate histogram using openCV and C++","uri":"/collections/programming/cpp/opencv/calculate-histogram/"},{"content":"7 Best Machine Learning Libraries You Should Know in 2021 Machine learning is currently the most talked-about topic in artificial intelligence.\nEngineering, medicine, business, social science, and other professions are all affected.\nMachine learning is easier than ever using today’s machine learning libraries, which include Python, C++, Java, Julia, and R, among others.\nIf you want to get started in this exciting field, here are some prominent machine learning libraries to look at.\n1. Pytorch Pytorch was created by Facebook and launched in 2016.\nPytorch is an open-source library based on the Torch framework that is well-known for its extensive use in computer vision, deep learning, and natural language processing. Pytorch, like Keras and Tensorflow, allows you to process datasets on the CPU.\nIn addition, if your dataset is huge, they include a GPU processor to execute your calculations. It’s also tensor-based. The library now offers C++ and Java bindings besides Python.\nTorchvision, torchtext, torchaudio, and TorchServe are among Pytorch’s subsidiary libraries, besides other utilities. These libraries are part of Pytorch’s machine learning capabilities, and you’ll encounter them while developing Pytorch models. Pytorch is simple to comprehend, as long as you’re familiar with machine learning ideas, thanks to its comprehensive tutorial-based documentation.\nYou can also use Pytorch to convert your datasets into a machine-readable format. As a result, it’s an ideal package for data preparation. Pytorch can always perform feature extraction, data cleaning, data splitting, and hyper-parameter tweaking.\n2. Keras Keras is a machine learning library included in TensorFlow. However, it differs because it is a higher-level API that comes packaged with TensorFlow. It’s also more human-friendly and Python-based. As a result, it is more implementable since it provides clear documentation that is easy to narrow down for machine-learning newbies.\nKeras has a wide range of machine learning capabilities, making it ideal for training both organised and unstructured data.\nFor training and testing your dataset, however, the package includes text and image-based algorithms. A unique feature of Keras is that it keeps you focused on the library, as it provides everything you need for your project in one piece. So you’ll hardly need to branch out to borrow utilities from other libraries. Hyper-parameter tuning, feature selection, rich data preprocessing layers, and data cleaning are some of its spectacularly built-in features.\nWith Keras, you can read images and texts directly from split folders in a parent directory and get a labeled dataset from them. And if your data is large and doesn’t sit in your machine memory, Keras offers a high-performance dataset object option. You can always switch to that.\n3. TensorFlow TensorFlow, which was introduced by Google in 2015, is more of a framework than a library. It’s a C++-based open-source library that operates by tracking dataflow graphs. TensorFlow is a powerful and adaptable tool that comes with several other built-in, unitary libraries for doing machine learning calculations.\nTensorFlow is a scalable platform for developing machine learning ideas such as artificial neural networks (ANN), deep neural networks, and deep learning.\nIn addition to Python, Tensorflow also supports Java, C++, Julia, Rust, Ruby, and JavaScript.\nWhile we may use TensorFlow with programming languages other than Python, using its cores with Python is easier because it completely supports TensorFlow’s implementation. If you need to switch versions later, development pipelines in other languages may cause API version compatibility issues. Unlike Keras, TensorFlow’s documentation is comprehensive, yet it may be too diverse for beginners to understand.\nHowever, it has a powerful community behind it, and there are many open-source TensorFlow examples available. TensorFlow has a benefit over Keras in that it may be used directly without Keras.\nOf course, because Keras is a branching class of TensorFlow, you can’t say the same for it.\n4. mlpack mlpack was released in 2008 and was written in C++ using the Armadillo linear algebra framework.\nIt, like Mlib Spark, allows you to use succinct and legible lines of code to apply most of the existing machine learning techniques and concepts straight to your dataset.\nIt enables CLI execution, which allows you to run your code and obtain instant replies, besides being available in programming languages like Python, C++, Go, and Julia. Although it enables binding with several other languages, utilising mlpack with another programming language on huge datasets that require complicated calculation may not be the best choice.\nAs a result, mlpack’s scalability with languages other than C++ is frequently an issue. If you’re a machine learning newbie who knows C++, you can still give it a shot. For various programming languages, there are easy-to-follow guidelines and examples available in the documentation.\nMlpack employs low-level code to conduct difficult to simple machine learning tasks quickly since it runs calculations using C++ principles.\n5. Scikit-Learn Scikit-learn, sometimes known as sklearn, is a Python-based machine learning framework that was first released in 2010.\nThe library is useful for a variety of machine learning tasks, including the modelling of both featured and unfeatured datasets.\nRight out of the box, Scikit-learn includes supervised techniques such as linear and logistic regression models, support vector machine (SVM), Naive Bayes, Decision Trees, and Nearest Neighbors, among others.\nIt also contains a wealth of unsupervised learning methods, such as clustering, the Gaussian model, neural network models, and more. Both supervised and unsupervised models are supported by scikit-learn. Because it’s totally Python-based, it’s an excellent place to start if you’re new to Python or machine learning.\nIf you’re new to machine learning or data science, start using scikit-supervised learn’s learning features. It is, on the whole, more beginner-friendly than the other libraries on this list.\nScikit-learn, unlike the other libraries described before, is heavily reliant on Numpy and Scipy to conduct high-performance mathematical calculations. It also makes use of Matplotlib to create engaging story-telling graphics.\n6. Mlib Spark Here’s something from Apache Spark that you might find useful.\nMlib Spark, which was released and declared open-source in 2010, runs machine learning algorithms using iterative calculations.\nMlib may use Hadoop or local data sources and workflows because of its iterative nature. It also has the ability to run sophisticated logic in a short amount of time.\nIn the end, it remains one of the quickest machine learning libraries available. It can do regression, clustering, classification, and recommendation models, among other machine learning algorithms.\nIn terms of data preparation and pattern mining, it also shines. The library is flexible, providing an API that works with Scala, Python, R,\nBecause Mlib Spark is a Spark embed, it updates with each Spark release. Mlib Spark comes with detailed documentation, so even a novice may quickly learn how to use it.\nHowever, one disadvantage is that it only supports a few programming languages, which may be a problem if you are unfamiliar with the languages it currently supports.\n7. Theano If you’re looking for a library that will help you break down troublesome issues into flexible algorithms, Theano is a good option.\nTheano is a strong library for running tiny to high-performance calculations that was created in 2007 by Yoshua Bengio in Montreal, Canada. Theano, like Scikit-Learn, uses Numpy to do numerical calculations. The library generates low-level C code and supports GPU-based computations.\nThis allows Theano to perform mathematical calculations faster, no matter how large they are.\nTensors are also used in its deep learning algorithms. Regardless of the data type, you may transform your dataset to legible float, binary, or integer points with Theano.\nHowever, you may not receive adequate community support. Because Theano isn’t as well-known as the other libraries we described earlier, this is the case.\nThat doesn’t make it any less approachable for newcomers. The lesson in the manuals is simple to follow. It is ideal for developing scalable machine learning models because of its ability to simplify complex arrays and optimise infinite computations.\nWhich libraries you should learn ? Although we’ve covered some of the most popular machine learning libraries, picking the best one might be difficult because they all serve very similar objectives and have only a few distinctions in their capabilities.\nIf you’re just getting started, starting with a beginner-friendly library like Scikit-Learn or Keras is a good idea.\nAside from that, selecting a library specifically for a project can help you reduce the number of complications in your development pipeline.\nHowever, studying the principles of machine learning through classes and tutorials is beneficial.\n","description":"Up ahead, we will discuss 7 of the best machine learning libraries that are preferred by machine learning enthusiasts and professionals around the globe.","tags":["reviews"],"title":"7 Best Machine Learning Libraries You Should Know in 2021","uri":"/collections/reviews/machine-learning-libraries-2021/"},{"content":"OpenCV - Image Roatation using C++ Image editing is becoming increasingly popular as mobile phones have built-in capabilities that allow you to crop, rotate, and do other things with your images.\nA common image processing operation is to rotate images by a specified angle. Although it appears to be a little complicated, OpenCV has some built-in functions that make it simple. Here is a straightforward OpenCV C++ code for rotating a picture. In this case, I’m using a track bar to dynamically modify the spinning angle.\nFundamental Image Transformation Operations Image rotation and translation are two of the most fundamental procedures in image editing. Both belong to the larger class of Affine transformations. Before delving into more complex transformations, you should first understand how to rotate and translate an image using OpenCV’s functions.\nImage Rotation in OpenCV By specifying a transformation matrix M, you can rotate an image by a specific angle theta. This matrix is often of the following form:\nOpenCV allows you to specify the image’s centre of rotation as well as a scale factor to resize it. In this situation, the transformation matrix is altered.\nIn the above matrix:\nwhere c_x \u0026 c_y are the coordinates along which the image is rotated.\nSteps in Rotating Image using OpenCV First, you must determine the centre of rotation. This is usually the middle of the image you’re attempting to rotate. Create the 2D-rotation matrix next. The above-mentioned getRotationMatrix2D() method is provided by OpenCV. Finally, using the rotation matrix you created in the previous step, apply the affine transformation on the image. OpenCV’s warpAffine() function does the job. getRotationMatrix2D() getRotationMatrix2D(center, angle, scale)\nThe following arguments are passed to the getRotationMatrix2D() function:\ncentre: the rotational axis of the input image. angle: the rotational angle in degrees. scale: an isotropic scale factor that scales the image up or down based on the value entered. If the angle is positive, the image will be turned counter-clockwise. If you want to rotate the image by the same amount clockwise, the angle must be negative.\nwarpAffine() The warpAffine() method transforms the image using an affine transformation. All parallel lines in the source image will remain parallel in the output image after applying the affine transformation.\nwarpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])\nThe function’s arguments are as follows:\nsrc: Source image. M stands for the transformation matrix. dsize: the output image’s size borderMode: the pixel extrapolation technique dst: the output image flags: a collection of interpolation methods such as INTER_LINEAR or INTER_NEAREST borderValue: the value that will be used in the case of a constant border, with a default value of 0. Okay, now that you understand the code and functions, let’s look at a specific example and try it out.\nCode #include \"opencv3/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e int angle = 40; const int ANGLE_MAX = 360; cv::Mat image; const std::string SOURCE_WINDOW = \"Orignal Image\"; void rotate_image(int,void* ) { cv::Point2f center((image.cols - 1)/2.0, (image.rows - 1)/2.0); const std::string FINAL_WINDOW = \"Rotating Image\"; cv::namedWindow( FINAL_WINDOW, cv::WINDOW_AUTOSIZE ); cv::Mat matRotation = cv::getRotationMatrix2D( center,angle , 1.0 ); cv::Mat rotated_image; cv::warpAffine(image, rotated_image, matRotation, image.size()); cv::imshow(FINAL_WINDOW.c_str(), rotated_image); } int main(int argc,char** argv) { if( argc != 2) { std::cerr \u003c\u003c \"Usage:\\n\"\u003c\u003c argv[0] \u003c\u003c \" \u003cimagefile\u003e\\n\"; return -1; } image = cv::imread( argv[1]); cv::namedWindow( SOURCE_WINDOW, cv::WINDOW_AUTOSIZE ); cv::imshow( SOURCE_WINDOW, image ); cv::createTrackbar(\"Angle\", SOURCE_WINDOW, \u0026angle, ANGLE_MAX,rotate_image); rotate_image(0,0); cv::waitKey(0); return 0; } Output References Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition ","description":"Learn how to rotate and translate images using OpenCV. Learn about the syntax and methods for rotating and translating images.","tags":["programming","cpp","opencv"],"title":"OpenCV - Image Rotation using C++","uri":"/collections/programming/cpp/opencv/image-rotation/"},{"content":"Introduction Have you ever seen lines drawn around mountain ranges and elevations on topographical maps? Topographical contours are the names given to these lines. They provide an elevation profile of a terrain. These lines are either created by hand or generated by a computer. This tutorial will look at how to draw contour lines on images and fill them using OpenCV.\nWhat are Contours? Contours are essentially curves that connect all continuous points (along the border) with the same hue or intensity. Contours are an effective tool for shape analysis as well as to object detection and recognition.\nUse binary pictures for greater precision. Apply threshold or canny edge detection before looking for contours.\nfindContours() in OpenCV is like finding a white object against a black background. So, keep in mind that the object to be found should be white, and the background should be dark.\nWe can recognise the edges of objects and pinpoint them in a picture using contour detection. Many fascinating applications, such as image-foreground extraction, simple-image segmentation, detection, and identification, rely on it as the first step.\nWhat is Convex Hull? A region/shape is considered convex if the line connecting any two points (chosen from the region) is contained inside that region. To put it another way, a convex hull is a convex polygon produced by joining the outermost points of a set of points on a two-dimensional plane.\nCalculating the convex hull of an object and then calculating its convexity flaws is a handy technique to analyse its form or contour.\nCode #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e cv::Mat img_gray,input_img; int thresh = 100; const int MAX_THRESH = 255; void draw_and_fill_contours(std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e\u0026 contours, std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e\u0026 hull, std::vector\u003ccv::Vec4i\u003e\u0026 hierarchy) { cv::Mat contours_result = input_img.clone(); cv::Mat fill_contours_result = cv::Mat::zeros(img_gray.size(), CV_8UC3); for (unsigned int i = 0, n = contours.size(); i \u003c n; ++i) { cv::Scalar color = cv::Scalar(0,0,255); cv::drawContours(contours_result, contours, i,color, 4, 8, hierarchy,0, cv::Point()); } cv::fillPoly(fill_contours_result,hull,cv::Scalar(255,255,255)); cv::imshow(\"Contours Result\",contours_result); cv::imshow(\"Fill Contours Result\",fill_contours_result); } void find_contours(int,void*) { cv::Mat canny_output; cv::Canny( img_gray, canny_output, thresh, thresh*2 ); std::vector\u003cstd::vector\u003ccv::Point\u003e \u003e contours; std::vector\u003ccv::Vec4i\u003e hierarchy; cv::findContours( canny_output, contours, hierarchy, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE ); std::vector\u003cstd::vector\u003ccv::Point\u003e\u003e hull(contours.size()); for(unsigned int i = 0,n = contours.size(); i \u003c n; ++i) { cv::convexHull(cv::Mat(contours[i]),hull[i],false); } draw_and_fill_contours(contours,hull,hierarchy); } int main (int argc, char** argv) { input_img = cv::imread(argv[1]); if (input_img.empty()) { fprintf(stdout,\"Could not open image\\n\\n\"); fprintf(stdout,\"Usage: %s \u003cinput image\u003e\\n\",argv[0]); return -1; } cv::cvtColor(input_img,img_gray,cv::COLOR_BGR2GRAY); cv::blur(img_gray,img_gray,cv::Size(3,3)); const std::string source_window(\"Source\"); cv::namedWindow(source_window.c_str()) ; cv::imshow(source_window.c_str(),input_img); cv::createTrackbar(\"Thresh: \",source_window,\u0026thresh,MAX_THRESH,find_contours); find_contours(0,0); cv::waitKey(); return 0; } /** Compiling:- g++ \u003cprogram_name\u003e.cpp $(pkg-config opencv4 --cflags --libs) ​ Run: ./a.out \u003cimg_file\u003e **/ Code Explanation The code is straightforward:-\nFirst we use imread() to read the input file and cvtColor() to convert the input image into gray scale. Before finding contours or boundaries of the image, we first find edges of the image. For that, we have used cv::Canny() edge detection algorithm. After that, we apply findContours() to find the contours of the image. The convexHull() algorithm is used to calculate the maximum area under the contours. Then, we use drawContours() function to draw contours of the image and fillPoly() function to fill the contours Output References Building Computer Vision Projects with OpenCV 4 and C++ Learning OpenCV 3: Computer Vision In C++ With The OpenCV Library OpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://docs.opencv.org/3.4/df/d0d/tutorial_find_contours.html https://stackoverflow.com/questions/33645213/how-to-fill-contour-line-using-opencv https://www.programmersought.com/article/10265183451/ ","description":"Fill contour using c++","tags":["programming","cpp","opencv"],"title":"OpenCV - Fill Contour using C++","uri":"/collections/programming/cpp/opencv/fill-contours-cpp/"},{"content":"Introduction The Harris Corner Detector is a corner detection operator frequently used in computer vision algorithms to extract corners and infer image characteristics. Chris Harris and Mike Stephens invented it in 1988 after improving Moravec’s corner detector.\nIn comparison to the preceding one, Harris’ corner detector directly considers the differential of the corner score concerning direction, instead of requiring shifting patches for every 45-degree angle, and has proven to be more accurate in discriminating between edges and corners. It has since been developed and used in a variety of techniques to preprocess photos for subsequent applications.\nWhat is Corner in an Image ? A corner is a point with two opposed edge orientations in its immediate surroundings. In other terms, a corner is the intersection of two edges, each of which represents a sharp shift in image brightness. Corners are the most essential features of the image, and they are usually referred to as interest points since they are unaffected by translation, rotation, or lighting.\nSo, let’s look at why corners are thought to be superior features or suitable for patch mapping. If we take the flat region in the above graphic, we can see no gradient change in any direction. Similarly, no gradient change is detected along the edge direction in the edge region. As a result, both the flat and edge regions are unsuitable for patch matching because they are not significantly different (there are many similar patches along the edge in the edge region). When we are in the corner region, we notice a significant gradient difference in all directions. Because of this, corners are said to be useful for patch matching (moving the window in any direction results in a considerable change in appearance) and generally more stable over the shift in viewpoint.\nHow Harris Corner Detection Algorithm Works ? Consider a small window surrounding each pixel p in an image. We aim to find all of these one-of-a-kind pixel windows. The amount of change in pixel values can be assessed by shifting each window by a tiny amount in a given direction and measuring the difference.\nIn more technical terms, we calculate the sum squared difference (SSD) between the pixel values before and after the shift and find pixel windows where the SSD is substantial for changes in all eight directions. Let us define the change function E(u,v) as the sum of all sum squared differences (SSD), where u,v are the x,y coordinates of each pixel in our 3 x 3 window and I is the pixel’s intensity value. The image’s features are all pixels with high E(u,v) values, as defined by some threshold.\nwhere:\nw(x,y) is the window at position (x,y) I(x,y) is the intensity at (x,y) I(x+u,y+v) is the intensity at the moved window (x+u,y+v) For corner detection, we must maximise the function E(u,v). That is, we must maximise the second term. Using Taylor Expansion on the previous equation and some mathematical processes, we arrive at the following final equation:\nNow, we rename the summed-matrix, and put it to be M:\nSo the equation now becomes:\nRemember that we want the SSD to be prominent in shifts in all eight directions, or the SSD to be tiny in none of them. We can derive the directions for both the highest and most minor improvements in SSD by solving M’s eigenvectors. The real value amount of these increases is given by the relevant eigenvalues.\nFor each window, a R score is computed:\nM’s eigenvalues are numbered λ1 and λ2. As a result, the values of these eigenvalues determine whether a region is a corner, an edge, or a flat surface.\nThe region is flat when |R| is small, as it is when λ1 and λ2 are small.\nThe region is an edge when R0, which happens when λ1»λ2 or vice versa.\nWhen R is huge, like when λ1 and λ2 are large, and λ1~λ2 is large, the region is a corner.\nHarris Corner Detection Code C++ #include \"opencv2/highgui.hpp\" #include \"opencv2/imgproc.hpp\" #include \u003ciostream\u003e cv::Mat src, src_gray; int thresh = 200; int MAX_THRESH = 255; std::string source_window(\"Source image\"); std::string corners_window(\"Harris Corner detection\"); void detectCornerHarris(int,void*) { int blockSize = 2; int apertureSize = 3; double k = 0.04; cv::Mat dst = cv::Mat::zeros( src.size(), CV_32FC1 ); cv::cornerHarris( src_gray, dst, blockSize, apertureSize, k ); cv::Mat dst_norm, dst_norm_scaled; cv::normalize( dst, dst_norm, 0, 255, cv::NORM_MINMAX, CV_32FC1, cv::Mat() ); cv::convertScaleAbs( dst_norm, dst_norm_scaled ); for( int i = 0; i \u003c dst_norm.rows ; i++ ) { for( int j = 0; j \u003c dst_norm.cols; j++ ) { if( (int) dst_norm.at\u003cfloat\u003e(i,j) \u003e thresh ) { cv::circle( dst_norm_scaled, cv::Point(j,i), 5, cv::Scalar(255), 2, 8, 0 ); } } } cv::namedWindow( corners_window.c_str() ); cv::imshow( corners_window.c_str(), dst_norm_scaled ); } int main( int argc, char** argv ) { if(argc != 2) { std::cout \u003c\u003c \"Error!!\\n\\nPlease specify input file..\\n\"; return -1; } src = cv::imread( argv[1] ); if ( src.empty() ) { std::cout \u003c\u003c \"Could not open or find the image!\\n\\n\"; std::cout \u003c\u003c \"Usage: \" \u003c\u003c argv[0] \u003c\u003c \" \u003cInput image\u003e\\n\" ; return -1; } cv::cvtColor( src, src_gray, cv::COLOR_BGR2GRAY ); cv::namedWindow( source_window.c_str()); cv::createTrackbar( \"Threshold: \", source_window, \u0026thresh, MAX_THRESH, detectCornerHarris ); cv::imshow( source_window.c_str(), src ); detectCornerHarris(0,0); cv::waitKey(); return 0; } /** Compiling:- g++ \u003cprogram_name\u003e.cpp $(pkg-config opencv4 --cflags --libs) Run: ./a.out \u003cimg_file\u003e **/ Explanation First we read input file and convert it into grayscale using cvtColor(). Then we apply Harris Corner using cornerHarris() function. We then normalize the image since the pixel value of dst is very small. Finally, we check if the threshold value of each pixel in the image to our specified threshold value; if it is larger, we put a circle around it. Output References Building Computer Vision Projects with OpenCV 4 and C++\nLearning OpenCV 3: Computer Vision In C++ With The OpenCV Library\nOpenCV 4 Computer Vision Application Programming Cookbook: Build complex computer vision applications with OpenCV and C++, 4th Edition Object-Oriented Programming with C++ | 8th Edition https://docs.opencv.org/3.4/d4/d7d/tutorial_harris_detector.html\nhttps://medium.com/data-breach/introduction-to-harris-corner-detector-32a88850b3f6\nhttps://www.geeksforgeeks.org/python-corner-detection-with-harris-corner-detection-method-using-opencv/\n","description":"The Harris Corner Detector is a corner detection operator that is frequently used in computer vision algorithms to extract corners and infer image characteristics.","tags":["programming","cpp"],"title":"Opencv - Harris Corner Detector using C++","uri":"/collections/programming/cpp/opencv-corner-harris-cpp/"},{"content":"Python Data Structure Cheat Sheet: Your Complete Guide Python is a popular programming language that is widely used in various fields such as data science, machine learning, web development, and more. Understanding data structures is essential for any programmer, as it helps in organizing and storing data efficiently. In this article, we will provide you with a Python data structure cheat sheet that you can use as a quick reference guide when working with data in Python.\nWhat is Data Structure? The most important thing is organizing, managing, and storing data to allow for easy access and efficient alterations. Moreover, Data Structures allow us to organize data so that we can store collections of data, relate them, and conduct actions on them as needed.\nSo, using Data Structures, we can structure our data in a way that allows it to be retrieved rapidly. However, one Data Structure is insufficient to accommodate all use case scenarios. As a result, we have a variety of data structures that can be utilized for various purposes.\nNeed Of Data Structure Consider the following scenario: you wish to look for a specific document in a file explorer that contains hundreds of documents. One method is to go through each document one by one in a sequential fashion, however this is a time-consuming process.\nAnother option is to go directly to the location where it is kept or where the linked papers are located.\nYes, your operating system (OS) performs this through the use of indexing and hashtables, which are a type of data structure. Even if there are many files, this decreases the amount of time required to search. This is why Data Structures are essential.\nNow that we know what are data structure let’s dive into built-in data structure in python\nTypes of Data Structure in Python Python provides implicit support for Data Structures, which allow you to store and access data. The data structures involved in this are as follows:\nList, Tuple, Dictionary, and Set are all types of data structures.\nNot only that, but Python also allows users to construct their Data Structures, giving them complete control over their functioning. The most well-known Data Structures are\nStack, Queue, Tree, Linked List, Graph, Hash Map, and so on.\nAll of the data structures listed above are also accessible in other programming languages such as C, Rust, Javascript,C++, etc. We have limited our discussion in this post to Python’s Built-In Data Structures.\nBuilt-in Data Structure in Python As the name implies, the Data Structures in this category are built into Python, making programming more accessible and allowing programmers and data scientists to achieve faster solutions. Python has the following built-in data structures:\nList Dictionaries Sets Tuples List Lists are the most basic data structures, and they are used to store data of various types in a sequential order. The Interpreter allocated addresses to each element of the list called an Index during list creation. The index value in the list begins at 0 and continues until the last entry; this index is known as the positive index. We also have negative indexing in Lists, which starts at -1 and allows us to access entries from the last to the first. Following are the code snippet to grasp lists and their features better.\nPros:\nVersatility: Python lists can store a variety of data types such as strings, integers, and even other lists.\nMutable: Lists in Python are mutable, meaning you can modify them after creation. This makes it easy to add or remove elements from a list.\nEasy to access: Elements in a list are indexed, making it easy to access individual elements within the list.\nMany built-in methods: Python provides many built-in methods for working with lists, such as sorting, reversing, and slicing.\nMemory efficient: Lists in Python are implemented using dynamic arrays, which means they can be resized as needed. This makes them more memory efficient compared to other data structures like arrays.\nCons:\nSlow for large data: When dealing with very large amounts of data, Python lists can be slower than other data structures like NumPy arrays or sets.\nNot suitable for certain operations: Certain operations like searching for an element in a list can be inefficient, especially for large lists. In these cases, other data structures like dictionaries or sets may be more suitable.\nTime complexity for some operations: Some operations on a Python list, like inserting or deleting elements from the middle of the list, can have a time complexity of O(n). This means that for very large lists, these operations can be slow.\nNot ideal for ordered data: While Python lists are indexed and maintain order, they are not optimized for searching and sorting, and can be slower than other data structures like arrays or linked lists.\nCreating List foo = [] # Empty list foo = [\"Another Techs\",52,53,786,110] # creating list print(foo) Accessing element in list foo = [\"Another Techs\",52,53,786,110] # Access all elements print(foo) # Access element at certain location say 2nd index print(foo[2]) # Access element from 1-3 print(foo[1:3]) # Access element in reverse order print(foo[::-1]) Adding Element To add elements to the list, we can utilize the append() ,insert and extend() functions.\nThe append() function joins all of the components provided to it as a single element.\ninsert() method is use to add element at particular index.\nThe extend() function adds the elements to the list one at a time.\nfoo = [\"Another Techs\",52,53,786,110] # Adding elment in list foo.append(72.3) # Adding another list foo.append([\"python\",\"Cpp\",11]) print(foo) # Adding another list element one by one in foo.extend([\"Data Science\",\"AI\",\"python3\"]) # Add element at particular index foo.insert(2,\"Javascript\") Deleting Element del keyword is used to delete element at specific index:\nfoo = [\"Another Techs\",52,53,786,110] del foo[4] print(foo) Remove specific element from the list To remove specific element by it’s value from the list we use remove() method:\nfoo = [\"Another Techs\",52,53,786,110,'cpp'] foo.remove('cpp') print(foo) Remove all the element from the list clear() method is use to remove all the elements from the list:\nfoo = [\"Another Techs\",52,53,786,110] foo.clear() print(foo) #empty list Sort List The sort() function is used to sort the list. However, while sorting a list, you must ensure that the data types of all the components are the same because we cannot compare two distinct data types, however, you can use a combination of float and int data types:\nfoo = [111,1,43,23,52,53,786,110] foo.sort() print(foo) Tuples Tuples are similar to lists, except that once data is inserted into a tuple, it cannot be modified in any way, i.e. it is immutable. The lone exception is when the data inside the tuple is mutable (e.g., a list), in which case we can update the tuple data. Let us now use the following examples to grasp tuples and their functionalities better.\nPros:\nImmutable: Tuples are immutable, meaning once they are created, you cannot modify them. This makes tuples more reliable and safe than lists, as they cannot be accidentally modified.\nFaster than lists: Since tuples are immutable, they are generally faster than lists for certain operations like indexing and iteration.\nSuitable for certain data types: Tuples are suitable for storing data that should not be modified, such as constant values, coordinates, or dates.\nMemory efficient: Tuples are more memory efficient than lists, as they don’t need to be resized or modified after creation.\nCan be used as dictionary keys: Since tuples are immutable, they can be used as keys in dictionaries, while lists cannot.\nCons:\nLimited functionality: Tuples have limited functionality compared to lists, as they cannot be modified after creation. This makes certain operations like adding or removing elements impossible.\nLess versatile: Tuples can only store elements of the same data type, unlike lists which can store a variety of data types.\nNot ideal for large datasets: Tuples are not ideal for storing large datasets, as they cannot be modified or resized. This means that if you need to add or remove elements from a tuple, you will need to create a new tuple, which can be inefficient for large datasets.\nLess readable: Tuples can be less readable than lists, especially when the tuple contains many elements.\nCreating Tuples foo = () # Empty tuple foo = (\"Another Techs\",52,53,786,110) # creating tuple print(type(foo)) print(foo) Accessing element in tuple foo = (\"Another Techs\",52,53,786,110) # Access all elements print(foo) # Access element at certain location say 2nd index print(foo[2]) # Access element from 1-3 print(foo[1:3]) # Access element in reverse order print(foo[::-1]) Return the index of element in Tuple To discover the index of a particular element in a tuple, we can use the index() function, which accepts the element’s value as an input and returns the index. If we pass an element that does not exist in the tuple to the index() function, we get a ValueError.\nfoo = (\"Another Techs\",52,53,786,110) print(foo.index(53)) Adding new elements to the Tuple We can use the ‘+’ operator to append the values in an existing tuple by passing it another tuple to be appended to:\nfoo = (\"Another Techs\",52,53,786,110) foo = foo + (\"Python\") print(foo) # Adding more than one element foo = foo + (\"Cpp\",\"AI\") print(foo) Dictionaries Data is stored in the form of key-value pairs in dictionaries.\nImagine a phone directory with hundreds of thousands of names of different people and their accompanying phone numbers to comprehend dictionary data structure. Here are the constant values (such as Name) and Phone Numbers, which we referred to as the keys.\nFurthermore, the numerous names and phone numbers are the keys’ values. If we want to get the values of the keys, we will need all of the names and phone numbers.\nSo a key-value pair is precisely that. Moreover, Dictionaries are used to hold this structure in Python.\nPros:\nFast lookup: Dictionaries provide fast lookup times for accessing data, as data is accessed using keys rather than index positions.\nEasy to update: Dictionaries are mutable, meaning you can add, modify, or delete key-value pairs as needed.\nVersatile data types: Dictionaries can store a variety of data types as keys and values, such as strings, integers, lists, or even other dictionaries.\nNo duplicate keys: Dictionaries do not allow duplicate keys, ensuring that each key is unique and allowing for easy retrieval of values.\nBuilt-in methods: Python provides many built-in methods for working with dictionaries, such as sorting, merging, or iterating over key-value pairs.\nCons:\nNot ordered: Dictionaries are not ordered, meaning that key-value pairs are not stored in any particular order. If you need to maintain order, you will need to use other data structures like lists.\nTime complexity for some operations: Some operations on a Python dictionary, such as searching for a key, can have a time complexity of O(n). This means that for very large dictionaries, these operations can be slow.\nMemory usage: Dictionaries can use a significant amount of memory, especially if they contain a large number of key-value pairs.\nNot suitable for certain operations: Dictionaries are not suitable for certain types of operations, such as iterating over values in a specific order, or performing mathematical operations on values.\nCreating a Dictionary foo = {} # Empty dict print(foo) foo = {11:'Another Techs',12:'Python',13:'Data Science'} print(foo) Returning the value of particular key in python dictinory We can only use the keys to access the components of a dictionary. We can use the get() function or just give the key values to retrieve the information.\nfoo = {11:'Another Techs',12:'Python',13:'Data Science'} print(foo.get(12)) # Output: Python Changing or Adding Key Value Pairs The keys can be used to change the values of the dictionary. As a result, we must first access the key and then alter the value. We just add another key-value pair to add values.\nfoo = {11:'Another Techs',12:'Python',13:'Data Science'} # Changing Value foo[12] = 'Cpp' print(foo) # Adding Key Value Pair foo[14] = \"Python\" print(foo) Returning sets of the elements in a dictionary foo = {11:'Another Techs',12:'Python',13:'Data Science'} print(foo.items()) Getting All the keys present in dictionary foo = {11:'Another Techs',12:'Python',13:'Data Science'} print(foo.keys()) Getting All the values of keys present in dictionary foo = {11:'Another Techs',12:'Python',13:'Data Science'} print(foo.values()) Deleting Key-Value Pair Delete Values: We can use the pop() function to delete values, which returns the deleted value.\nDeleting a Key-Value Pair: To recover the key-value pair, use the popitem() method, which provides a key and value tuple.\nClearing the Entire Dictionary: The clear() function can be used to clean the entire dictionary.\nfoo = {11:'Another Techs',12:'Python',13:'Data Science'} # Pop the specified element bar = foo.pop('12') print('Value:', bar) print('Dictionary:', foo) # Pop the complete key-value pair bar = foo.popitem() print('Key, value pair:', bar) print('Dictionary', foo) # Make the dictionary Empty foo.clear() print('Empty Dictionary', foo) Sets A set is a data type consisting of a collection of unordered elements and is a mutable (changeable) collection of unique components, i.e. there are no duplicate copies of elements. Unlike arrays, which are type-specific, elements in sets can be of any data type. Because the values of a set are unindexed, indexing operations cannot be performed on them.\nPros:\nUnique elements: Sets only store unique elements, meaning there are no duplicates. This makes them ideal for removing duplicates from lists or other collections.\nFast operations: Sets provide fast operations for checking membership, intersection, union, and difference between sets. This is because sets are implemented using hash tables.\nMath operations: Sets can be used to perform math operations like union, intersection, and difference between sets. This can be useful in many applications, such as data analysis or graph theory.\nFlexible data types: Sets can store a variety of data types, including numbers, strings, and tuples.\nMutable: Sets are mutable, meaning you can add or remove elements as needed.\nCons:\nUnordered: Like dictionaries, sets are unordered. This means that elements are not stored in any particular order, and you cannot access elements using an index.\nCannot store mutable data types: Sets cannot store mutable data types like lists or other sets. This is because the hash value of a mutable object can change, making it difficult to implement the hash table.\nMemory usage: Like dictionaries, sets can use a significant amount of memory, especially if they contain a large number of elements.\nType casting: Type casting can be slow, especially when converting from lists to sets or vice versa.\nCreating Sets foo = {'Another Techs','Python','Data Science'} print(type(foo)) print(foo) Accessing Element of sets Note:We cannot access the set elements using the index numbers because, as previously stated, set elements are not indexed. As a result, if we wish to access the items of a set, we can use a for loop to do so.\nfoo = {'Another Techs','Python','Data Science'} for elem in foo: print(elem) Adding Element Using one of the two functions, we may add the new elements to a set.\nThe add() function is used to add a single element.\nTo adding multiple element use the update() function.\n# Adding Elements foo = {'Another Techs','Python','Data Science'} foo.add('Javascript') print(foo) # Adding more than one element foo.update('Cpp',\"AI\",\"pandas\",\"numpy\") print(foo) Remove element from a set: foo = {'Another Techs','Python','Data Science'} foo.re­mov­e(\"Python­\") # If \"Python\" is not present, raises a KeyErorr # using Discard method foo.di­sca­rd(­\"­Python\") # Removes the element, if present #Remove every element from the set foo.cl­ear() Operation on sets foo = {'Another Techs','Python','Data Science'} bar = {'Another Techs','Python','cpp','javascript','Data Science'} ## Union of sets foo | mySet2 # Inters­ection of two sets foo \u0026 bar # Difference of two sets foo - bar # Symmetric difference of two sets foo ^ bar References https://docs.python.org/3/tutorial/datastructures.html https://www.tutorialspoint.com/python/python_data_structure.html https://whataftercollege.com/python-programming/built-in-data-structures-in-python/ https://www.slideshare.net/EdurekaIN/what-are-data-structures-in-python-list-dictionary-tuple-explained-edureka ","description":"Simplify your understanding of Python data structures with our comprehensive cheat sheet for 2021. Perfect for beginners and experienced programmers alike, this guide covers everything you need to know about Python data structures, including lists, tuples, sets, and more","tags":["programming","python"],"title":"Python Data Structure Cheat Sheet- Your Complete Guide","uri":"/collections/programming/python/python-data-structure-cheat-sheet-2021/"},{"content":"C++17 - New Parallel Algorithm of STL C++17 introduces parallel algorithms. However, there aren’t many implementations where the additional functionalities can be used.\nThe concept is straightforward. More than 100 algorithms are included in the Standard Template (STL) for searching, counting, and manipulating ranges and their constituents. 69 of them are overloaded in C++17, and a few new ones are added. A so-called execution policy can be used to invoke the overloaded and new algorithms. You can specify whether the method should run sequentially, parallelly, or parallel and vectorized by using the execution policy.\nC++ Parallel Algorithm of STL There are almost 100 algorithms in the Standard Template Library for finding, counting, and manipulating ranges and their elements. C++17 adds new overloads to 69 of them and adds new ones to others. A so-called execution policy can be used to launch overloaded and new algorithms. You can indicate whether the method should run sequentially, in parallel, or in parallel with vectorization using an execution policy. You must add the header \u003cexecution\u003e if you want to use the execution policy.\nExecution Policy of Parallel Algorithm Three execution policies are defined in the C++17 standard:\nstd::execution::sequenced_policy : sequential execution std::execution::parallel_policy : Parallel execution std::execution::parallel_unsequenced_policy : Parallel and unsequenced execution The important thing to remember is that the execution policies are permissions rather than obligations. Each library implementation may decide what and how much can be parallelized.\nTo use parallel algorithms, you need at least forward iterators.\nHere is code snippet of sort algorithm which applies all execution policies.\n#include \u003cvector\u003e //for vector #include \u003calgorithm\u003e // for sort #include \u003cexecution\u003e // for parallel execution int main() { std::vector\u003cint\u003e vec = {21,34,53,98,22,7,244,52,60,72,89,44,57}; //standart sequential sort std::sort(vec.begin(),vec.end()); // sequential execution std::sort(std::execution::seq,vec.begin(),vec.end()); // permittin parallel execution std::sort(std::execution::par,vec.begin(),vec.end()); // permitting parallel and vectorized execution std::sort(std::execution::par_unseq,vec.begin(),vec.end()); } The example demonstrates that the classic variant of std::sort can still be used . Furthermore, in C++17, you can specify whether you want to utilise the sequential , parallel , or parallel and vectorized versions.\nException If an exception occurs while using an algorithm with an execution policy, the function std::terminate is invoked. The installed std::terminate::handler is called by std::terminate. As a result, the std::abort function is invoked by default, resulting in abnormal programme termination. The handling of exceptions distinguishes between the invocation of an algorithm without an execution policy and the invocation of an algorithm with a sequential std::execution::seq execution policy. The exception is propagated when the algorithm is invoked without an execution policy, and so the exception can be handled.\nWith C++17, 69 STL algorithms received new overloads, and new algorithms were introduced.\nAlgorithm The table below shows whether parallel and unsequenced execution are supported by each of the C++17 algorithms that accept execution policies. The use of an unsupported algorithm and execution policy will result in sequential execution.\nReferences https://en.cppreference.com/ https://software.intel.com/content/w/develop/articles/get-started-with-parallel-stl.html https://www.modernescpp.com/index.php/c-17-new-algorithm-of-the-standard-template-library ","description":"A performance optimization lesson will teach you about C++17 parallel algorithms and how to improve C++ code performance using this library and the Intel Parallel STL","tags":["programming","cpp"],"title":"Parallel Programming with C++","uri":"/collections/programming/cpp/parallel-programming-stl/"},{"content":"C++17 - New Parallel Algorithm of STL C++17 introduces parallel algorithms. However, there aren’t many implementations where the additional functionalities can be used.\nThe concept is straightforward. More than 100 algorithms are included in the Standard Template (STL) for searching, counting, and manipulating ranges and their constituents. 69 of them are overloaded in C++17, and a few new ones are added. A so-called execution policy can be used to invoke the overloaded and new algorithms. You can specify whether the method should run sequentially, parallelly, or parallel and vectorized by using the execution policy.\nC++ Parallel Algorithm of STL There are almost 100 algorithms in the Standard Template Library for finding, counting, and manipulating ranges and their elements. C++17 adds new overloads to 69 of them and adds new ones to others. A so-called execution policy can be used to launch overloaded and new algorithms. You can indicate whether the method should run sequentially, in parallel, or in parallel with vectorization using an execution policy. You must add the header \u003cexecution\u003e if you want to use the execution policy.\nExecution Policy of Parallel Algorithm Three execution policies are defined in the C++17 standard:\nstd::execution::sequenced_policy : sequential execution std::execution::parallel_policy : Parallel execution std::execution::parallel_unsequenced_policy : Parallel and unsequenced execution The important thing to remember is that the execution policies are permissions rather than obligations. Each library implementation may decide what and how much can be parallelized.\nTo use parallel algorithms, you need at least forward iterators.\nHere is code snippet of sort algorithm which applies all execution policies.\n#include \u003cvector\u003e //for vector #include \u003calgorithm\u003e // for sort #include \u003cexecution\u003e // for parallel execution int main() { std::vector\u003cint\u003e vec = {21,34,53,98,22,7,244,52,60,72,89,44,57}; //standart sequential sort std::sort(vec.begin(),vec.end()); // sequential execution std::sort(std::execution::seq,vec.begin(),vec.end()); // permittin parallel execution std::sort(std::execution::par,vec.begin(),vec.end()); // permitting parallel and vectorized execution std::sort(std::execution::par_unseq,vec.begin(),vec.end()); } The example demonstrates that the classic variant of std::sort can still be used . Furthermore, in C++17, you can specify whether you want to utilise the sequential , parallel , or parallel and vectorized versions.\nException If an exception occurs while using an algorithm with an execution policy, the function std::terminate is invoked. The installed std::terminate::handler is called by std::terminate. As a result, the std::abort function is invoked by default, resulting in abnormal programme termination. The handling of exceptions distinguishes between the invocation of an algorithm without an execution policy and the invocation of an algorithm with a sequential std::execution::seq execution policy. The exception is propagated when the algorithm is invoked without an execution policy, and so the exception can be handled.\nWith C++17, 69 STL algorithms received new overloads, and new algorithms were introduced.\nAlgorithm The table below shows whether parallel and unsequenced execution are supported by each of the C++17 algorithms that accept execution policies. The use of an unsupported algorithm and execution policy will result in sequential execution.\nReferences https://en.cppreference.com/ https://software.intel.com/content/w/develop/articles/get-started-with-parallel-stl.html https://www.modernescpp.com/index.php/c-17-new-algorithm-of-the-standard-template-library ","description":"A performance optimization lesson will teach you about C++17 parallel algorithms and how to improve C++ code performance using this library and the Intel Parallel STL","tags":["programming","cpp"],"title":"Parallel Programming with C++","uri":"/post/parallel-programming-stl/"},{"content":"Three Black Crows Candlestick Pattern The term “three black crows” refers to a bearish candlestick pattern that may indicate an uptrend’s reversal. Candlestick charts depict a security’s opening, high, low, and closing prices for the day. The candlestick for stocks moving higher is white or green. They become dark or red as they descend.\nThree consecutive long-bodied candlesticks that opened within the real body of the preceding candle and closed lower than the previous candle make up the black crow pattern. Traders frequently combine this signal with other technical indicators or chart patterns to confirm a reversal.\nThree black crows constitute a visual pattern, thus there are no computations to be concerned with while determining this indicator. During three consecutive trading sessions, bears overtake bulls, forming the three black crows pattern. Three bearish long-bodied candlesticks with short or no shadows or wicks appear on the pricing charts.\nThe bulls will start the session marginally higher than the previous close, which is characteristic of three black crows, but the price will be pushed lower throughout the day. Under bearish pressure, the price will eventually close near the session low.\nThe shadow cast by this trading action will be very short or nonexistent. Traders frequently view this three-session downward pressure as the commencement of a bearish downturn.\nFormation of Three Black Crow Candlestick Pattern Candle 1 This pattern’s first candlestick should be a long-bodied bearish candlestick that forms as a continuation of the current upswing.\nThe closing price of a bearish candle should be lower than the opening price, indicating that the bears are attempting to drive prices lower.\nCandle 2 A bearish candlestick should be used as the second candlestick. It can have either a long or short body.\nThe opening price of this candlestick should be between the midpoint and closing price of the first candlestick, i.e. between the midpoint and closing price of the first candle.\nCandle 3 A bearish candlestick should be used as the third candlestick. A long or short-bodied candle can be used.\nThis candlestick’s opening price should be inside the genuine body of the second candlestick, i.e. the midpoint or closing price of the second candle.\nThe height of the second candlestick should not be broken by the third candle.\nThese three candlesticks have the potential to be Bearish Marubozu.\nA long-bodied bearish candlestick pattern in which the closing price is the low price and the opening price is the high price for that day is known as a Bearish Marubozu.\nIn the Bearish Marubozu, there are no shadows.\nWhat Does Three Black Crows Tell Us ? During an uptrend, the three black crows pattern appears, frequently foreshadowing the end of a bull market. It forms when the bears outnumber the bulls for three trading sessions in a row. In other words, it’s a clear sign of a bearish trend reversal.\nAfter a period of great market performance, it is natural for the bulls to relax their hold and let the bears to have some fun, causing prices to fall. This pattern alerts traders to the onset of a negative trend, with prices projected to fall in the next sessions.\nIt is not necessary for each candle to open within the body of the previous one. The first candle appears during an uptrend, whereas the next two appear during a decline.\nIt can also appear near a Doji, which is an uncertain candlestick formation that depicts market hesitancy before to a trend reversal. It’s also worth noting that the pattern can appear in both negative and positive market conditions.\nThe three black crows will frequently occur in bearish rallies or short upswings in a bearish trend.\nNotes Three bearish long-bodied candlesticks form the Three Crows pattern, which is a bearish reversal pattern.\nBecause the Three Black Crows is a bearish reversal pattern, it should only be studied after an ascent.\nIt’s worth noting that these three candlesticks have the potential to be Bearish. Marubozu\nTo validate the creation of this candlestick pattern, traders might use volume and technical indicators.\nTrading in Three Black Crows As previously said, the three black crows pattern signals the start of a negative trend in the market. You should pay attention to the length of the candlesticks once you’ve identified what appears to be a three black crows pattern.\nTo confirm that the bears have complete dominance, the second and third candles should be roughly similar in size. If the third candle is noticeably smaller than the rest, the pattern is weak and useless.\nOnce an uptrend has run its course, the pattern begins to emerge. This signifies the start of a significant downturn, and you should take a short position.\nOn a mini pullback, you should enter after the pattern has developed. The Three Black Crows pattern appears in the chart above, followed by two candles of consolidation before going lower.\nStops should be placed above the pattern’s start, with the goal of catching a break beneath the chart pattern. A pullback to a moving average or another level of support could be the case.\nBottom Line Crows are thought to bring ill luck. The three black crows pattern acquired its name because its appearance indicates the end of an uptrend.\nIt is part of the Japanese candlestick chart family, which is increasingly commonly utilised by day traders to forecast trend changes and plan market entry and exit positions.\nAfter a run, this pattern signals that the uptrend is weakening and the bears are gaining control. It can assist you in catching reversals and entering trades before the true momentum kicks in.\nIt signifies a negative reversal and likely profit-taking in a particular stock or the market in the following sessions when it occurs at the top of any trend.\nThe three black crows design, like other candle formations, has its limitations.\nTo confirm reversals, traders should utilise it in conjunction with other technical indicators and chart patterns.\nLimitation of Three Black Crows Because the three crows pattern causes prices to decline, traders should be aware of oversold conditions, which could lead to consolidation before a further drop in prices.\nTraders should look at additional chart patterns or technical indicators in addition to the three black crows pattern to confirm the reversal.\nRefrences http://forexop.com/candlesticks/three-black-crows/ https://trading.funituresited.com/three-black-crows-pattern-explained-for-beginners/ https://www.feedroll.com/candlestick-patterns/1265-three-black-crows-pattern/ https://forextraininggroup.com/ ","description":"Three black crows is a bearish candlestick pattern that predicts a current uptrend's reversal.","tags":["crypto"],"title":"Three Black Crows - Candlestick Pattern","uri":"/collections/crypto/three-black-crow/"},{"content":"Understanding the Upside Gap Two Crows Candlestick Pattern for Crypto Trading When it comes to crypto trading, technical analysis plays a crucial role in identifying profitable trading opportunities. One such technical analysis tool is candlestick patterns, which help traders predict market trends based on past price movements.\nThe upside gap two crows candlestick pattern is one such pattern that traders should be aware of. This pattern consists of three candles, where the first and third candles are long and bullish, while the second candle is short and bearish. The second candle also has a gap up from the first candle, which creates an “upside gap.”\nThe upside gap two crows pattern is a bearish reversal pattern, indicating a potential trend reversal from a bullish to a bearish market. It signifies that the bulls are losing momentum and the bears are taking over, leading to a potential downtrend.\nTo use this pattern effectively in your crypto trading strategy, it’s essential to understand its characteristics and potential impact on the market. This pattern can be useful in identifying entry and exit points in the market and placing stop-loss orders.\nUpside Gap Two Crows In an uptrend, the Upside Gap Two Crows is a three-line bearish reversal pattern. Although the pattern’s name implies that it is made up of two candles, it comprises three lines.\nA candle that appears as a long line with a white/green body forms the first line. It could be a white/green candle, a long white/green candle, a white/green marubozu, an opening white/green marubozu, or a closing white/green marubozu.\nAny black/red candle, except for doji, whose body is positioned above the previous candle’s body, can be used as the second line. In other words, the candle must close higher than the previous close.\nA black/red candle also forms the third line. Its body engulfs the body of the previous candle. Furthermore, the candle’s closing price is higher than the closing price of the first line, forming a gap.\nOn the next candles, the pattern must be validated. It’s worth noting that the initial line of the pattern creates a support zone that must be disrupted. It’s possible that if the pattern isn’t confirmed, it’ll just be a brief stop in an upswing.\nNOTES:\nThe upside gap two crows is a three-candle pattern that indicates a slowing of momentum in an uptrend and may indicate a reversal lower.\nThe pattern begins with a large up candle in an uptrend, followed by a gap higher into a down candle, and finally a larger down candle that engulfs the previous.\nA lower reversal is unlikely. After the pattern, the price may move sideways or rally.\nBefore acting, some traders wait for “confirmation.” Before selling or shorting, wait for the price to fall below the low of the third candle.\nIdentify Upside Gap Two Crows The upside gap two crows candlestick pattern is a bearish reversal pattern with three days of construction. To successfully recognise the upward gap two crows pattern, a trader needs to look for the following features.\nThis candle must be followed by a lengthy white/green first candle that signals the continuance of the uptrend and a solid bearish candlestick.\nA second bearish day is represented by a small black or colourful second candle with a small genuine body that gaps up.\nBecause it opens above it and shuts below it, a larger black or coloured third candle that gaps up completely immerses the second candle.The third candle must also shut above the close of the previous day.\nWhat Does Upside Gap Two Crows Pattern Tell Us In technical analysis, an upside gap two crows is a bearish reversal indicator. When the following conditions are met, the pattern emerges.\nA lengthy white (or green) candlestick represents a bullish candle that continues the uptrend, indicating a closing price much above the open price.\nDespite the security gapping higher at the open, candle 2 is bearish. As a result, this candle is black, with a closure below the open, and it gaps up from the previous candle.\nA third bearish candle has appeared. The candle opens higher than the open of Candle 2 and ends lower than Candle 2 but higher than the close of Candle 1. A larger down candle that “engulfs” Candle 2 visually represents this.\nThis design has various distinguishing characteristics. The pattern must first appear during a substantial rise. Second, the initial candle must be a large bullish candlestick (white or green) in the uptrend. This candle should be followed by a bearish candlestick (black or red) with a bit of genuine body that gaps up. Finally, the third candle must be a bearish candlestick that engulfs the previous candle, opening above it and closing below it. The third candle, however, must close above the first day’s closing.\nThe two-crow upside gap indicates that the security may be turning over as its upward trend comes to an end and a downturn begins. The reasoning behind this analysis is that the bulls have been unable to maintain upward momentum despite two strong opens (on Candles 2 and 3), implying that sentiment is shifting from bullish to negative.\nBecause the pattern is only three bars long, it is useful to look at the background and for confirmation when trading it. In a strong uptrend, the pattern could simply be a pause before the price resumes its upward trajectory. Waiting for confirmation includes waiting for the price to drop more before acting on the pattern. A current long trade, for example, is only terminated if the price continues to fall below the low of the third candle. A trader could alternatively short or sell near the third candle’s closure (no confirmation), with a stop loss placed above the third candle’s high if going short.\nTrading with Upside Gap Two Crows Many beginner traders will try to short the markets as soon as they learn about a pattern like an upside gap two crows, but this is not a good idea. To establish that a given price move is likely to occur, most candlestick patterns must be utilised in conjunction with other technical analysis techniques.\nSo, let’s take a closer look at how you may use the upward gap two crows to your advantage in trading!\nOversold Seeing if the market is overbought is one way to tell if it’s about to turn around as it develops a bearish reversal pattern. A large number of markets exhibit strong mean-reverting tendencies. This implies they make exaggerated movements in one direction, later corrected by a trend in the opposite direction. As a result, these markets are prone to being overbought, implying that they have risen too far and will soon reverse.\nWhen to know market gap is too much Some methods are more popular than others. Let’s look at the two most prevalent approaches.\nRSI- Using the RSI indicator to determine when a market has risen too much is a time-tested strategy. An overbought market is defined as the RSI over 70, which is consistent with the indicator’s primary meaning.\nAnother typical strategy is to require that the market perform its highest close or open a particular number of bars back. This is also the definition that the double seven trading technique use.\nVolume Another effective way is to use volume conditions, which provide a clearer picture of what’s going on in the market.\nIf a market’s volume rises, for example, it means that more transactions backed up a move, making it more relevant, at least in principle.\nHere are a few of our favourite volume settings:\nWe frequently compare the current candle’s volume to the previous candle’s volume. This allows us to determine whether the volume is high or low in comparison to previous data.\nOther times, we require volume to be at its maximum or lowest x-bars back reading.\nAs always, backtesting should be used freely, and you should soon find something that works.\nLimitaion of Upside Gap Two Crows If the price does decrease following the construction of the pattern, this candlestick pattern does not predict how far it will fall. This means that other types of technical analysis, such as price action analysis, are needed to discover an exit point for short positions or to forecast how far the price may fall.\nThe pattern does not always lead to a downward reversal. Following the pattern, the price might either move sideways or continue higher.\nConclusion In conclusion, understanding candlestick patterns such as the upside gap two crows can help traders make informed decisions and maximize their profits in the crypto market. Incorporating technical analysis tools into your trading strategy can significantly improve your chances of success.\nReferences https://www.youtube.com/watch?v=jmmxeX4GYI8 https://www.nothardtrading.com/upside-gap-two-crows-candle-pattern-explained/ http://www.candlescanner.com/candlestick-patterns/upside-gap-two-crows/ ","description":"Learn how to use the upside gap two crows candlestick pattern in your crypto trading strategy. Our guide provides a detailed analysis of this pattern and its potential impact on your trades.","tags":["crypto"],"title":"Understanding the Upside Gap Two Crows Candlestick Pattern for Crypto Trading","uri":"/collections/crypto/upside-gap-two-crows/"},{"content":"Belt Hold Line Candlestick Pattern Belt Hold is candlestick pattern is considered as a modest reversal pattern that can suggest a bearish or bullish trend reversal depending on the pattern and direction of the trend it appears. Belt Hold Candlestick pattern is called yorikiri in Japanese..\nThe model is one single pattern, nearly the same as Marubozu’s, because the pattern showed the force of bearish and bullish activity. It has a large, real body with small or no shadows. The single bar in an upward trend is a possible top-turn pattern and the pattern is a bare ribbon holding pattern.\nBearish Belt hold consists of a black candle, opening at or near the top and closing on or around the bottom, leaving very little bottom or top shadows. A bullish hold pattern, on the other hand, appears downstream and is a possible reverse pattern of the bottom. The candle opens at or close to the high and ends at or near the low. It consists of a rising candle.\nThe height of these bars indicates a potential change in sentiment; thus, the pattern becomes increasingly crucial as the candles grow in size.\nBoth the bullish and bearish belt hold patterns are more dependable when they appear at the market’s extremes, as indicated by support, trend lines and moving averages,pivot points , and resistance levels. They become much more important when combined with other patterns, such as the engulfing pattern or the dark cloud cover pattern.\nThe belt hold is a particular candlestick pattern that provides one of the most specific clues of the current market’s direction. However, simply understanding how to spot this pattern is insufficient.\nTypes of Belt Hold Candlestick Pattern Bullish Belt Hold Candlestick Pattern The Bullish Belt Hold Pattern is a single bar Japanese candlestick pattern that indicates a potential reversal of a current decline. A trading day begins at its lowest level in this candlestick pattern, but as the day goes, the stock starts to move up, eventually closing around a high. That being stated, it is not required for the trading day to end at its peak.\nThe Bullish Belt Hold candle resembles a white opening Marubozu candle. The candle pattern is such that it opens at the low of the period and then rallies to close around the high, leaving only a tiny upper shadow and no lower shadow.\nThis pattern reappears in decline after a string of bearish candlesticks. Furthermore, the opening price of canldle is significantly lower than the price of the candle previous day. The pattern is known as a “belt hold” because it closes well within the previous candle’s body, preventing the price from falling further.\nIdentifying a Bullish Belt Hold Line Belt Holding Requirements\nThe candlestick never trades below its opening price. The bar must be upbeat. The preceding bar had to be bearish. Optional conditions\nThe candle must be extinguished within the body of the previous candlestick. The candlestick should close close to the high point. The candlestick must open lower than the previous candlestick’s low. Bearish Belt Hold line Candlestick Pattern A bearish belt hold is a pattern that typically indicates a shift in investor sentiment from bullish to bearish. On the other hand, the bearish belt hold is not considered highly dependable because it occurs frequently and is frequently erroneous in anticipating future share prices. When establishing trend predictions, as with any other candlestick charting method, more than two days of trade should be considered.\nBearish belt holds are relatively easy to identify, but they must be validated by looking at durations that stretch beyond the day period. The previous day’s candlesticks should be in a clear uptrend, indicating that mood has shifted. To help confirm the authenticity of the signal, the candlestick should belong, and the next session’s candlestick should likewise be bearish.\nIdentifying a Bearish Belt Hold Bearish Belt Holding Requirements\nThe candlestick does not trade over its opening price. The candlestick must be negative. The previous candle must have been bullish. Optional conditions\nThe belt hold bar must be able to close within the body of the first candlestick. The candlestick must shut near the bottom point. The candlestick must open higher than the previous candlestick’s high. Psychology behind Belt Hold Candlestick Pattern When a bullish belt hold candlestick appears, it implies that purchasers controlled the share price for the whole trading session because the share price never fell below its initial share price.\nThe share price has only risen since the trading session began, which is why a bullish belt hold has no lower shadow. The buyers have been in command throughout the trading session.\nThe bearish belt hold suggests a short-term shift in attitude from bullish to bearish. The top of the bearish belt hold is converted into a resistance line. If the belt holds resistance line aligns with an already formed one, it receives more weight.\nHow to Trade Belt Hold Line Pattern Entery After spotting the belt, hold a candlestick, open a position in the direction of the candle and go long immediately after it closes.\nStop Loss Always use a stop-loss order to safeguard your belt hold transactions, just like you would any other trade. The stop-loss should be placed precisely beneath the entry candle’s low. However, because the marubozu candle appears during periods of extreme volatility, the price action will frequently reach the stop if it is positioned correctly on the other side of the candle.\nAs a result, it is recommended that you set your stop-loss one candle away from the marabozu. If you are buying a stock on the low belt signal, set your stop-loss below the candle’s low that follows the closing marubozu candle. If you are selling on a belt hold signal, your stop-loss order should be placed above the candle that precedes the marubozu.\nProfit Set an initial aim when using a belt hold line trade. The first option is to exit your position when the stock reaches a size that is at least twice or three times the size of the marubozu pattern.\nThe other way for profit taking is to employ price action rules to determine when to exit your trades; this, of course, requires more skill and, most importantly, discipline.\nReferences https://www.investopedia.com/terms/b/bearishbelthold.asp https://www.angelbroking.com/knowledge-center/share-market/bullish-belt-hold https://www.candlescanner.com/candlestick-patterns/bearish-belt-hold/ https://stephenbigalow.com/ ","description":"Belt Hold is candlestick pattern is considered as a modest reversal pattern that can suggest a bearish or bullish trend reversal depending on the pattern and direction of the trend it appears.","tags":["crypto"],"title":"Bullish And Bearish Belt Hold Line Candlestick Pattern","uri":"/collections/crypto/belt-hold-line/"},{"content":"Top 10 Chrome Flags to enable in 2021 Google Chrome is the most used browser. It’s Excellent choice, whether you are using an Android phone, Mac or Windows PC. The Chrome stable is terrific on its own, but it’s not for power users. There are many tweaks that you can make to meet your specific needs.\nYou may be a tester if you don’t mind bugs and like to test new features. Already a member of the Chrome OS Beta channel or Chrome OS Dev channel. You can have Chrome in a variety of different ways with these different versions. Chrome Flags are a great way to try out a few features.\nThis begs the question: What exactly is a Chrome Flag? What is a Chrome Flag? Do you know which flags are worth enabling and why? You can find out more about You’re here to learn all you can say about Chrome Flags. Let’s take a look at some of the options. Take a look at these flags, their capabilities, and how you can enable them. We will also suggest some of our favorite flags enable (on both). Chrome OS and Chrome browser), so you can try new features. Right away\nWhat is a chrome flag? Chrome Flags are essentially experimental features that Google has created. Currently, testing is being done on Chrome OS or Chrome browser. It’s now in beta. It is important to remember that some Flags are only available for Chrome OS. Chrome browsers for Android, iOS and macOS are supported. Flags will eventually be taken down when they are part of a stable Chrome releases or gets absorbed into Chrome development tools.\nAfter you enable Flags, your browser must be restarted. If you are using Chrome OS, your computer will need to restart for the change to occur.\nThey are unstable and could cause unintended behavior in your browsing device. It is also important to know that all browser-based Flags are not tested for online security protocols. This means that you should have some Security risk when you doing financial transactions online using chrome flag which are not tested.\nYou can run different versions of Chrome browsers or Chrome OS . There are many Flags to choose from. You can find the Chrome Beta, Dev, or Different Flags will be featured on Canary channels. If you are interested in a specific Flag, please let us know. If you are looking for the most Flags available, this is your best option.\nSo it would be best if you took some risks when you live on the cutting edge of Chrome. These small risks are not enough to worry about. Let’s talk flags!\nHow can I enable Chrome Flags Chrome Flags can be a good but sometimes dangerous option. It is easy to find and enable Flags. Flags can be enabled by following this simple procedure. It is entirely independent of your operating system. Locate available Flags:\nOpen your Chrome browser and type: chrome://flags/\nClick enter to go to the Flags main page. From This page allows you to scroll through the seemingly endless list of Flags There are many options. You can quickly browse the options and decide to enable or disable them.\nFlags can be disabled using the drop-down menus. However, this is quite simple. It is overwhelming.\nYou can search the www.flagsearch.com site to find specific types of Flags. On a Mac, you can use Control+F and Command+F to navigate the page. You can use Control+F or Command+F on a Mac if you are not sure. It is much easier to enable a particular Flag. The following tools are available to assist you in allowing a specific Flag.\nAttach a precise tag to the Flag in hand.\nType: chrome://flags/#tag\nThis will take to you directly to the Flag. This will take you to the Flag. Syntax: #tag should be replaced by the appropriate tag. Each Chrome Flag comes with a tag.\nIdentifying enabled Flags and Resetting Flags Chrome Flags have a number of annoying aspects. Determine which Flags are allowed. The majority of Flags should be visible. The Flags that you have personally enabled are listed at the top of this Flags page.\nTop 10 Chrome Flags you should enable right now You now know what a Chrome Flag looks like, but you may want to give it a try. It’s nice, to begin with, Flags that are useful and informative and have few shallow risk. Flags that are related to some consistency bugs in dark mode can cause crashes, but they aren’t fatal to Browser or OS\nPlease take note of the Chrome versions that each Flag applies to. There are some These Flags are for Chrome OS only, but others can be used to Chrome OS as well. Browser across all platforms\nAll things in dark mode chrome://flags/#enable–force-dark (Mac, Linux Windows, Chrome Android, OS)\nchrome://flags/#webuidark-mode Only (Chrome OS only)\nchrome://flags/#darklight-mode (ChromeOS only)\nThe dark mode is for you if you are like me. It’s blindingly distracting at work for me. It is not possible to disable it. These Flags all have a commonality; however, they are not related. Different things may apply. For the first Flag, dark mode will be in effect.\nAll web content is found in Chrome browser on any operating system. All web content in your Chrome browser across any operating system.\nThe second Flag allows WebUI to use dark mode when you have Chromebook.\nThe third Flag addresses dark mode in the Chrome OS interface. This Flag will turn on a dark mode toggle in the fast Settings tray This Flag can be used to force Chrome into light mode. OS, if that’s your thing. You can get genuinely dark if you wish. All three Flags can be enabled on Chromebook.\nYou should be aware that the dark/light toggle switch can sometimes be disabled. Inconsistent. To see the difference, you may have to switch it on and off again. Correct theme applied to your Chrome OS device, default apps and Chrome OS.\nEnable autofill prediction chrome://flags/#show_autofill-type–predictions Windows, Chrome OS and Android\nThis Flag simplifies your life by loading auto-filled text in fields\nBefore you start typing anything. Your name, address and zip code will be displayed. Now, you will be able to access your information.\nAll forms can be loaded automatically online when you submit your shipping or billing information. Pretty You can save time and have a minimal risk by using Flags.\nTab Hover Cards Images chrome://#tabhover-card–images (Android):\nThe hovering card previews are my favorite feature in Safari. Get for pages in each tab. This Flag will allow you to bring this feature. You can access your Chrome browser from any platform. You can hover over a tab in Chrome. You’ll see a preview of the web content within that tab, just as on macOS.\nEnable Trash Folder chrome://flags/#files–trash (Chrome OS only):\nPeople coming from a Macintosh or Windows PC will notice the absence of a Chrome OS trash folder If you have recently switched to Chrome OS and need your You can activate this Flag by trashing it back. You will be able to toggle on after that. You can see the Trash folder under Files on Chrome OS. This is quite handy. If you have a bad habit or are prone to accidentally deleting things, Flag it.\nSystem Emoji Picker chrome://#enable cros-ime system-emoji-picker:\nChrome OS has suffered for years from fragmented integration among the different components. Touch UI and keyboard UX. Inserting is one of the most annoying problems. Emojis require you to use the virtual keyboard. This Flag will allow you to: You can now right-click or long-press to open an emoji picker Chrome OS 91. Chrome OS 91 offers a search function and all the emojis. You would expect it on an Android or iOS smartphone.\nPassword import chrome://#PasswordImport Chrome OS only:\nA valuable flag for anyone moving from a Mac to Chrome OS. Or Even those who use multiple browsers and operating systems, like me, can still benefit from them all. All the time. This Flag allows you to import passwords from Safari.\nOpera, Firefox, Microsoft Edge to Chrome. You can’t remember if you don’t know what to do. Your 20-character password, with five special symbols. This is the Flag to Enable\nCopy the link to the text chrome://#copy–link-to–text (Mac, Linux Windows, Chrome OS Android):\nSometimes, you might want to share a passage from an article online. Of course, you can share the regular link with the person you know. Shared with the need to scroll through to locate the desired text. Copy Link to text Flag lets you share a link that will automatically be Highlight the text in an online document or webpage. This is an obvious advantage for students, teachers, and anyone else who uses it. As part of their job, they spend a lot of time doing online research.\nMultiPaste virtual keyboard clipboard chrome://#enable cros-virtual keyboard-multipaste\nPeople who work all day with images and links know how valuable they are. A good clipboard is possible. This Flag allows you to enable Google’s MultiPaste Chrome OS keyboard is currently being tested. This Flag will be enabled once it is. Clipboard functionality will be added to the virtual Chrome OS keyboard. Starting There you will find all images, links and text that you have recently saved.\nCopy\nBloggers will love this feature. This feature makes it easy for us to share our Chromebooks make it a lot easier to do your job.\nThese are our top Chrome Flags to help you get started with your modding Journey. You can always find new Flags to explore. You’ll also have more options for experimental channels. Remember to Before enabling a Flag, it is essential to be familiar with its functions. There are Some Flags can seriously impact the performance of a Chromebook Particularly battery life.\nYou can make things worse by allowing something you don’t know to happen. You can roll back to the original settings for each Flag. Chrome Flags Chrome OS and Chrome can have a lot more fun with this feature. Browsers can be used across different devices. You can also modify your browser in many other ways.\nYour Chrome experience including Android apps and Linux apps. Let’s get started. Let us know what Chrome Flags you are using any mods that you do not like in the comments.\n","description":"Chrome Flags are essentially experimental features that Google has created. Currently, testing is being done on Chrome OS or Chrome browser.","tags":["techs"],"title":"Top 10 Chrome Flags to enable in 2021","uri":"/collections/techs/top-10-chrome-flags-2021/"},{"content":"What is GitHub Copilot- Everything you need to know OpenAI and GitHub launched a technical preview for a new AI tool called Copilot. It lives within Visual Studio Code and autocompletes code snippets.\nCopilot is more than just a parrot back program, GitHub says that this was the case before. Instead, it analyzes what you have already written and generates new matching codes, with specific functions that were once called. Automatically writing the code to import Twitter, draw scatterplots, or Get a Goodreads rating.\nAccording to a blog article by Nat Friedman, it works best with Python, Go, Ruby, Javascript, and TypeScript.\nFeatures of GitHub Copilot Convert comments into code: Let GitHub Copilot assemble your code by leaving a comment.\nAutofill: GitHub Copilot is great for producing boilerplate quickly and repetitive code Patterns. Give it some examples, and it will generate your own!\nTests: The backbone of any organization is the tests of a robust software engineering project. You can import a unit testing package. Let GitHub Copilot recommend tests that correspond to your implementation Code\nShow me other options: You might like to see a few? Different approaches? GitHub Copilot will provide a list of solutions. You can use the code as it is or modify it to suit your needs.\nGitHub sees this as an evolution in pair programming. Two coders can work together on the same project in order to catch each other’s mistakes. You can avoid making mistakes and accelerate the development process. Copilot is one of the best tools for managing your development projects.\nThis is the first major outcome of Microsoft’s $1 billion investment in OpenAI. OpenAI is the research company now headed by Sam, a former president of Y Combinator Altman. Altman has taken the reins of OpenAI. Status to a “capped profit” model. Microsoft invested and began licensing its GPT-3 text generation algorithm.\nCopilot uses an algorithm called OpenAI Codex. OpenAI CTO Greg Brockman refers to as a descendant from GPT-3.\nGPT-3, OpenAI’s most popular language-generating algorithm is available. This can produce text that is sometimes unrecognizable from human writing. It is able to write convincingly due to its sheer volume (175 billion parameters) or flexible knobs, which allow the algorithm to link letters, words, phrases, and sentences.\nGPT-3 generates English; OpenAI Codex generates code. OpenAI plans to release Codex via its API, later Developers will be able to build their own apps using the technology this summer.\nThe codex was trained with terabytes of openly accessible code pulled from GitHub as well as English language.\nExamples ScreenShot Visual Studio Code will send comments and code written by the developer, GitHub Copilot service synthesizes and recommends the implementation. The service is optimized to work with small groups according to GitHub. Functions with meaningful names for parameters such as the sortByKey returnRandomElement above.\nRefrences https://copilot.github.com/ https://www.infoq.com/news/2021/07/github-copilot-pair-programmming/ https://www.kdnuggets.com/2021/07/github-copilot-ai-pair-programmer.html ","description":"GitHub and OpenAI launched a technical preview for a new AI tool called Copilot. It lives within Visual Studio Code and autocompletes code snippets.","tags":["review"],"title":"What is GitHub Copilot- Everything you need to know","uri":"/collections/reviews/github-copilot/"},{"content":"Understanding Tweezer Top and Bottom candlestick Pattern This guide will explain Tweezer Top candlesticks and Bottom candlesticks and how traders use them to detect market shifts.\nWe will examine the relationship between tweezer top and tweezer bottom candlesticks, how to find these formations on charts, and how they indicate directional movement.\nWhat does Tweezer means? A tweezer can be described as a technical analysis pattern that uses two candlesticks. It can indicate a market top or bottom. Tweezer patterns can be reverse patterns. They are formed when more than one candlestick touches the exact bottom for a bottom pattern for a market tweezer, or two or more candlesticks touch a top for a top pattern for a market tweezer.\nTweezer bottoms can be considered short-term bullish patterns that reverse, while tweezer tops can be regarded as bearish reversals. Both formations were unable to push the top and bottom further. Each type of pattern requires careful observation and research in order to be correctly understood.\nWhen bulls push prices higher during an uptrend, a bearish tweezer tip is often seen. This is generally considered a strong bullish signal. On the second day, traders reverse their market sentiment. The market opens but does not surpass the previous day’s highs and then heads straight down, often eliminating most prior period gains.\nA bullish tweezer top is achieved when prices are falling, and bears continue to push them lower. This is usually a solid bearish trend. Day 2 is a reversal. Prices open but don’t breach the previous day’s lows and then head sharply higher. Day 2’s bullish move can quickly erase losses of the prior trading session.\nTweezers are an investment strategy that allows traders to be more precise in identifying market trends. Although tweezers come in many different forms, there are a few common traits. These candlestick patterns often appear at market-turning points. They can be used to analyze the market or provide signals to trend traders.\nIn the popular Japanese candlestick charting technique book Japanese Candlestick Charting Techniques, Steve Nison made tweezers a mainstream item. A red candle that is dark or dark indicates that the close was below the opening, while a green or white candle signifies that the closing price was higher than the opening.\nWhat make Tweezers so important candlestick pattern? It may be a leading indicator indicating a short-term trend reversal or price swing may be taking place. It is similar to a pair of tweezers “plucking” a top or bottom on a chart. A completed tweezer may be used to verify if there has been a significant high or low. Tweezer may be used to confirm or reinforce other indicators of reversal. A failed tweezer can indicate a continuous movement is in progress and may be helpful in stop-loss placement. Tweezer top “fails” if a new high is reached immediately after completion (candle), while a tweezer base “fails” if the next candle reaches a new low. Tweezer Top Candlestick Pattern The tweezer-top candlestick pattern, which features two candlesticks, is a bearish reversal type. The pattern begins with a green candlestick that appears when a stock is experiencing an uptrend. The second day opens high and makes a similar high to the first.\nWhat does Tweezer Top Candlestick Chart Pattern tell us? The Tweezer Top candlestick patterns are formed when the prior trend is up. The bullish candlestick looks like it is continuing the uptrend. The high of the bearish candle’s second day’s high on the following day indicates resistance. The bulls appear to increase the price, but they aren’t willing to buy at higher values. The highest candles that have almost the same height as the top indicate resistance. This signal may also mean that an uptrend could be reversed and form a downtrend. The bearish reversal will be confirmed the next day when the bearish candle forms.\nCriteria for identifying tweezer tops Three factors identify tweezer tops:\nThe stock market is currently in an uptrend A solid green body is seen on the first day The formation of the red bodies on the second day. This has a similar high to the day before. How do you interpret the reading of top patterns from tweezer? The resistance zone is indicated by the second candle’s high. Despite the bulls pushing the price up, they don’t want to buy at higher rates. The bears are forced to take action and move the price down. The strength of resistance is also indicated by identical height of the highest candles, which suggests that an uptrend could pause or reverse and turn into a downtrend. The third-day bearish reversal candles are made when the trend reversal candle is usually confirmed.\nTweezer Bottom Candlestick Pattern The bullish reversal pattern is called the tweezer top. A red candlestick marks this candlestick pattern on the first day when there is a downtrend. The second day’s low is similar to the day before.\nWhat does Tweezer Top Candlestick Chart Pattern tell us? The Tweezer Bottom candlestick patterns are an indication of a downtrend. The formation of a bearish tweezer candlestick looks like a continuation of the current downtrend. The low of the bullish candle indicates the support level on the following day. The strength of the support is indicated by the bottom candles that are almost equal in low. This signal may indicate that the downtrend could be reversed and form an uptrend. The bulls then step in and move the price up. The bullish reversal will be confirmed when the bullish candle forms the next day.\nCriteria for identifying tweezer bottoms These are the three things that can help you identify tweezer bottoms\nThe stock market is currently in a downtrend A solid red body was observed the first day The formation of the green bodies on the second day. This has a low similar to the day before. How do you interpret the reading of Bottom patterns from tweezer? The second candle’s low indicates a support area. The bears are pushing the price lower, but they won’t sell below that price. The bulls then step in and drive the price up with great force. The support strength is evident in the identical lows of both candles, which indicates that the downtrends could reverse or pause. The trend is confirmed by the third day of bullish reversal candle formation.\nRefrences http://midasinvestments.blogspot.com/2013/05/forex-tutorials-part-8-advanced.html https://fxsignl.blogspot.com/2019/10/tweezer-bottom-candlestick-pattern.html https://hitandruncandlesticks.com/tweezer-top-candlestick-pattern/ https://iqtradingpro.com/make-money-iq-option-tweezer-candlestick-pattern/ ","description":"A tweezer can be described as a technical analysis pattern that uses two candlesticks. It can indicate a market top or bottom. Tweezer patterns can be reverse patterns.","tags":["crypto"],"title":"Understanding tweezer top and tweezer bottom candlestick pattern","uri":"/collections/crypto/tweezer-candle-stick-pattern/"},{"content":"Apple App Tracking Transparency Features - Complete Guide Apple’s latest iOS 14.5, and iPadOS 14.5, introduced App Tracking Transparency. Apple states that App Tracking Transparency allows you to control which apps can track your activity across different companies’ apps or websites. Apps will need to ask users if they are able to track them once this feature has been enabled.\nYou may still be unfamiliar with the tracking technology. Think about all the times you searched for a product you might need just once. Although you may have purchased the item, you will be bombarded with advertisements for similar products on Facebook and Instagram. Do you feel this is happening to you? Facebook and other companies use user data such as browsing history to provide “personalized” ads. Although this may sound fine at first, it can become tedious after you see 50 advertisements for toilet bowl cleaners.\nWhat is App Transparency Transparency (ATT) App Tracking Transparency (or ATT) is a new feature in iOS, iPadOS and tvOS 14.5. It requires apps to ask permission to track your activity on other websites and apps. An “advertising ID” number is a unique number that Apple devices use to identify their device. This can be used for tracking and ad targeting. App developers can create detailed records of your iPhone and iPad usage by associating this identifier with other information.\nAll apps that use iOS 14.5 must follow the new App Tracking Transparency framework by Apple to ask you permission to track your movements. This does not apply to your activity other than company apps. Facebook can, for example, track your activity on Facebook Messenger, Instagram and WhatsApp without you asking. However, if it wants information about your interactions with other apps or tracks you across different websites, it must request your permission.\nMost iPhone and iPad users don’t know that their activity is being tracked. Apps regularly track what they do on other websites or in other applications.\nHow to enable App Tracking Transparency for iPhone and iPad App Tracking Transparency is automatically enabled for all users once iOS 14.5 has been installed. Here’s how to check if it is on or turn it off.\nLaunch Settings in Your iPhone or iPad. Scroll down and click privacy. TapTracking. Verify that the toggle is on. Allow apps to request to track is ON (green). To turn it off, tap the toggle until it turns gray. App Tracking Transparency will be enabled once the app launches. Apps that have met Apple’s new guidelines will display a prompt next to launch the app. The prompt will ask for permission to track your activities across the app and other websites of the companies. You can choose to either allow or decline to allow the tracking.\nAll the apps that ask for permission to track your movements will be listed under the tracking section in Privacy.\nHow to allow an application to track you if it has been rejected previously You can toggle the permission for each app you choose if you have previously refused to allow them to track your data.\nLaunch Settings in Your iPhone or iPad. Scroll down and click privacy. TapTracking. Tap the toggle next to the app whether you wish to allow tracking. How to find out what data apps have collected App tracking transparency is a way to prevent an app from monitoring your iPhone activity across all apps. It doesn’t prevent apps from collecting information directly from you in-app and then selling it to advertisers.\nYou can use privacy labels to find out what an app knows about you. Search for the app you are looking for in the App Store. After you have found it, tap it to go to the App Privacy section. All data categories that can be “linked to your identity” through the app will be displayed.\n","description":"Apple's latest iOS 14.5 and iPad OS update includes a new tracking feature. App Tracking Transparency allows you to control which apps can track your activity across different companies' apps or websites.","tags":["techs"],"title":"Apple App Tracking Transparency Features - Complete Guide","uri":"/collections/techs/apple-app-tracking-transparency-feature/"},{"content":"Javascript Beginners Tips and Best Practices 2021 I’ll discuss JavaScript tips, tactics, and best practises for beginners that I use and find valuable in this blog.\nUsing Numeric Separator When dealing with enormous numbers, this is one of the most commonly utilised operators. When a separator (simply a _) is used in a number, it looks nicer than when the number is not separated.\nFor Example:\nInstead of this:\nlet number = 12345678; try this:\nlet number = 12_345_678; It also works with any other numeric base:\nconst binary = 0b1010_0111; const hex = 0x12_34_56_78; Delete and Splice To remove an item from an array, use splice instead of delete. The object attribute will be deleted, but the array will not be reindexed or its length updated. It appears to be undefined as a result of this.\nDelete \u003e someArray = [\"1\", \"2\", \"3\", \"4\"]; [(\"1\", \"2\", \"3\", \"4\"])] \u003e delete someArray[0]; true \u003e someArray[0]; undefined It’s worth noting that the property isn’t set to undefined; rather, it’s deleted from the array, making it appear undefined. When recording the array, the Chrome dev tools make this distinction evident by reporting empty.\nSplice Splice() removes the element from the array, reindexes it, and modifies its length.\n\u003e myArray = [\"1\", \"2\", \"3\", \"4\"] [\"1\", \"2\", \"3\", \"4\"] \u003e myArray.splice(0, 2) [\"1\", \"2\"] \u003e myArray [\"3\", \"4\"] Checking for many conditions We can put all values in an array and use indexOf() or includes() to match multiple entries.\nif (value === 1 || value === \"one\" || value === 2 || value === \"two\") { } Instead use:\nindexOf() if ([1, \"one\", 2, \"two\"].indexOf(value) \u003e= 0) { } includes if ([1, \"one\", 2, \"two\"].includes(value)) { } Bitwise operators ~~ Math.floor() can be replaced with the double NOT bitwise operator.\nconst floor = Math.floor(12.8); //12 Instead use:\nconst floor = ~~12.8; //12 map and for loop To cycle through an array’s items, use the map() function method.\nvar squares = [1, 2, 3, 4].map(function (item) { return item * item; }); console.log(squares); // Output =\u003e [1, 4, 9, 16] Code that is less cluttered — When doing the same tasks, map almost always uses less code than for. It can occasionally be written effectively on one line, however for takes at least two or three lines, with braces included. Additionally, scope isolation, a reduction in the number of variables required, and reduced size all contribute to objectively clean code.\nImmutability is a term used to describe the ability to change one’s mind. The original array will not be changed in any way. This could be advantageous in situations where the original array is still required. Although for loops can be implemented without updating the old array, this involves additional code and requires us to update our new array as part of the loop operation. map(), on the other hand, keeps things cleaner because you only have to work in one scope to keep immutability.\nRounding Numbers in javascript The toFixed() method transforms a number to a specified amount of decimals after it has been rounded.\nvar num = 12.345678; num = num.toFixed(2); console.log(num); //Output =\u003e 12.34 Inside a loop, try-catch should be avoided Each time the catch clause is executed, the try-catch construct produces a new variable in the current scope, where the caught exception object is assigned to a variable.\nvar items = [\"foo\", \"bar\"], for (var i = 0, len = items.length; i \u003c len; ++i) { try { // do something that throws an exception } catch (e) { // handle exception } } Instead Use:\nvar items = [\"foo\", \"bar\"], try { for (var i = 0, len = items.length; i \u003c len; ++i){ // do something that throws an exception } } catch (e) { // handle exception } When an error occurs, the first one allows you to continue the loop, whereas the second leaves it. If an exception thrown by your code isn’t severe enough to bring your programme to a halt, the first option is best.\nconsole.table To display javascript object in a tabular manner, use console.table:\nvar items = [ { fruits: \"Apple\" }, { fruits: \"Banana\" }, { fruits: \"Orange\" }, { fruits: \"Mango\" }, ]; console.table(items); Always use Semicolon It is a good habit to use semi-colons to end lines. You won’t be notified if you forget it because the JavaScript parser will insert it in most circumstances, but depending on Automatic Semicolon Insertion (ASI) is not recommended.\nGoogle, Airbnb, and jQuery all have Javascript style guides that incorporate this.\nDon’t forget “var” in your code Always double-check that you’re not assigning a value to an undeclared variable when assigning a variable’s value for the first time.\nWhen an undeclared variable is assigned, a global variable is created automatically. Attempt to avoid using global variables.\nOther programmes can readily overwrite global variables. For example, if two different components of an application establish global variables with the same name but different purposes, it can lead to unexpected problems, and debugging such a situation will be a nightmare.\nIn general, you should aim to scope your code so that it uses as little global scope as feasible. The more global variables you utilise in your script, the less likely you are to be able to use it in conjunction with other scripts.\nVariables in a function should normally be local so that they disappear when the function is exited.\n","description":"I will discuss JavaScript tips, tactics, and best practises for beginners that I use and find valuable in this blog","tags":["programming","javascript"],"title":"Javascript Beginners Tips and Best Practices 2021","uri":"/collections/programming/javascript/javascript-beginner-tip-2021/"},{"content":"Harami Candlestick Pattern Before you start wondering, the name ‘Harami’ does not refer to the Hindi word harami 😜. It’s thought to be an old Japanese word for “pregnancy.” When you see the candlestick formation, you’ll appreciate how intuitive this word is.\nWhat does harami candlestick pattern means? The harami candlestick is a Japanese candlestick pattern that consists of two candles and signals whether the market is likely to reverse or continue. As shown below, the harami candlestick pattern can signal both bullish and bearish signals:\nThe first candle (pregnant candle) is a large candle that continues the current trend, and the trailing candle is a small candle that protrudes like a pregnant woman, as shown in the photographs above. It’s worth noting that the second candle will technically gap inside the first. Gapping on forex/stock/crypto charts, on the other hand, is uncommon due to the 24-hour nature of currency trading. As a result, the theoretically perfect harami candlestick pattern is uncommon in the FX market, as gaps are narrow and the second candle frequently forms a small inside bar of the first.\nThe confirming candle is used to advise traders whether the smaller following candle initiates a reversal or continues the trend established by the starting candle. The harami pattern’s and other candlestick patterns’ popularity stems from their ability to catch a reversal at the most opportune time with minimal risk. Traders will be able to achieve exceptionally favorable risk-reward ratios as a result of this.\nPsychology behind harami candlestick pattern The stock market is constantly a battleground between bulls (buying) and bears (sellers). It’s all about supply and demand.\nConsider the following scenario:\nFor the past few days, a stock has been declining. A large red candle appears one day, indicating that the sellers have complete control.\nThe stock opens the gap higher the next day. Those who are short on the stock begin to fear that it will rise in price as a result of the price increase.\nThey cover their short bets, causing the price to rise even more.\nThe grip of the sellers is diminishing as the price rises. The purchasers are gaining the upper hand.\nIf the stock opens higher on the third day (after the Bullish Harami formation), it’s a sign that the trend may be changing and the stock price may be rising.\nBullish harami candlestick pattern A bullish harami is a candlestick chart signal that indicates the end of a bearish trend. A bullish harami may be described by some investors as a signal to place a long position on an asset.\nInvestors studying for harami candlestick patterns should start by looking at periodic market performance in candlestick charts. A bullish harami relies on initial candles to indicate that a downward price trend is continuing and that a bearish market appears to be pushing the price lower. Harami patterns appear over two or more days of trading, and a bullish harami relies on initial candles to indicate that a downward price trend is continuing and that a bearish market appears to be pushing the price lower.\nThe bullish harami pattern indication is represented on the chart by a lengthy candlestick followed by a smaller body, known as a Doji, that is totally contained within the vertical range of the previous body. A line drawn around this pattern is said to resemble a pregnant woman by some.\nA smaller body on the following Doji must close higher within the body of the previous day’s candle to form a bullish harami, indicating a larger possibility of a reversal.\nNotes:\nA bullish harami is a candlestick chart signal that indicates a price reversal in a bear market.\nIt’s usually signaled by a minor price increase (shown by a white candle) that can be contained within the given equity’s recent downward price movement (represented by black candles).\nBearish harami candlestick pattern A bearish harami is a two-bar Japanese candlestick pattern that indicates a price reversal is imminent. A tall white candle is followed by a little black candle in this arrangement. The second candle’s opening and closing prices must be contained within the first candle’s body. The formation of a bearish harami is preceded by an uptrend.\nThe potency of the pattern is determined by the size of the second candle; the smaller it is, the more likely a reversal will occur. A bullish harami, which is preceded by a downtrend and predicts that prices may reverse to the upside, is the polar opposite of a bearish harami.\nTo make a bearish harami pattern more useful as a trading signal, traders usually combine it with other technical indicators. For example, when a bearish harami emerges during a retracement, a trader may use a 200-day moving average to confirm the market is in a long-term decline and enter a short position.\nNotes:\nA bearish harami is a candlestick chart signal that indicates a price reversal in a bullish trend.\nA slight price decline (shown by a black candle) that can be contained within the given equity’s upward price trend (represented by white candles) from the previous day or two is usually indicative.\nTo boost the chances of a successful transaction, traders might employ technical indicators like the relative strength index (RSI) and the stochastic oscillator with a bearish harami.\nAdvantages and disadvantages of harami candlestick pattern Advantages Simple to recognize\nChance to profit from significant fluctuations with high risk-to-reward ratios\nIn forex/stock/crypto trading, it is often employed.\nDisadvantages Prior to execution, confirmation is required. How to trade harami candlestick pattern 1. Harami candlestick trading with indicators The harami candlestick pattern will be combined with Bollinger bands in this trading method.\nOnly trade the harami candlestick pattern, which appear when the price reaches a Bollinger band level on the upper or lower side.\nFor example, you can enter a short position after the price touches the upper Bollinger band at the same moment a harami is formed, as seen below. trading with indicators\nOne should hold their position and close it until the price touches the lower Bollinger band.\n2. Harami candlestick trading with price action The price action strategy option should always be included in our analysis because the harami candle is a price action component.\nWhen we trade on price movement, we are completely reliant on the chart’s price action.\nThis implies no indicators, oscillators, or moving averages, among other things.\nChart patterns, candle patterns, support and resistance levels, and other indicators should be used.\n","description":"Harami candlestick is a two candle pattern. The first candle is usually long, and the second candle has a small body. The second candle is generally opposite in colour to the first candle. On the appearance of the harami pattern, a trend reversal is possible.","tags":["crypto"],"title":"Harami Candlestick Pattern- Everything you need to know about","uri":"/collections/crypto/harami-pattern/"},{"content":"Inverted Hammer Candlestick Pattern When it comes to trading, knowing how to recognize potential reversals will help you maximize your profits. One such signal that can assist you in identifying new trends is the inverted hammer candlestick pattern.\nThis tutorial will tell you everything you need to know about the inverted hammer.\nWhat does the inverted hammer candlestick pattern means ? The real body of an inverted hammer candle is small, with an extended upper wick and little or no lower wick. It appears near the bottom of a downtrend and indicates the possibility of a bullish reversal. The longer upper wick indicates that the bulls are attempting to push the price higher. The validity of this move will be confirmed or rejected by price action in the future.\nThe shooting star should not be confused with the inverted hammer. Both candles have similar appearances, yet their meanings are vastly different. At the top of an uptrend, the shooting star is a bearish indicator, while at the bottom of a downtrend, the inverted hammer is a bullish signal.\nHow Inverted Hammer Candlestick is formed? When bullish traders acquire confidence, an inverted hammer candlestick appears. Bulls attempt to drive the price as high as they can, while bears (or short-sellers) attempt to fight the higher price. The positive tendency, however, is too powerful, and the market ends up at a higher price.\nIdentifying Inverted Hammer Candlestick Pattern As a little body, the inverted hammer design. There is also an enlarged upper wick, but there isn’t much in the way of a lower wick. This will be apparent at the bottom of a downtrend and could signal a possible bullish reversal.\nFurthermore, the longer upper wick may be signaling to investors that the bulls intend to push prices higher. Following price action, which may reject or confirm the coming adjustments, a more accurate picture will emerge.\nGreen Inverted Hammer vs Red Inverted Hammer A bullish, green Inverted Hammer candlestick is formed when the low and open are the same, and it is regarded as a stronger bullish sign than when the low and close are the same (a red Inverted Hammer).\nPsychology behind inverted hammer candlestick pattern An inverted hammer candlestick pattern indicates that buyers are exerting market pressure. It warns that after a bearish trend, there may be a price turnaround. It’s vital to remember that the inverted hammer candlestick shouldn’t be used as a stand-alone indication; always double-check any potential signals with other forms or technical indicators.\nFinally, before acting on the inverted hammer, examine your trading plan.\nAdvantages and disadvantages of the inverted hammer candlestick pattern Advantages If the inverted hammer candle initiates a new uptrend right away, traders can enter the market at the start of the trend and profit from the entire upward movement.\nOn a chart, the Inverted Hammer Candlestick is easy to spot.\nDisadvantages Price action is represented by the Inverted Hammer, which is a single candle. Without evaluating further supporting evidence/indicators, relying just on a single candle to overturn market momentum might lead to sub-optimal results.\nThe inverted hammer candle may indicate a brief uptick in positive price activity, but not a longer-term trend reversal. This can occur if purchasers are unable to maintain buying pressure in the face of a strong downward trend.\nHere’s how to trade an inverted hammer candlestick pattern if you come across one. When deciding whether or not to trade when the inverted hammer candlestick pattern appears, it’s vital to keep an eye out for other important signals that could indicate a possible reversal. However, if you are convinced that a change will occur, you can use spread bets or CFDs to trade. Both of these are ancillary products that allow investors to trade on both decreasing and rising prices.\nThere are numerous possibilities, such as trading, going along, or buying, but there is also the option to sell or go short if the signal is not promising enough and the downward trend appears to be likely to continue.\n","description":"The real body of an inverted hammer candle is small, with an extended upper wick and little or no lower wick. It appears near the bottom of a downtrend and indicates the possibility of a bullish reversal. The validity of this move will be confirmed or rejected by price action in the future.","tags":["crypto"],"title":"Inverted Hammer Candlestick Pattern","uri":"/collections/crypto/inverted-hammer/"},{"content":"Understanding the Shooting Star Candlestick Pattern When the term “Shooting Star” is mentioned, many envision a celestial object descending towards the earth’s surface. In the realm of Technical Analysis, however, a shooting star takes on a different meaning—it represents a bearish candlestick pattern characterized by a long upper shadow and the absence of a lower shadow.\nWhat Exactly is the Shooting Star Candlestick Pattern? The shooting star candlestick pattern is identified by a bearish candlestick with a significant upper shadow, minimal or no lower shadow, and a small real body near the period’s low. This pattern typically emerges after a period of upward movement in prices. Specifically, it occurs when a security opens, experiences substantial upward momentum, but ultimately closes near its opening price.\nTo be classified as a shooting star, this pattern must manifest during a price advance, and the distance between the highest price of the period and the opening price should be more than twice the size of the shooting star’s body. Additionally, there should be little to no shadow below the real body of the candlestick.\nNotes:\nAfter an advance, a shooting star pattern appears, indicating that the price may begin to plummet.\nThe formation is bearish since the price attempted a substantial climb during the day, but sellers took control and pulled the price back down toward the open.\nAfter a shooting star, traders usually wait to observe what the next candle (period) does. They may sell or short if the price falls during the next term.\nIf the price rises following a shooting star, the formation could have been a false indication, or the candle could be indicating a potential resistance area in the candle’s price range.\nKey Psychological Insights into the Shooting Star Candlestick Pattern The shooting star pattern signifies a potential price top and an impending reversal. It is particularly effective when it appears after a series of 2-3 consecutive rising candles with higher highs, indicating a strong upward trend. This pattern reflects the battle between buyers and sellers during the trading session.\nAt the start of the period, buyers push the price significantly higher, demonstrating continued buying pressure. However, as the period progresses, sellers step in and drive the price back down towards the open, erasing the gains made earlier in the day. This shift indicates that buyers have lost control by the session’s close, and sellers are gaining dominance.\nThe extended upper shadow of the shooting star represents buyers who initially entered the market during the period but are now facing losses as the price retraces back to the opening level. Confirmation of the shooting star pattern comes from the subsequent candlestick, which should open lower or near the previous close and then move lower with increased volume. This confirmation suggests a high probability of a price reversal and potential further decline.\nTrading Strategies and Best Practices Key Takeaways:\nBefore trading with the shooting star, keep the following considerations in mind:\nTrade Entry: Confirm that the prior trend is an active bullish trend before entering a shooting star trade.\nStop Loss: When trading the shooting star candlestick pattern, you should always strive to place a stop-loss order.\nProfit: For this trade, the price goal should be equivalent to the size of the shooting star pattern.\nEntry and Confirmation: Before executing a trade based on the shooting star pattern, it’s crucial to confirm the prevailing trend. Ensure that there is an active bullish trend before considering a short position triggered by the shooting star pattern.\nRisk Management: Implementing a stop-loss order is essential when trading the shooting star candlestick pattern. This risk management strategy helps limit potential losses in case the trade doesn’t unfold as expected.\nProfit Target: The price target for a trade initiated by the shooting star pattern should ideally match the size of the pattern itself. This approach helps set realistic profit-taking levels based on the pattern’s structure.\nBenefits and Limitations of the Shooting Star Pattern Benefits: Simplicity: The shooting star pattern is straightforward and easy to identify, making it a valuable tool for new technical traders.\nConfirmation: When combined with other technical indicators or analysis, the shooting star pattern can provide strong confirmation of a potential price reversal.\nLimitations: Single Candle Significance: In a robust uptrend, a single candlestick pattern like the shooting star may not carry significant weight on its own. It’s crucial to consider broader market trends and confirmatory signals.\nRisk Management: Relying solely on the shooting star pattern without proper risk management strategies, such as stop-loss orders, can expose traders to unnecessary risks.\nConclusion The shooting star candlestick pattern serves as a valuable tool for traders seeking to identify potential bearish reversals in price trends. By understanding its formation, psychological implications, and best practices for trading, traders can leverage this pattern effectively in their decision-making processes. However, it’s essential to complement the shooting star pattern with comprehensive risk management strategies and additional technical analysis for optimal trading outcomes.\n","description":"A shooting star is a candlestick with a long upper shadow, little or no lower shadow, and a little true body. It appears after a period of upward movement. The distance between the high price and the opening price must be more than twice the size of the shooting star's body.","tags":["crypto"],"title":"Shooting Star Candlestick Pattern - Everthing you need to know","uri":"/collections/crypto/shooting-star/"},{"content":"Candlestick Pattern - Everything you need to know about Doji Star Consider the market conditions when the buying trend is strong, but some traders also expect the current trend to reverse; this is why they sell. What happens in this situation? If all traders go out and sell, the market will fall. But when it is not strong enough, the market will reflect indecision. Traders pay close attention to these moments to predict when market trends may change. But how do you know when it will happen by looking at the graph? Well, technical traders will look for Doji candlestick patterns that appear on the trading chart.\nDoji candles belong to the Japanese candlestick chart family. It is named for its unique training. We will try to understand what a Doji candlestick is and what its support level should be when you see it.\nMeaning of Doji Star candlestick pattern The Doji candlestick pattern is a three-column pattern. It is considered to be a sign that the current market trend may be about to reverse. It is a versatile candlestick pattern with two variants, bullish and bearish. Its variants depend on your existing trends.\nTo understand the Doji candlestick pattern, it is important to understand a simple Doji candlestick pattern. When the opening price and the closing price are at the same or almost the same level, a Doji candlestick pattern is formed. The pattern looks like a plus or a cross.\nKey Points:\nThe Doji pattern is a 3 column inverted candlestick pattern. Start with a long candle, open it to draw a Doji, and then reverse it with a larger candle in the opposite direction. Type of Doji Star candlestick pattern Doji Star – Looks like a star with the same opening and closing value and the same length of the top and bottom wicks. This happens when neither a bullish or bearish trend is enough to affect market sentiment.\nLong-legged Doji – A star-shaped Doji with extended upper and lower wicks. It also represents an undecided mood with high volatility.\nDragonfly Doji – You can find it at the bottom of the downtrend, indicating the rejection of lower prices. Unlike the Doji Star and Long-legged Doji, Dragonfly does not portray the indecision of the market. Rather, it portends a possible uptrend reversal. You can recognize the dragonfly by its unique appearance, lack of real body, and long lower wick.\nPrice Doji – It is represented by a single horizontal line, which represents the final hesitation of the market. This pattern appears at the opening and closing, both high and low are the same.\nGravestone Doji – Gravestone Doji is located on the other side of Dragonfly Doji. Appears in an uptrend, indicating that the market refuses to accept higher prices. It is a Doji candle with no real body and extended upper shadow.\nPsychology behind Doji Star Pattern Technical analysts believe that all known information about the stock is reflected in the price, which means that the price is valid. However, the evolution of prices in the past has nothing to do with the evolution of prices in the future, and the real price of a share may have nothing to do with its true or intrinsic price. Therefore, technical analysts use tools to help filter noise to find the most likely trades.\nOne tool that was formed by a Japanese rice trader named Honma from the town of Sakata in the 18th century, and it was introduced to the West in the 1990s by Steve Nison.\nEach candlestick pattern has four data sets to help define its shape. Based on this form, analysts can make assumptions about price behavior. Each candle is based on the opening price, the highest price, the lowest price, and the closing price. The time duration or tick interval used does not matter. The complete or hollow bars created by the candlestick pattern are called the real bodies. The lines that extend to the outside of the body are called shadows. Stocks whose closing price is higher than their opening price will have a hollow candle. If the stock price closes lower, the body will have a full candle. One of the most important candle patterns is called a Doji.\nWhen the opening price and the closing price of a stock are almost the same, a Doji is created, which refers to both the singular and the plural forms. Doji often looks like a cross or a plus sign, and the body is small or non-existent. From an auction theory perspective, Doji represents the indecision of buyers and sellers. We are all the same, so prices have nowhere to go; buyers and sellers are at a standstill.\nSome analysts interpret this as a sign of retracement. However, this may also be the time when buyers or sellers gain momentum for continued trends. Dojis usually appear during the integration period and can help analysts identify potential price breakouts.\nBullish Doji Star candlestick pattern The bullish doji pattern is a three-column pattern formed in a downtrend. The first column has a long red(or black) body, and the second column opens lower. It is closed like a doji pattern, and the trading range is small. The third bar closed above the midpoint.\nTechnical analysts use bullish doji candles to determine the reversal of the current long-term market downtrend. Experts see the bullish doji pattern as a signal to buy. They also use it to check the weather and avoid selling assets. It mainly appears at the bottom of the chart. This is a bell that indicates that the bulls are coming after a long-term bearish phase.\nIdentifing bullish Doji Star candlestick? Look for a normal red candle at the bottom of the chart on the first day. If it confirms the current downtrend and shows that the price closed below the opening price.\nNext, look for the little doji on the next day, which shows that there is almost no difference between the opening price and the closing price. Now look for the third candle that shows an upward gap.\nTrading in the bullish Doji Star pattern? If the price starts to move in the opposite direction, you should consider taking a long trade while holding your stop loss to ensure safety. You can also try checking the 5 minute and 15 minute time frames to analyze this pattern and take appropriate precautions. Once the bullish doji pattern forms, the price starts to rise. Therefore, if you start trading after confirming this mode, you will likely make a profit.\nBearish Doji Star candlestick pattern A bearish doji candle is a bearish reversal pattern that appears in an uptrend. It is represented by two lines. The body of the first candle is very long because it is rising in an uptrend. Then, especially the doji that opened and closed above the first candle formed.\nHow to identify a bearish Doji Star candlestick? You should look for a candlestick with a long green(or white) line and a doji above the first candle. You should also remember that the shadow of the doji will not be too long and the shadow of the line will not overlap. Therefore, these tips can easily identify a bearish doji candlestick pattern.\nTrading in bearish Doji Star pattern? A bearish doji is a signal that shows the end of an uptrend and the beginning of a bearish reversal that caused the price to fall. Therefore, it is wise to sell the stocks/crypto-coins whenever there is a bearish doji pattern.\nLimitations of a Doji In isolation, the Doji candle is a neutral indicator that provides little information. Also, dojis are not common. Therefore, it is not a reliable tool for detecting events such as price reversals. When it happens, it is not always reliable. There is no guarantee that the price will continue to develop in the expected direction after the candle is confirmed.\nThe size of the tail or wick of the doji plus the size of the confirmation candle can sometimes mean that the entry point of the trade is far from the stop loss position. This means that traders need to find another stop loss position, or they may need to abandon the trade, because too large a stop loss may not justify the potential return from the trade.\nCalculating the potential gains of doji trading can also be difficult, because candlestick patterns usually do not provide price targets. Other techniques, such as other candlestick patterns, indicators, or strategies are needed to exit the trade in a profitable situation.\nExample Charts ","description":"Doji candles belong to the Japanese candlestick chart family. It is named for its unique training. We will try to understand what a Doji candlestick is and what its support level should be when you see it.","tags":["crypto"],"title":"Doji Star Candlestick Pattern - Everything you need to know about","uri":"/collections/crypto/everything-you-need-to-know-about-doji-star/"},{"content":"Javascript String Methods Cheat Sheet 2021 Use these JavaScript string methods to easily perform character manipulation.\nWhen programming in JavaScript, you often come across scenarios that require string manipulation. For example, when retrieving email , you may need to convert all characters to lowercase or use regular expressions to check if the entered password meets conditions.\nJavaScript string methods will help you easily perform all these operations on the string according to your requirements. Here are some string methods and examples to help you understand them better.\nJavaScript String Methods A string is a basic data structure consisting of characters. This data structure is part of all the major programming languages, including Python, JavaScript, Java, etc.\nString methods are pre-built JavaScript methods that can help developers perform common operations on strings without having to write code manually. They are executed using dot notation attached to the string variable.\nBecause they are just JavaScript functions, they always end with square brackets and can contain optional parameters. Understanding is essential before proceeding. Let’s get started and understand these methods in more detail.\nFirst, let’s declare a string with value “Hello, Welcome to Another Techs.”:\nlet str = \"Hello, Welcome to Another Techs.\"; String.toUppercase() and String.ToLowerCase() methods The string method ** toLowerCase () ** converts all characters in a given string to lowercase format. Similarly, the ** toUpperCase () ** method converts all characters to the uppercase format. These functions do not modify the original string.\nLet’s use a simple example to see these two methods:\nconsole.log(str); console.log(str.toUpperCase()); console.log(str.toLowerCase()); When you run the above code in the console, you will receive the following output :\nHello, Welcome to Another Techs. HELLO, WELCOME TO ANOTHER TECHS. hello, welcome to another techs. String.replace() method As the name suggests, the replace() method can help you replace the part of the string with another part. This method has two parameters: The first is the substring to be replaced, and the second is the substring to be replaced. This method does not modify the original string in any way.\nFor example, if you want to replace Another Techs with string javascript cheatsheat, you can use the replace() method like this:\nlet new_str = str.replace(\"Another Techs\", \"javascript cheatsheat\"); console.log(new_str); Hello, Welcome to javascript cheatsheat. String.trim() method The method trim() removes all spaces in the string, spaces before the first character and after the last character. This method does not require you to pass any parameters, nor does modify the original string. It is very useful for validating user input on forms.\nLet’s explore this string method with a new example:\nlet untrimmedString = \" Welcome to Another Techs \"; let trimmedString = untrimmedString.trim(); console.log(untrimmedString); console.log(trimmedString); \" Welcome to Another Techs \" \"Welcome to Another Techs\" String.concat() method concat() method is used to concatenate two or more strings. You can add one or more parameters to this method to concatenate them into a single string. It will not make any modifications to the original string. This is an example that shows concatenating two strings to form a new string:\nlet s2 = \" How are you?\"; let new_str = str.concat(s2); console.log(new_str); Hello, Welcome to Another Techs. How are you? String.charAt() method charAt() string method returns the character at index specified in the string. It only accepts one parameter, the index of the character to be retrieved. The index value ranges from 0 to and the length is -1. This is an example of the charAt() method:\nconsole.log(str.charAt(11)); console.log(str.charAt(2)); o l String.substring() method substring() method is used to obtain a substring or part of the original string. This method takes two parameters: start index and end index. The output substring starts from the specified start index and is printed to the end index -1.\nThis is a quick example of the ** substring () ** method:\nconsole.log(str.substring(4, 8)); \"o, W\" String.search() method The search () method helps to find a specific substring or characters in the original string. This method accepts a set of characters or substrings as parameters and keeps track of the entire string. When a match is found, the starting index of the matching part is returned. Otherwise, this method returns 1.\nYou can use the search() method in the following ways:\nconsole.log(str.search(\"Techs\")); console.log(str.search(\"6\")); 26 -1 String.indexOf() and String.lastIndexOf() methods The ** indexOf () ** method can help you find the first index where the specified character or substring appears in . It starts from the left of and traces the string to check if the given parameter matches.\nLet’s take as an example to find the index where Another appears in the string:\nconsole.log(str.indexOf(\"Another\")); 18 If the given parameter does not exist in the string, the method returns with a value of 1.\nSimilarly, the lastIndexOf() method returns the index of the last occurrences of a given character or string. This is an example:\nconsole.log(str.lastIndexOf(\"l\")); 9 String.split() method split() method is used to split all words or characters into a string according to the separator parameter passed to the method. The method’s return type is an array. The array consists of characters or substrings, divided according to the given separator. The method does not modify the original string.\nFor example, if you pass a space (\"\") as the separator parameter to the divide method, the output will look like this:\nlet splitString = str.split(\" \"); console.log(splitString); Array(5) [ \"Hello,\", \"Welcome\", \"to\", \"Another\", \"Techs.\" ] If you don’t pass the parameter to the ** split() ** method, it will return a array containing a single element composed of the value of a string variable.\nString.charCodeAt() method Similar to the charAt method, the charCodeAt() method returns ASCII valueof the character at the specified index. This string method only takes as a parameter, which is the index from which characters will be retrieved.\nstr.charCodeAt(9); str.charCodeAt(str.length - 1); 108 46 Similarly, the index value ranges from 0 to length-1. If you try to pass an index that exceeds the allowable limit, this method will return -1.\n","description":"When programming in JavaScript, you often come across scenarios that require string manipulation. For example, when retrieving email , you may need to convert all characters to lowercase or use regular expressions to check if the entered password meets conditions.","tags":["programming","javascript"],"title":"Javascript String Methods Cheat Sheet 2021","uri":"/collections/programming/javascript/javascript-string-methods-cheat-sheet-2021/"},{"content":"How to Buy Cryptocurrency in India Cryptocurrency has a tale of distrust and strict regulation in India. The Reserve Bank of India (RBI) and the Ministry of Finance have listed the potential use of volatility for investors’ dangerous and illegal activities as their concerns. After the Bitcoin crash in 2017, the Ministry of Finance warned investors to stay away from Bitcoin and other cryptocurrencies. The Ministry of Finance has compared virtual currencies with Ponzi schemes and warned that they will not be regulated or protected by the Indian government.\nAfter trying to ban banks from dealing with Indian cryptocurrency exchanges, a Supreme Court ruling, and an attempt to ban cryptocurrencies forever, Indians can still buy cryptocurrencies. Investors who want to buy cryptocurrencies can still buy through Indian exchanges such as WazirX or international exchanges such as Binance. Take a look at some of the exchange options that India offers.\nIndia is a country with strict regulations on cryptocurrency and plans to ban any virtual currency that does not come from the government. Therefore, investors may need to be cautious and be prepared to liquidate their cryptocurrency holdings. This guide will explain how to continue buying cryptocurrency and other things to keep in mind.\nHow does Cryptocurrency Investment Works? Cryptocurrencies came into life in 2009 with the work of the first cryptocurrency – Bitcoin.\nBitcoins do not have a physical appearance like the US dollar or Indian Rupee note. You can only see the bitcoin address.\nBitcoin codes are created during the bitcoin mining process. The mining process needs large powerful computers and a continuous electricity supply. It’s not sufficient to mine bitcoins in India due to the high price of power in India.\nMost of the bitcoins are mined in China and countries like Norway where computer infrastructure and power are cheap.\nPeople like you and me can purchase bitcoins directly from the crypto exchange. I will go through the bitcoin exchange process later in the article.\nI should also inform you that It’s a good practice to have the cryptocurrencies safe in a wallet rather than storing them in the exchange for security reasons.\nHow to Buy Cryptocurrency in India? Buying crypto in India is still a legitimate process that investors can quickly take part in by opening an account with a crypto exchange. Citizens can trade various currencies, hold virtual currency in wallets, and more once they have registered for an account.\nWhat You Need To Buy Crypto In India? Before setting up an account online with a crypto exchange, you will need to confirm your identity with some documents. Various exchanges will have various rules when it comes to account setup, but the following things are good to have on hand to make the process faster:\nYou may need to upload photos of your Aadhaar card or other acceptable ID A private and secure internet connection (public WiFi poses security issues) A cell phone you can use for two-factor authentication An account with an exchange that operates in India A bank account you can withdraw from to deposit rupees to your exchange A secure method of cryptocurrency storage. Most brokerages will have a good built-in wallet or protected vault system that you can use. Choose a platform to buy cryptocurrencies Cryptocurrency Exchange Screen A cryptocurrency platform is an exchange or website that helps connect buyers and sellers of cryptocurrencies. The platform helps to trade and exchange various cryptocurrencies.\nWithout a platform, it is difficult to invest in cryptocurrencies unless you meet the cryptocurrency seller in person.\nIt is also important to choose the correct platform, it is secure and compatible with your cryptocurrency transactions.\nFind a cryptocurrency exchange with INR pairs so that you can directly invest in cryptocurrencies without having to convert Indian currencies to Bitcoin or USDT.\nSecurity The security of cryptocurrency exchanges and cryptocurrencies is an important factor, because if the exchange is not secure, your cryptocurrency and funds may be stolen.\nYou should choose an encrypted exchange that uses encrypted transactions, stores encrypted currencies in cold locations (not connected to the Internet), and ensures the security of the trading platform.\nEase of use The exchange must provide a simple and fast trading platform that can be used on the internet and smartphones. So you can trade anytime, anywhere.\nThe platform should allow you to deposit/withdraw in local currency without any problems.\nExchange rate Transaction fees are typically less than 1% of each transaction, and if you do a large number of transactions, the fee may be less.\nIn addition to the transaction fees, you also have to pay the withdrawal fees. You should compare all the fees the exchange charges for its services.\nYou should choose a platform that provides all facilities at a reasonable cost, because transaction fees represent a large investment cost.\nBest Cryptocurrency Exchange Platform 2021 1. WazirX - Best Cryptocurrency Exchange in India ** WazirX ** is the most popular cryptocurrency exchange in India, starting trading with on March 8, aiming to become the most trusted cryptocurrency exchange operating in India. It plans to launch a fully functional cryptocurrency exchange that will support different cryptocurrency pairs. WazirX also launched its own WRX Coin token. You can earn 100 WRX for free by joining the exchange. Lately, WazirX was taken by Binance, the world’s largest exchange. Provides a large number of supported tokens.\n2. CoinSwitch Coin switch is an instant cryptocurrency exchange that can help users make transactions between more than 400 cryptocurrencies. It has a very simple and easy-to-use interface. Users can also use credit cards to buy cryptocurrencies on Coinswitch.\n3. Coindcx CoinDCX is another great cryptocurrency exchange in India. CoinDCX provides instant fiat-to-crypto currency conversion free of charge, so users can access various financial products and services supported by industry-leading security procedures and insurance protection. CoinDCX provides users with an integrated combination of products, including its P2P store to buy 100+ cryptocurrencies immediately at INR\n4. Cashaa Cashaa is a UK-based cryptocurrency banking solution, launched in India in 2019. Cashaa is a cryptocurrency exchange that allows users to buy 7 Bitcoins (BTC), Ethereum (ETH, USDT and CAS, and INR. The list is and the list is constantly growing. Cashaa (CAS) crypto coin is the native cryptocurrency of the platform. Cashaa follows strict KYC and all accounts are verified on the same day at . Cashaa India is located in Mumbai. As mentioned on the Cashaa website, will provide users with an access card with a key so they can withdraw their funds.\n5. Zebpay Zebpay is India’s oldest and largest cryptocurrency exchange. Due to the ban imposed by the Reserve Bank of India, Zebpay moved to Malta, also has an office in Singapore, and currently supports more than 150 countries. Zebpay charges a transaction fee of 0.15% of the manufacturer’s fee and 0.25% of the taker’s fee. They also charge different withdrawal fees for different cryptocurrencies, and the Zebpay app is also available for Android and iOS phones.\n6. Colodax Colodax was founded by the CrypDates team in 2017. It is a B2P (broker to Peer) exchange that allows Indian traders to easily deposit and withdraw Indian rupees. The broker is a high-frequency trader affiliated with our exchange, and users need to complete KYC to trade on our platform. Now refer a friend to get up to 10,000 NPXS tokens. You can trade all major cryptocurrencies on Colodax, such as Bitcoin (BTC), Ripple (XRP), Ethereum (ETH), Litecoin (LTC), etc.\n7. BuyUcoin Launched in 2016, BuyUCoin is the oldest cryptocurrency exchange still operating in India after the Reserve Bank of India ban. It is another Indian market leader in the cryptocurrency market. It provides a simple and reliable platform to buy, exchange, store and accept many cryptocurrencies, such as Bitcoin, Ethereum, Ethereum Classic, List, NEM, Civic, Litecoin, Bitcoin Cash, and so on.\nBuyUcoin has more than 250,000 users exchanging assets in more than 30 cryptocurrencies on the platform. Users can use credit cards to buy cryptocurrencies directly on the exchange. Buy you coin also added a repeat buy function to so that users can invest as SIP for encryption. BuyUcoin developed and proposed a “sandbox” framework to monitor cryptocurrencies in India to avoid the possibility of a total ban on cryptocurrencies in India\n","description":"Buying crypto in India is still a legal process that investors can easily take part in by opening an account with a crypto exchange. Citizens can trade different currencies, hold virtual currency in wallets and more once they have registered for an account.","tags":["crypto"],"title":"How to Buy Cryptocurrency in India","uri":"/collections/crypto/how-to-buy-cryptocurrency-in-india/"},{"content":"Evening Star Pattern The evening star candlestick pattern is the bearish counterpart of the morning star pattern in technical analysis. Because the evening star pattern is a top reversal, it should be acted upon if it occurs after an uptrend.\nWhat does Evening Star Pattern means? The Evening Star composes of three candles. The first candle of the evening star pattern should be light-colored and have a relatively large body. The second candle is a star. It is a candle with a shorter body and does not touch the body of the previous candle. The gap between the two real bodies of the candlestick makes it a Doji star.\nThe evening star shows the first visualization of weakness, since the buyer could not raise the price until it is much higher than the closing of the previous period. This weakness is confirmed by the following the stars. These candles should be a dark and must close well with the previous candle.\nAn evening star candlestick pattern should have a gap separating the first and second real bodies and then another gap separating the second and third real bodies.\nHow to identify Evening Star Candlestick Pattern? Identifying the evening star candlestick pattern on the forex/stock/crypto chart is more than just identifying the three main candles. What is needed is to understand past price behavior and where the pattern appears in existing trends.\nEstablish an existing uptrend-the market should show higher highs and lows.\nBig bullish candle -Big bullish candle is the final product of heavy buying pressure and the continuation of an existing uptrend. At this point, traders should only look for long trades, because there is no evidence of reversal.\nBig bearish candle - This candle shows the first sign of new selling pressure. In the non-forex market, this candle opens downwards from the closing price of the previous candle, marking the beginning of a new downward trend.\nFollow-up price action - After a successful reversal, traders will notice lower highs and lows, but the risk of failed moves should always be managed by using well-positioned stop losses.\nIn the market, traders will always look for signs of indecision when buying pressure subsides and the market is flat. This is the ideal place to see the Doji candles.\nWhat does the evening star pattern show? An evening star pattern is a useful tool for technical analysis because it can predict changes in investor sentiment and price momentum.\nAs mentioned above, the evening star pattern consists of three candles, one per period. Bullish candles are long on the first day and asset prices have strong bullish momentum. Following the sharp rise in prices reflected by the bull gap, momentum began to weaken the day after the star appeared.\nHowever, the second day was still a wavering day between bullish and bearish sentiment. If there is a gap down when the market opens on the third day, it indicates that momentum will reverse and traders have made a short decision. When the price closed much lower at the end of the third day, the evening star pattern is confirmed.\nHow do technical analysts trade when they see the evening star pattern? Once the evening star candlestick pattern appears, traders may wish to use it as a signal to place a sell order. This may be particularly useful before major news releases, as the star indicates that the market will lack the belief that the upward trend will continue. However, traders who want to reduce their risk may wish to wait and use the star as a signal, planning to enter the market by selling in a subsequent downtrend. This is because the breakout can follow the initial reversal to a lower trading range.\nThe following is an example of one way to trade the evening star pattern:\nSetting a correct time frame for the chart - This depends on many trading strategies and will give traders a more comprehensive understanding of price movements.\nUnderstand the opening, high, low and closing prices - viewing a chart with a 1-day candlestick chart gives traders a good idea. Traders will see the daily opening and closing prices, as well as the highest and lowest prices.\nWait for the daily RSI to exceed 70-most traders see RSI over 70 as a clear overbought signal. This is a common method used by forex/stock/crypto exchange traders.\nDegradation Time Frame - After confirming that the RSI is above 70 (overbought state) on the long-term chart, it’s time to zoom in.\nDifference between Evening Star Pattern and Morning Star Pattern Contrary to the evening star pattern, the morning star pattern sets the trend from bearish to bullish. The first candle of the morning star pattern is a long bearish candle, indicating bearish price momentum. “Morning Star” is a short-body candle (bullish or bearish) or doji.\nThe third candle is bullish, confirming the reversal and offsetting most of the loss from the first candle. Ideally, there is a space between the first candle and the morning star and a space between the morning star and the confirmation candle.\nIn addition to the evening and morning stars, there are other star patterns. All other star patterns are reversal patterns, which can help traders make buy or sell decisions. Although there has been controversy over whether technical analysis can be a profitable investment tool, the evening star pattern is considered a reliable tool for predicting bearish momentum.\nExample chart for evening star candlestick pattern ","description":"The evening star candlestick pattern is the bearish counterpart of the morning star pattern in technical analysis. Because the evening star pattern is a top reversal, it should be acted upon if it occurs after an uptrend","tags":["crypto"],"title":"Evening Star Candlestick Pattern","uri":"/collections/crypto/evening-star/"},{"content":"Understanding Merge Sort Algorithm Merge sort is a sorting algorithm based on “divide and conquer” technology. It is one of the most efficient classification algorithms.\nIn this blog, you will learn about the working principle of the merge sort algorithm, the merge sort algorithm, its time and space complexity, and its implementation in various programming languages such as C++, Python, JavaScript and Java.\nHow does the merge sort algorithm work? The operating principle of merge management is divide and conquer. Merge Sort Iteratively decompose the array into two equal sub-arrays until each sub-array contains one element. In the end, all these sub-matrices were merged times to order the resulting array.\nWith the help of Example, this concept can be explained more effectively. Consider an unsorted array with the following elements: {40,29, 45, 5, 11, 84, 12}.\nHere, the merge sort algorithm splits the matrix into two halves, calls for the two halves, and then merges the two ordered halves.\nSpace and Time Complexity of the Merge Sort Algorithm The Merge sort algorithm can be expressed in the form of the following recurrence relation:\nT(n) = 2T(n/2) + O(n)\nAfter solving this recurrence relation using the master’s theorem or recurrence tree method, you’ll get the solution as O(n logn). Thus, the time complexity of the merge sort algorithm is O(n logn).\nThe best-case time complexity of the merge sort: O(n logn)\nThe average-case time complexity of the merge sort: O(n logn)\nThe worst-case time complexity of the merge sort: O(n logn)\nThe auxiliary space complexity of the merge sort algorithm is O(n) as n auxiliary space is required in the merge sort implementation.\nMerge Sorting Algorithm Below is the Pseud code for merge sort:\nMergeSort(arr[], left, right) if left \u003e= right return else Find the middle index that divides the array into two halves: middle = left + (right-left)/2 Call mergeSort() for the first half: Call mergeSort(arr, left, middle) Call mergeSort() for the second half: Call mergeSort(arr, middle+1, right) Merge the two halves sorted in step 2 and 3: Call merge(arr, left, middle, right) C++ Implementation of merge sort #include \u003ciostream\u003e #include \u003cvector\u003e #include \u003calgorithm\u003e #include \u003citerator\u003e #include \u003cnumeric\u003e //Function which merges the array template\u003cclass T\u003e static void merge(T\u0026 items,auto first,auto mid,auto last) { //temp vector will hold sorted elements std::vector\u003ctypename T::value_type\u003e temp; //reserving bytes of the memeory to avoid memory allocation temp.reserve(std::distance(first,last)); auto left = first; auto right = std::next(mid); for(auto i = first; i \u003c= last ; ++i) { //checks if the left part come to an end of not if(left \u003e mid){ temp.push_back(*right); right = std::next(right); } //check if the right part come to an end or not else if(right \u003e last){ temp.push_back(*left); left = std::next(left); } //Check which elements is smaller else if(*left \u003c *right) { temp.push_back(*left); left = std::next(left); } else{ temp.push_back(*right); right = std::next(right); } } //copies the sorted element back to the original array std::move(temp.begin(),temp.end(),first); } //Function for sorting template\u003cclass T\u003e void mergeSort(T\u0026 items,auto first,auto last) { if(first \u003c last) { auto mid = first;//mid iterator will point to mid element in the array std::advance(mid,std::distance(first,last)/2); //Finding the middle of array //firt half of array mergeSort(items,first,mid); //second half of array mergeSort(items,std::next(mid),last); merge(items,first,mid,last); } } //Function for printing the element template\u003cclass T\u003e void printElement(const T\u0026 items, const std::string\u0026 heading) { std::cout \u003c\u003c heading \u003c\u003c std::endl; std::copy(items.begin(),items.end(), std::ostream_iterator\u003ctypename T::value_type\u003e(std::cout,\" \")); std::cout \u003c\u003c std::endl; } //Main program int main() { std::vector\u003cint\u003e elem({40,29, 45, 5, 11, 84, 12}); printElement(elem,\"Unsorted Array:\"); mergeSort(elem,elem.begin(),elem.end()); printElement(elem,\"Sorted Array:\"); } Python Implementation of merge sort def merge_sort(elem): elem_length = len(elem) if elem_length == 1: return elem mid = elem_length // 2 left = merge_sort(list[:mid]) right = merge_sort(list[mid:]) return merge(left, right) def merge(left, right): output = [] i = j = 0 while i \u003c len(left) and j \u003c len(right): if left[i] \u003c right[j]: output.append(left[i]) i += 1 else: output.append(right[j]) j += 1 output.extend(left[i:]) output.extend(right[j:]) return output def main(): elem = [40,29, 45, 5, 11, 84, 12] print(elem) elem = merge_sort(elem) print(elem) JavaScript Implementation of merge sort \u003cscript\u003e function merge_sort (elem) { if (elem.length === 1) { return elem } const middle = Math.floor(elem.length / 2) const left = elem.slice(0, middle) const right = elem.slice(middle) console.log(middle); return merge( merge_sort(left), merge_sort(right) ) } function merge (left, right) { let output = [] let left = 0 let right = 0 while (left \u003c left.length \u0026\u0026 right \u003c right.length) { if (left[left] \u003c right[right]) { output.push(left[left]) left++ } else { output.push(right[right]) right++ } } return output.concat(left.slice(left)).concat(right.slice(right)) } const elem = [40,29, 45, 5, 11, 84, 12] console.log(merge_sort(list)); \u003c/script\u003e Java Implementation of merge sort public class MergeSort { public static void main(String[] args) { int[] elem = {40,29, 45, 5, 11, 84, 12}; int[] merged = mergeSort(elem, 0, elem.length - 1); for (int val : merged) { System.out.print(val + \" \"); } } public static int[] merge(int[] left, int[] right) { int[] sorted = new int[left.length + right.length]; int i = 0; int j = 0; int k = 0; while (i \u003c left.length \u0026\u0026 j \u003c right.length) { if (left[i] \u003c right[j]) { sorted[k] = left[i]; k++; i++; } else { sorted[k] = right[j]; k++; j++; } } if (i == left.length) { while (j \u003c right.length) { sorted[k] = right[j]; k++; j++; } } if (j == right.length) { while (i \u003c left.length) { sorted[k] = left[i]; k++; i++; } } return sorted; } public static int[] mergeSort(int[] elem, int left, int right) { if (left == right) { int[] br = new int[1]; br[0] = elem[left]; return br; } int middle = (left + right) / 2; int[] fh = mergeSort(elem, left, middle); int[] sh = mergeSort(elem, mid + 1, right); int[] output = merge(fh, sh); return output; } } ","description":"Merge sort is a sorting algorithm based on divide and conquer technology. It is one of the most efficient classification algorithms.Merge Sort Iteratively decompose the array into two equal sub-arrays until each sub-array contains one element.","tags":["programming"],"title":"Understanding Merge sort algorithm","uri":"/collections/programming/merge-sort-algorithm/"},{"content":"Windows 11 - All new features Windows has always been the scene of global innovation. It has always been the backbone of global companies and a place where aggressive startups have become household names. Many of us wrote our first email here, played our first PC game, and wrote our first line of code. Windows is where people create, connect, learn, and achieve - a platform trusted by more than a billion people today.\nMicrosoft has now announced its new release Windows 11. With this release, windows come with tons of new features. Here are some of those features:\nA new UI design Microsoft has redesigned the UI to increase users’ experience It is modern, fresh, clean, and looks awesome. From the new windows home button and taskbar to every sound, font, and icon, everything is beautifully designed to keep you in check and give you a sense of calm and relaxation.\nWindows has always tried to help you work the way you want by providing the flexibility of multiple windows and the ability to capture applications side by side. As of Windows 11, Microsoft has introduced Snap Layouts, Snap Groups, and Desktops to provide a more powerful multitasking method that allows you to keep track of what you need to do.\nIntegration of Microsoft Teams into the Taskbar In Windows 11, Microsoft introduces Microsoft Teams Chat in the taskbar. Now you can instantly connect with all your personal contacts via text, chat, voice or video anytime, anywhere, no matter what platform or device they use on Windows, Android, or iOS. If the person you connect with, on the other end has not downloaded the Teams app, you can still contact them via two-way text messages.\nImproved Gaming experience If you are a gamer, Windows 11 is for you. Windows 11 unleashes the full potential of your system hardware, allowing some of the latest gaming technologies to serve you.\nFor example:\nDirectX 12 Ultimate, which can achieve stunning, immersive graphics at high frame rates; DirectStorage can achieve faster loading times and more detailed game worlds; and Auto HDR provides a wider and more vivid color gamut for a truly fascinating viewing experience.\nNew and faster widgets Windows 11 brings you closer to the news and information that matters to you through a new and faster widget - a new personalized feed, backed by world-class browser performance provided by AI and Microsoft Edge. We often use mobile phones to check the news, the weather, or notifications. Now you can open a similar planning view directly from the desktop. When you open a custom feed, it slides across your screen like a piece of glass, so it doesn’t interfere with what you’re doing. For creators , widgets will open up a new space in Windows to provide personalized content.\nNew Microsoft Store The new Microsoft Store has been redesigned to increase speed and has a new design that is beautiful and easy to use. Not only it offer you more apps than ever, but also make all content - apps, games, shows, movies - easier to search and discover through curated stories and collections.\nRunning Android apps on windows Microsoft has announced that it will introduce Android applications to Windows for the first time. Starting later this year, people will be able to discover Android applications in the Microsoft Store and download them through the Amazon Appstore.\n","description":"Microsoft has redesigned the UI to increase users' experience of windows 11.From the new windows home button and taskbar to every sound, font, and icon, everything is beautifully designed to keep you in check and give you a sense of calm and relaxation.","tags":["review"],"title":"Window 11 - All new Feature you need to know about","uri":"/collections/reviews/windows-11-is-here/"},{"content":"Candlestick Pattern - Morning star For a long time, investors have been carefully studying the candlestick patterns that appear in the price trajectory. This is to predict the future. These areconsidered price signals in technical analysis.A fascinating set of reversal pattern analysis are those that indicate stars. A star is composed of a small real body (green/red or white/black), which separates the large real body before it. In other words, the actual body of the star may be within the upper shadow line of the previous trading day; all that is required is that the candles do not overlap.\nWhat is Morning star pattern? Morning star candlstick is a visual pattern composed of three candles, and technical analysts interpret it as a bullish signal. Morning star pattern formed after a downtrend, indicating that it started to climb upwards. This is a sign of a reversal of the previous price trend. Traders observe the formation of Morning Star and then use other indicators to find confirmation that a reversal has indeed occurred.\nFormation of Morning star candlestick pattern The first candle is a strong bearish candle (red).\nThe second candle is a special candle called the Spinning Top.\nLastly,third is a bullish candle (green) whose length is at least equal to half of the first candle.\nDifference between Morning Star and Doji Morning star pattern There is a slight variance in the morning star pattern. A doji is formed when the middle candlestick’s price action is essentially flat. This is a little candlestick, like the plus symbol, with no discernible wicks. Compared to a morning star with a thicker middle candle, the doji morning star more clearly displays the market’s uncertainty.\nBecause more traders can easily spot a morning star-forming, the arrival of a doji after a black candle typically results in a more aggressive volume increase and a proportionately longer white candle.\nWhat does morning star candlstick pattern tells us? The morning star pattern’s small real body represent a stalement between the bulls and bear. The bear are obviously in charge in a brisky descending market(crypto,stock,foreing exchange). With the emergence of a morning star in such an environment,it is a signal of a shift from the seller being in control to a deadlock between the selling and buying forces.This dealock may have occured either because of a diminution in the selling forces or an increase in the buying pressure. Either way, the morning star pattern tells us the rally’s prior power has slightly dissipated. This means the market will move upwards.\nUsing Morning Star Candlestick Pattern in Forex Trading Morning star pattern is a powerful price signal with high precision. The morning star candlestick pattern is very popular with price action traders. The best combination is to use analytical indicators to identify forex trends. Then use morning star pattern to determine the entry point.\nThere are many way you can trade in forex using morning star pattern few of them are:\n1. Combining with Support In this combination, the support area is considered to be retained. If there is a morning star pattern, the price is likely to rebound. You can enable a high security UP option.\nConditions: a 5-minute candlestick price chart, a support zone, and an expiration time of 15 minutes or more.\nOpenning a trade When the price falls into the support zone and forms a morning star candlestick pattern, turn on the UP option.\n2. Combining with RSI indicator RSI technical indicator is always a powerful indicator for price trend analysis. When combined with the morning star sail pattern, they will provide you with a good point for bottom fishing.\nConditions: 5-minute candlestick price chart, RSI indicator (14), expiry time of 15 minutes or more.\nOpenning a trade When the RSI indicator appears in the oversold zone (30) and Morning star mode, turn on an UP option.\nNotes Don’t use morning star candlestick pattern just to find a forex trade. Combine it with at least one indicator or other price signal to get a higher probability of winning. Restrict the use of morning star pattern when the market deviates. Because the accuracy of this candlestick pattern in the side market is not high. Limitation of Morning Star Pattern Limitation of Morning star pattern is that since this is a three-candle pattern, you must wait until the end of the third trading candle to complete the pattern. Normally, if this third candle is a tall white or green candle, we will get a good signal after the market has rallied sharply. In other words, the termination of morning star pattern may not provide attractive risk / reward trading opportunities. One option is to wait for the morning star support area correction and start eating the bulls.\nAlthough the ideal morning star and evening star should not allow any of their three entities to touch, but in a market where the opening and closing prices are the same or the closing price is the same, the definition of the morning star (and the evening star) is more flexible.\n","description":"Morning star candlestick pattern is a visual pattern composed of three candles, and technical analysts interpret it as a bullish signal. This is a sign of a reversal of the previous price trend.","tags":["crypto"],"title":"Morning Star Candlestick Pattern","uri":"/collections/crypto/morning-star/"},{"content":"Beginner guide to Snapcraft,Snap,Snapd and Snap Store Package Manager is a set of integrated services that make it easy to install, update, remove, and configure packages/programs on computers.\nEspecially with regard to the linux operating system, you can choose from a wide range of package managers, such as Pacman(Arch/Manjaro), apt(Ubuntu/Debian), yum(Red Hat/Cent Os),dnf(Fedora). Each of these package managers has different features that can distinguish them from the others.\nHowever, a relatively new package manager, Snap, has become a viable replacement for traditional package managers. Let’s take a look at Snap, its advantages and disadvantages, and how to install and use it on Linux.\nWhat is a Snap? Snap is a cross-platform packaging and deployment system developed by Ubuntu manufacturer Canonical for the Linux platform. It is compatible with most major Linux distributions, including Ubuntu, Debian, Arch Linux, Fedora, CentOS, and Manjaro.\nWhat is Snaps? Like any other package manager, Snap also includes a package called snaps. Unlike the traditional package manager counterparts, these packages have no dependencies and are easy to install.\nThe snaps end with the extension .snap, essentially a compressed file system using the SquashFS format, contains the entire package module, including the app, its dependent libraries, and additional metadata.\nWhat is Snapd? Snapd (or snap daemon) uses snap metadata to configure a security sandbox for applications on the system. Since it is a daemon process, the entire task of maintaining and managing the instant environment in happens in the background.\nWhat is Snap Store? Snap is located in the Snap Store, and you can browse and download them like other package managers. In addition, you can also use the option to publish your own instant packages directly to the Snap store, which is something that traditional package managers cannot achieve.\nIn addition to these elements, Snap has another basic component , called channel. The channel is responsible for defining the version of the installation snaps and tracking updates on its system. Therefore, when you install or upgrade snaps of , you can specify which channel you want to continue to use for each of these operations.\nInstalling Snap in Linux Below is the command to install snap on different linux distros:\nOn Debian/Ubuntu/Linux Mint based distros:\nsudo apt update \u0026\u0026 sudo apt install snapd On CentOS/RHEL- based distros:\nyum install epel-release \u0026\u0026 yum install snapd To install snap on Fedora:\nsudo dnf install snapd On Arch Based Distros:\nsudo pacman -S snapd Once installed, you have to start snap daemon(snapd service).\nFor systemd user you can run the following command:\nsudo systemctl start snapd.socket # To start snapd services sudo systemctl enable snapd.socket # To start services at boot Usage of Snap Finding in Snap Command to find app accross different category:\nsnap find category_name #For Example snap find development Installing a snap Package Command to install app accross different category:\nsudo snap install app #For example sudo snap install steam After installation, you can find the program in the “Applications” menu of the Linux distribution. Then you can run it directly from the menu or enter its name through the terminal.\nList all the snap apps Command to list app :\nsnap list To Know the version information of Snap app Command to know version of snap applications:\nsnap list package_name #For Example snap list steam Updating snap apps Command to update snap applications:\nsnap refresh To update a particular package\nsnap refresh app #For Example snap refresh steam Removing Snap app Command to remove snap applications\nsnap remove package_name #For Example snap remove steam Advantages of using Snap Each instant package runs independently to avoid interference with other packages on the system. Therefore, when you delete a snap package, the system removes all of its data, including dependencies, without affecting other packages. Needless to say, this also provides a more secure environment, because one package cannot access the information in another package.\nSnaps comes with dependencies (libraries) and can easily access the program immediately, because you no longer need to manually install missing dependencies to run on your system.\nThe real-time update will automatically adjust according to the set time interval. Therefore, you always run the latest version of the program on your system.\nSnap makes it easy for developers to distribute their software directly to users, so they don’t have to wait for the Linux distribution to start.\nIn addition to the previous point, another benefit of having developers responsible for packaging and distributing their software is that they don’t have to create a release-specific package because it comes with the required dependencies.\nDisadvantages of using Snap Because snaps are bundled with dependencies, they are larger and take up more disk space than other package manager counterparts.\nDue to included dependencies, snaps are distributed as compressed file system images and must be installed prior to installation. Therefore, snaps run slower than traditional software packages.\nAnother downside to allowing developers to distribute software packages is that these software packages have not been rigorously reviewed or vetted by the community, so there is a risk that they contain malware seen a few years ago.\nAlthough Snap allows developers to distribute their snaps directly to users, the distribution pipeline requires them to set up a Canonical account and host their snaps there. This goes against the nature of the open-source methodology, because even if the software remains open-source, the package management system is still controlled by the entity.\nRegarding malware risks, Snap now uses an automated malware test to scan user-uploaded packages for malicious code, and then is distributed on the Snap Store.\nSince the Snap backend is still closed source and controlled by Canonical, many major Linux distributions disagree with the idea of Snap as the default package manager on their systems.\n","description":"Snap is a cross-platform packaging and deployment system developed by  Canonical for the Linux. It is compatible with most major Linux distributions, including Ubuntu,Linux mint, Debian, Arch Linux,Fedora, CentOS, and Manjaro.","tags":["linux"],"title":"Beginner Guid to Snap and Snap Store","uri":"/collections/linux/beginners-guid-to-snap/"},{"content":"Piercing line pattern For each bearish pattern, there is also a bullish pattern. So previously we have seen a bearish pattern - The Dark Cloud Cover. Now its counterpart is the bullish piercing line pattern.\nWhat is the Bullish Piercing line Pattern The piercing line pattern is considered a bullish reversal candlestick pattern that is at the bottom of a downtrend. When bulls enter the stock/crypto market and prices rise, it usually indicates a change in trend.\nThis piercing line pattern consists of two downtrend candles(red and green). The first candle is a red (or black) body and the second candle is a green (or white) body. This green (or white) candle has a lower open price, preferably below the low of the red (or black) prior candle. This is the support for buyers to push the price higher to reach more than 50% from the last bearish candle.\nThe piercing line candlestick pattern is similar to the bullish engulfing pattern. In the bullish engulfing pattern, the green(or white) candle engulfed the entire previous red(or black) candle. In the piercing pattern, the green(or white) candle pierced, but does not envelop the previous entity. In the form of perforation, the higher the degree of penetration, the more likely it is to become a fund investment.\nHow to identify piercing line pattern The body of the first candle should be black or red and the body of the second candle should be white or green.\nThe downward trend has been obvious for a long time, and a long negative line appears at the end of the trend.\nThe opening price of the next time period is lower than the price of the previous time period.\nThe white or green candles close to more than half of the black or red candles.\nWhat does piercing line pattern tell Us After the strong downtrend took effect, the sentiment was bearish. The fear became more prominent and the price gap narrowed. The bears may even drive prices down even further, but before the end of the time period, the bulls intervene and reverse the prices drastically. The price closed near the high of the time period, a move that almost offset the price drop the time period before. Now, this worries the bears. More purchases in the next time period confirmed this move.\nExample chart for piercing line pattern ","description":"The piercing line pattern is considered a bullish reversal candlestick pattern that is at the bottom of a downtrend. When bulls enter the stock/crypto market and prices rise, it usually indicates a change in trend.","tags":["crypto"],"title":"Piercing Line Candlestick Pattern","uri":"/collections/crypto/piercing-pattern/"},{"content":"Dark Cloud Cover Candlestick Pattern What it is? A dark cloud cover is a bearish reversal candlestick pattern in which a falling candle (usually black or red) has an opening price higher than the closing price of the previous rising candle (usually white or green), and then the closing price is lower than the middle of the rising candle. point.\nThis pattern is important because it shows the momentum change from up to down. The pattern is created by an upward candle and a downward candle. Traders analysis that the price will continue to fall on the next (third) candle.\nWhat Does it means? Each candle has its own meaning and tells a unique story about the dynamics of the market.\nThe uptrend market is driven by positive sentiment. The bulls are under control and continue to drive the market higher.\nWith the formation of the first bullish candle of the dark cloud cover pattern, we notice that the buying pressure is high and buyers continue to support the uptrend.\nAs the market opens higher, this sentiment will continue until the next day’s open. Buying pressure continues to drive the market higher.\nHowever, it will soon be discovered that the positive gap has exhausted the final market share of purchasing power. Shorts control and push prices to make up for the gap.\nAt that stage, as more and more investors began to worry that the market was not as strong as they thought, selling pressure increased. More and more people began to sell their positions, and the market closed below the midpoint of the previous candle.\nMost traders believe that only dark cloud cover patterns that appear after an uptrend or general price increase are useful. As prices rise, this pattern becomes more important in marking possible downtrends. If the price movement fluctuates, the pattern is not important, because the price can remain volatile after the pattern.\nCriteria for dark cloud cover candlestick pattern Existing bullish uptrend. The up (bullish) candle in this uptrend. Gaps in the next time period. The gap from the top becomes a bearish candle. The bearish candle closed below the midpoint of the previous bullish candle. The dark cloud cover pattern is further characterized by black(red) and white(green) chandeliers with longer bodies and relatively short or shadowless shadows. These attributes show that, in terms of price movements, the drop is very decisive and significant. Traders can also follow this pattern to look for confirmations in the form of bearish candles. The price is expected to drop after the dark clouds are covered, so if it doesn’t, it indicates that the model may fail.\nThe close of the bearish(red) candle can be used to get out of a long position. Alternatively, if the price continues decreasing, the merchant can leave the next day (the ASA has been confirmed). If you approach near the close of bearish candle, you can increase the stop-loss in the next period. There is no profit goal for the pattern of the dark cloud cover. The merchant uses other candy patterns to determine whether to finish short trade depending on the dark cover of the cloud.\nExample Chart for dark cloud cover ","description":"Dark cloud cover refers to the candlestick pattern in technical analysis, which is a bearish reversal signal. It is observed when the down candle opens above the closing price of the previous up candle and continues to close below the midpoint of the up candle on the candlestick chart.","tags":["crypto"],"title":"Dard Cloud Cover - Candlstick Pattern","uri":"/collections/crypto/dark-cloud-cover/"},{"content":"Engulfing Candlestick Pattern for Stock/Crypto Trading Before entering Engulfing pattern, you should know the following …\nThe main goal of trading is to determine who “can” control the price and then trade in that direction. I say “probably” because we can never be sure whether aggressive buyers/sellers of the past will keep the momentum going into the future.\nFor example, when the price reaches a support zone, traders may think that buyers are more likely to control the market, thus pushing the price up to resistance. Then the merchant will enter a long transaction (purchase).\nTrend traders can do similar things after a certain pullback. You will then buy a currency pair, believing that buyers who were in control in the past will actively withdraw from the market.\nWhat in does Engulfing Pattern means? Traders use the engulfing candlestick pattern to enter the market, waiting for a possible trend reversal. This candlestick pattern marks the reversal of the current trend. These are two candlesticks, one of which completely “wraps” the body of the other candle. To get an effective engulfing pattern, the first candle must fit the body of the next. The engulfing candle can be either bearish or bullish, depending on its position in the current trend. The opposite is also possible. Since it consists of two candles, it is classified as a double candle mode.\nA candle shows the opening and closing prices of each trading cycle. Your time range can vary from one second to one day or more, depending on your chart settings. Seeing two side-by-side histograms can make a good comparison of the market direction from one moment to the next. The color of the candle indicates whether the price direction is ascending (green or white) or descending (red or black).\nCriteria: The market should be an indefinable trend (uptrend or downtrend). The second real body must engulf the previous real body. Second real body must be of the opposite color. Bearish Engulfing Pattern The bearish engulfing chart pattern is a technical pattern that indicates that the price is about to go lower. It consists of a tall candle (green) and a large downward candle (red), which surrounds the smaller upward candle. This model is necessary because it shows that sellers are performing better than buyers. These sellers are actively reducing prices, more than buyers can increase prices.\nThis pattern is the complete opposite of the bullish pattern. It provides the best signal when you see an uptrend and show an increase in selling pressure. Candles primarily lead to a trend reversal, as more and more sellers enter the market to further lower prices. This pattern consists of two candles, the second candle completely enveloping the previous green candle.\nHow to identify: When a bearish pattern appears, the price movement should show a clear uptrend. The huge bearish candle indicates that sellers are actively entering the market, providing an initial bias for further bearish momentum. The trader will then confirm that the trend is reversing by using indicators, support and resistance levels, and the subsequent price action that occurs after this pattern.\nBullish Engulfing Pattern The bullish engulfing pattern is the complete opposite of the bearish pattern. When it appears at the down of an downtrend, it provides the strongest signal and indicates an increase in buying pressure. As more buyers enter the market and increase prices further, a bullish engulfing candle usually triggers a reversal of an existing trend. This pattern includes two candles, the second candle fully covering the “body” of the prior green candle.\nHow to Identify: When there is a bullish pattern, the price trend should show a clear downtrend. The huge bullish candle indicates that buyers are actively entering the market, providing an initial bias for further bullish momentum. The trader will then confirm that the trend is reversing by using indicators, key support and resistance levels, and subsequent price action after the engulfing pattern.\nExample Charts ","description":"An engulfing chart pattern is a technical pattern that indicates lower prices to come. It consists of a high (green) candle followed by a large down (red) candle that engulfs the smaller up candle. The pattern is necessary because it signals that sellers have overtaken the buyers.","tags":["crypto"],"title":"Candle Stick Pattern- The Engulfing Pattern","uri":"/collections/crypto/the-engulfing-pattern/"},{"content":"Tutorial- Downloading YouTube videos using Python Requirements Python (Obvious) pytube Installing pip install pytube Downloading Youtube video After installing pytube, return to the text editor, open the Python file and import pytube:\nfrom pytube import YouTube Copy the URL of the youtube video you want to downalod. Create instance of it in the next line of python code:\nurl = 'https://www.youtube.com/watch?v=example' video = YouTube(URL) The pytube module works by giving you different streaming options. However, the video has a different stream resolution. So pytube allows you to download videos based on these.\nFor printing different stream from YouTube object write down following code:\ndef print_stream(video): print(video.streams) print_stream(video) OUTPUT:\n[\u003cStream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"25fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\"\u003e, \u003cStream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"25fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\"\u003e, \u003cStream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"25fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"25fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"25fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"25fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"25fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"25fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\"\u003e, \u003cStream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e, \u003cStream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e, \u003cStream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e] From ouput of different stream choose the extension you want, then write down the following code to get the strem of that extension.\next_video = video.streams.filter(file_extension='your_extension') print(ext_video) OUTPUT:\n[\u003cStream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"25fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\"\u003e, \u003cStream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e, \u003cStream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e, \u003cStream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\"\u003e] However, the module returns different streaming resolutions, starting with 360p at 720p and 1080p (possibly more). But when you look closely, every resolution has an itag value.\nFor example, itag = “22” for res = “720”, and itag for 360p resolution is 18.\nYou can use this itag value to call the stream, including the get_by_itag () function:\ndef get_video_by_itag(ext_video,itag_num): return ext_video.get_by_itag(itag_num) video_stream = get_video_by_itag(ext_video,your_tab_num_here) Now, all you need is to downlaod video using downlaod method:\ndef downlaod_stream(stream,file_name,path): stream.downlaod(filename = file_name,output_path=path) downlaod_stream(video_stream,\"First Youtube video\",\"./\") Here the output_path is your prefferd downlaod directory.\nAltogether your code will look something like this:\n#youtube video downaloder code from pytube import YouTube def print_stream(video): print(video.streams) def get_video_by_itag(ext_video,itag_num): return ext_video.get_by_itag(itag_num) def download_stream(stream,file_name,path): stream.download(filename = file_name,output_path=path) if __name__ == '__main__': url = 'https://www.youtube.com/watch?v=example' video = YouTube(URL) print_stream(video) ext_video = video.streams.filter(file_extension='your_extension') print(ext_video) video_stream = get_video_by_itag(ext_video,your_tab_num_here) download_stream(video_stream,\"First Youtube video\",\"./\") ","description":"Downloading youtube video using python. Here we will be using pytube module for downaloding youtube videos.pytube is a lightweight, Pythonic, dependency-free, library for downloading YouTube Videos.","tags":["programming","python"],"title":"Downloading YouTube videos using Python","uri":"/collections/programming/python/downloading-youtube-video-using-python/"},{"content":"Hanging Man Candlestick Pattern for Stock/Crypto Trading What is Hanging Man Candlestick Pattern? Hanging Man is a bearish reversal candlestick pattern with a longer lower shadow and smaller real body. This candlestick pattern appears at the end of an uptrend, indicating further weakness in price movements.\nHanging Man is formed when the bulls raised prices and are now unable to apply further pressure. There is no upper shadow and the lower shadow is twice the length of its body. Technical analysis on hanging man model gives traders the opportunity to close a buy position and enter a short position\nFormation of Hanging Man Pattern When the opening price, the highest price, and the closing price are approximately the same, a hanging man form will be formed, just like a hammer. In addition, there is a very long lower shadow, which should be at least twice the actual length of the body.\nWhat does Red and Green Hanging Man tell us? When the high and open prices are equal, a bearish red hanging candle forms. This pattern is considered a stronger bearish signal than when the high and close prices are the same, forming a green hanging man.\nAlthough the green candlestick is still bearish, it is considered less bearish due to the close of the day.\nHow to identify Hanging Man pattern in candlestick chart: Here are some points to consider when identifying Hanging Man pattern on a candlestick chart:\nThere is little or no upper shadow. The length of the lower shadow must be twice the length of the entity. The entity must be on the upper side of the candle. Strategy for trading on Candle Stick Hanging Man Pattern For some traders, the confirmation candle the next day, coupled with the fact that the uptrend line support was broken, gave a potential signal to go short.\nIt is important to reiterate that technical analysis on hanging man patterns are not a sign of potential shorting; Other indicators should be used to determine to sell signals.\nFormation Examples ","description":"Hanging Man is a bearish reversal candlestick pattern .This candlestick pattern appears at the end of an uptrend, indicating further weakness in price movements.","tags":["crypto"],"title":"Hanging Man Candlestick Pattern","uri":"/collections/crypto/hanging-man-pattern/"},{"content":"Best Linux distros of 2021 for beginners, mainstream and advanced users Looking for a new Linux experience? Get to know the best Linux distribution for all user levels, from beginners to experts.\nAs a free and open-source operating system, Linux continues to spread its wings, attracts the attention of novices and experienced people.\nCheck out some of these desks and set up a desk that suits your interests and abilities.\n1. Ubuntu Using one of the most popular OS platforms, you can’t go wrong. It is very suitable for users who want to get involved in Linux every day while learning the basics of .\nTechnically speaking, Ubuntu is derived from Debian and comes with Long Term Team Support (LTS) releases. You can install a stable version of the operating system on the desktop without unnecessary obstacles.\nIt comes with the GNOME desktop environment by default and is pre-installed with ready-to-use applications such as Firefox, LibreOffice, music players, and video players (such as Rhythmbox and Audacious) to use.\nDownload Ubuntu\n2. Linux Mint Looking for a lightweight Linux distribution? If so, dial Linux Mint. It has the essence of Debian and Ubuntu, providing a user-friendly -person experience for both novice and advanced users.\nSince it is community-driven, you can let do whatever you want. Although it is based on Ubuntu, Mint is not equipped with a GNOME desktop, but with its own native environments, such as Xfce, Cinnamon, and MATE.\nIt is only available in 64-bit, for example. After installation, you can enjoy the look and feel of , rich polished icons, new themes, modified taskbar and high-resolution background images.\nDownload Linux Mint\n3. Elementory OS If you are a big fan of Mac, then you will love Elementary OS. This operating system replicates the look and feel of Mac and has proven to be the ideal platform for designers and creatives. It’s modern, stylish, and intuitive desktop design is just that.\nThe initial setup and design are quite lightweight, and the focus on productivity and privacy. In the latest version, Elementary OS provides with multitasking view, do not disturb mode, and picture-in-picture mode to ensure has the best productivity experience.\nLike Ubuntu, it is also based on GNOME and equipped with the Pantheon desktop environment.\nDownload Elementory OS\n4. Solus Although it is a general-purpose Linux operating system, Solus provides developers with an ideal desktop environment. It supports various advanced editors and IDE(Integrated development environment).\nDevelopers can manage code in control systems such as Git, GitKraken, Bazaar, and Git-Cola. In addition, Solus supports different programming languages, including Go, Rust, PHP, Node.js, and Ruby.\nFinally, you can download various developer tools from their built-in repositories to enhance your long-term experience.\nDownload Solus\n5. OpenSUSE OpenSUSE provides open-source tools available for developers and system administrators. This is a community-driven environment, which means that provides OpenSUSE users with what they want.\nWhat really makes this platform great is its robustness and the ability to install directly on other Linux desktop environments (KDE, MATE, GNOME, Cinnamon, etc.).\nCurrently, many other open-source natives versions generally lack these features.\nDownload OpenSUSE\n6. FEDORA. Fedora is often mentioned as one of the most popular Linux distributions and is also for good reason. Like OpenSUSE, is also an open-source operating system, free to use, reliable, and easy to use.\nFedora offers three different versions, including workstations, servers, and the Internet of Things. Advanced users can choose to customize the operating system according to their needs.\nHowever, unlike some other distributions, only a limited number of pre-installed applications are available in this release.\n7. Debian Debian is one of the most stable and recognized operating systems provided by Linux. It forms the basis of other environments, such as Ubuntu, PureOS, SteamOS, Knoppix, Tails and so on.\nThis release is known for its simple and smooth updates because provides its updates within the set release cycle. Users can use the Live CD to install this operating system, which includes the easy-to-use Calamares installer.\nThe installer mentioned above is very suitable for beginners; after that, 4,444 advanced users can use the full-featured installer on their systems.\nDowload Debian\n","description":"Looking for a new Linux experience? Get to know the best Linuxdistribution for all user levels, from beginners to experts.As a free and open-source operating system, Linux continues to spread its wings,attracts the attention of novices and experienced people.","tags":["review"],"title":"Best Linux distros of 2021 for beginners, mainstream and advanced users","uri":"/collections/reviews/top-linux-distros-for-beginners/"},{"content":"Hammer Candlestick pattern What is a hammer candlestick? Traders in the financial market(crypto/forex/stock) often use candlestick charts as a good visual aid to analyze and monitor the performance of a specific price in a given time period. They have the most flexibility to understand the trend of the market. The model can help traders measure the market sentiment of financial assets. For example, the hammer candle is a bullish pattern, which is formed when the asset price falls from its opening price and is close to the support level, and only rebounds at and closes at a high level.\nWhen it comes to bullish candles, a popular pattern is the hammer candle formation. The hammer is one of the most important reversal patterns that traders should pay attention to. The hammer is considered a bullish reversal, but only when it appears under certain conditions. The pattern generally forms near the bottom of a downtrend, indicating that the market is trying to define the lower.\nThe hammer candle is at the bottom of the downtrend and indicates a possible (bullish) reversal in the market. The hammer line is the candlestick pattern. When the stock opens, it drops sharply during the day and then rebounds to near the opening price. The candle pattern looks like a hammer. The long lower wick at the low point of the day looks like . The handle and the main body of the opening and closing prices form the head of that looks like a hammer. The lower part is usually twice the size of the candle body, but it can be larger. For a clear understanding of the Hammer candle and its appearance, please refer to the table below.\nWhen the high and the closing price are equal, a bullish hammer candle forms. It is considered a stronger pattern because the bulls can completely reject the shorts and the bulls can push the price higher before the opening price.\nThe lower shadow of the hammer suggests that the market tested for support and demand. When the market found the support area of the day low, the bulls started to push the price higher and were close to the opening price. Therefore, the bearish push to the downside was rejected by the bulls.\nInverted Hammer Inverted Hammer candlesticks mainly occur at the bottom of the downtrend, which can be used as a warning of possible reversal upward. It should be noted that the reversal pattern is a warning of ’s potential price changes, and is not a signal to buy in itself. The inverted hammer, like the shooting star, is created when the opening, low, and closing prices are approximately the same. In addition, there is a very long top shadow, which should be at least twice the length of the actual body. When the low and the opening price are equal, the inverted hammer bullish candle is formed, which is considered a stronger bullish signal. After a long downtrend, the formation of an inverted hammer is bullish, because the price rises sharply during the day and is hesitant to move down. Despite this, sellers re-enter stocks, futures, or the currency and push the price back near the open, but the fact that the price may rise significantly indicates that the bulls are testing the price. power of the bears. What happened the day after the hammer pattern reversal allowed traders to know if the price would rise or fall.\nLimitation There is no guarantee that the price will continue to rise after the candle is confirmed. The long shadow hammer and the powerful confirmation candle can push the price higher in two periods. This may not be an ideal buy point, because the stop loss may be far from the entry point, putting traders at risk, and does not justify the potential return.\nFormation Chart Example ","description":"The hammer Candlestick Pattern is one of the most important reversal patterns that traders should pay attention to. The hammer is considered a bullish reversal from , but only when it appears under certain conditions. The pattern generally forms near the bottom of a downtrend, indicating that the market is trying to define the lower","tags":["crypto"],"title":"Hammer Candlestick Pattern","uri":"/collections/crypto/hammer-signal/"},{"content":"Winget Package Manger for Windows If you have ever used any time today, then you will be familiar with the idea of the package manager. When I use it to switch to , I might miss that package manager very much.\nIt’s been on Windows 10 for some time, like the excellent third-party solution Chocolatey. But now, Microsoft has its own Windows Package Manager called.\nAfter being previewed for a whole year, it recently reached v1.0. It has not yet come out with Windows, but it is ready and does not require batches to install it on your computer. This is what you need to know.\nWhat is the Windows Package Manager? Windows Package Manager is a command-line tool used to manage software, which can be used in Windows 10 via command prompt or powershell. The implementation is very similar to the Linux package manager.\nWindows Package Manager itself does not host any packages. Instead, users create lists, add these lists to the central repository, and then formulate these lists to get software from the common home page on the web.\nIt could be Github, a software developer’s website, or even a Microsoft Store. One of the benefits of the Windows Package Manager is how easy it is to create a manifest to install the software.\nOf course, this is more than just installing things on the PC. During the process of the preview period, the feature list increased significantly. With the reaching v1.0. Using it to manage the software on your own PC or multiple remote computers is a feasible proposal if you work in the company.\nInstalling Windows Package Manager If you were a Windows Package Manager user before the preview stage, you do not need to perform any special actions to get v1.0. The is still shipped the same way, so suppose you have downloaded the update to the app installer on the Microsoft Store, or you are using the Windows 10 insider build, then you should be happy. You can verify this by typing winget --info in the terminal.\nFor beginners, there is now a more simplified way to install Windows Package Manager. There is a direct link in the winget v1.0, but you can also go directly and get it from there. In any case, Github is worth, because there is a lot of useful information there.\nGet the latest version from the release page by downloading the .appxbundle file. After downloading, just open it like any Windows executable file, and the built-in “application installer” of Windows 10 will do the rest.\nFinding software/application using Windows Package Manager One of the most basic functions of Windows Package Manager and number is that you want to install it to install an application on your Windows 10 PC. But Windows Package Manager can also help you find the application you are looking for.\nThe repository is currently on Github, but it has to be crawled in a huge list, which is hardly an easy-to-use experience. On the contrary, there are two important commands to remember:\nwinget install package_name winget search package_name All Windows package manager commands are called using the term winget . So, for example, if you want to search for Microsoft PowerShell , you can enter the following command:\nwinget search powertoys You will then see a table showing packages that match your search terms. It will contain the specific ID you need to download. You don’t always need it, but use something like PowerShell, there are multiple versions available, you will need it. To download, you should enter:\nwinget Install Microsoft.Powershell Or, if you prefer something with an attractive user interface, has an excellent third-party tool that you should check. Extract’s entire Windows Package Manager repository, but make look easier. An additional benefit is that it can generate the installation scripts required by multiple applications simultaneously. only needs to be simply copied and pasted.\nUninstalling software/application using Windows Package Manager The uninstall feature is one of the features added later during the Windows package manager preview and must be manually enabled in the config JSON file. As of v1.0, this is no longer the case, and the functions have been fully integrated into it.\nTo uninstall the application using the Windows Package Manager, the command template is as follows:\nwinget uninstall package_name This is all you have to do. The feature currently appears to be limited to packages installed using the Windows Package Manager.\n","description":"Windows Package Manager is a command-line tool used to manage software,which can be used in Windows 10 via command prompt or. The implementation is very similar to the Linux package manager.","tags":["softwares"],"title":"winget: Package manager for windows","uri":"/collections/softwares/windows/windows-package-mangaer-winget/"},{"content":"Everything you need to know about Cryptocurrencies In recent years, the popularity of cryptocurrencies such as Bitcoin, Dogecoin, and Ethereum has increased. Many new terms and concepts have been introduced to the public. These terms and concepts can be unimaginable and difficult to understand. A study shows that 33% of Americans have not seen, read, or heard anything about Bitcoin. 44% said they had seen it and had read or heard of “just a few.” However, conversations about cryptocurrency have become increasingly common, especially during attacks, when attackers demand payment in cryptocurrency, increasing the awareness of victims, companies, and municipalities.\nWhat is a cryptocurrency? In its simplest form, cryptocurrency is computer code generated by publicly available software, which allows people to store and send values online. The open-source code originated from Bitcoin years ago and runs on private computer networks worldwide.\nThis code verifies the transactions and groups them into a public record called the blockchain. This is a large file containing transactions and the first download can take several days.\nThe value of cryptocurrencies is generally expressed in US dollars and is set for public transactions carried out by exchanges. This value can vary greatly; Today, the cost of a single Bitcoin is roughly $ 36,900, which is a decrease of $ 4,444 from almost $ 60,000 in May.\nHow are cryptocurrencies made? Think of cryptocurrencies as digital gold. “We think the value of gold is because other people also think it has value, and there are few available at,” said David Sacco, an intern in finance and economics at the University of New Haven Department. The same idea governs the value of cryptocurrency. If more people invest in cryptocurrency because they believe that others see its value, the price of the cryptocurrency will increase and vice versa. But this also means that the number of available cryptocurrencies must be strictly controlled to maintain their value.\nThe algorithm for generating cryptocurrencies can be downloaded from the developer’s website. In theory, anyone can use it to create a new cryptocurrency. However, the competition in this process is very fierce because the actual number of cryptocurrencies to be put into circulation is limited. These limits vary by cryptocurrency and are set by the person who created the code. For example, the algorithm Bitcoin limits the number of Bitcoins that can be generated to 21 million. At that point, it will no longer be manufactured.\nCreating a new currency requires huge computing power to solve complex mathematical equations that generate a cryptocurrency unit. According to an analysis by the University of Cambridge, globally, this process consumed more electricity than the Netherlands in.\nAccording to the 4,444 Bitcoin developers created by the famous Bitcoin developer Luke Dashjr, there may be approximately 70,000 Bitcoin “mines” in operation today. Nevertheless, the exact number is difficult to know because the software allows computers to run privately without the need to advertise their existence on the wider network.\nAt the very least, to run a Bitcoin mining farm (also known as a full node) requires a strong internet connection, ’s massive download capacity, and 350 GB of available storage space, which can be found on most new laptops. These nodes also require at least 512 megabytes of random access memory, which is much lower than ordinary laptop computers. There are many developers behind. The software distributes new bitcoins based on the speed at which the miners’ 4,444 computers add transactions to the blockchain. So unless you’re one of the fastest people, you might not believe many. Today, the system allows the creation of 6.25 bitcoins every 10 minutes, and the code halves the number every four years.\nSome companies and entrepreneurs operate a large number of cryptocurrency miners, and the chance of obtaining a higher percentage of new coins into circulation has increased. Riot Blockchain is a US-listed company that is believed to operate one of the largest companies in the world. The company’s 190,000-square-foot facility is located in Rockdale, Texas. The city has a population of approximately 5,800 and has access to 4,444 cheap electricity. Cryptocurrency investment is welcome.\nHow many cryptocurrencies are there? There are thousands of different types of cryptocurrencies available for purchase and transactions, with more being created. But they are not the same.\nSome, such as Bitcoin, have a long history and higher brand awareness. Others, such as Dogecoin, are the result of Internet hype. These are usually controlled by computers running free and open-source code.\nAt this year’s peak, Bitcoin accounted for 70% of ’s share, but due to China’s new regulatory barriers, this share has dropped to about 40%.\nMaintain an up-to-date list of cryptocurrencies added through the authenticity verification process.\nWho created the cryptocurrency? Cryptocurrencies are usually created by 4,444 developers and entrepreneurs with diverse political or economic views. Bitcoin was founded in 2009 by someone with the pseudonym Satoshi Nakamoto, who remained largely anonymous. Ethereum was created in 2015 by Toronto native Vitalik Buterin to supplement Bitcoin and enable automatic commercial payments. Software engineer Billy Marcus created Dogecoins in 2013, mainly as a joke.\nWhere is the cryptocurrency stored? Cryptocurrency is not technically stored anywhere. It is not saved in a folder or on a hard disk. Evidence of how much cryptocurrency has is stored on the blockchain.\nEvery new transaction updates the ledger across the entire network: when new bitcoins are mined and when someone moves their cryptocurrency.\nTo access your cryptocurrency, you need a private key or a complex password that was generated by the code when you created your wallet. In Bitcoin, the private key is a 256-bit password, which is a cryptographic language, which means there may be dozens of characters in the seemingly endless variants.\nThe private key creates a unique signature, allowing you to use your cryptocurrency for transactions. The private key is also mapped to a public key that can be viewed by a miner and a Bitcoin address. You can think of it as similar to a public bank account. The address is a unique string of letters and numbers, a total of 26 to 35 characters, is case sensitive and shows where the encrypted currency is sent on the blockchain.\nPrivate keys can be stored in special virtual wallets, which are applications provided by cryptocurrency exchanges. When you register to buy cryptocurrency, you will get a wallet. Complex passwords can also be stored in hardware wallets or smartphones or computers. You can also print a copy of the storage key in a safe place.\nThe difference between an encrypted wallet and a smartphone wallet is that may store your credit and debit card information. They are usually encrypted. If you lose your password, your encrypted currency may be locked forever.\nHow is cryptocurrency transferred between people and companies? Traditional payment systems rely on banks to verify transactions, while 4,444 cryptocurrency transactions are verified by miners on the blockchain. Miners run math checks to make sure the transaction is valid, and most nodes must agree that is a valid transaction before adding it to the blockchain.\nMost people trust cryptocurrency exchange services like Coinbase, or they buy and sell cryptocurrencies. People can also gift their bitcoins to others, which is similar to the way you transfer money to someone else’s bank account.\nAs more and more companies adopt cryptocurrency, people can use it to do more things. Some companies like AT\u0026T now accept cryptocurrency as an offer. Now you can use cryptocurrency to buy travel tickets.\nWhat government regulations exist? Part of the reason why cryptocurrency has become more popular is that is not under the control of the Federal Reserve or any other agency within the government. However, it is required to pay taxes set by the IRS in 2014. Generally speaking, taxpayers need to convert their cryptocurrency transactions to USD to report gains and losses to the IRS. Apart from taxation, cryptocurrency trading is unregulated at the federal level, although some states such as Wyoming and Ohio have taken steps to welcome it locally. Wyoming enacted Law No. , which is a “Utilities Token Act” that makes the operation of blockchain businesses easier, while Ohio allows companies to use cryptocurrency to pay various taxes.\nAre cryptocurrency transactions secret? No, they are registered. What is the secret, or at least it is difficult to know, is that received and sent a transaction, because the transaction listed on the blockchain does not have a name attached to it. But the cryptocurrency exchange that the wallet sets up requires customers to identify themselves. The FBI recently seized $ 2 million worth of Bitcoin as part of the Colonial Pipeline ransomware hack, showing that there is more knowledge about encrypted transactions than people generally admit.\nSince cryptocurrencies are exchanged in public documents, you can see when and where the funds are transferred. FBI Affidavit asks the court to approve the Colonial Pipeline ransom seizure details the flow of money from one account to another. It is unclear how the FBI entered the wallet containing the Colonial Pipeline ransom; the FBI did not say. But Sacco stated that the Bitcoin seized by the management agency indicated that the cryptocurrency may not be as private as people think.\n","description":"In this blog we will understand what is cryptocurrencies,how are they made and where cryptocurrencies are used.","tags":["crypto"],"title":"Every thing you need to know about cryptocurrencies","uri":"/collections/crypto/every-thing-to-know-about-cryptocurrency/"},{"content":"React 18 is Out! react core team recently released an alpha version of react18. this version pays more attention to user experience and internal architecture changes, including adaptive concurrency functions,it gives you more control over dom rendering events.\nInstallation npm install react@alpha npm install react-dom@alpha What’s new in React 18 1. New Root API You may be accustomed to seeing something like this at the top level of the application:\nimport React from \"react\"; import ReactDOM from \"react-dom\"; const container = document.getElementById(\"root\"); ReactDOM.render(\u003cApp /\u003e, container); Isn’t it normal? correct. This ReactDOM.render() is now called Legacy Root API. It works in exactly the same way as React 17. You can still keep it, but it will eventually be deprecated.\nThe new root API looks a bit different:\nimport React from \"react\"; import ReactDOM from \"react-dom\"; import App from \"App\"; const somecontainer = document.getEleementById(\"root\"); const root = ReactDOM.createRoot(somecontainer); root.render(\u003cApp /\u003e); It is very similar! Use ReactDOM.createRoot instead of the legacy method.\nWith this change, a few things happened:-\nThe hydrate method is gone, now it is an option in createRoot-The render callback is gone (now anything can be passed to \u003cApp/\u003e)\nIf you don’t use these two functions, you don’t have to worry about their changes. If you’d like to learn more about them, here are some sample code changes from the React core team.\nBy switching to the new root API, you will automatically get the new out-of-the-box enhancements that React 18 offers.\nSuspense Suspense updates are all to improve server-side rendering. One of the main problems we face in the server-side rendering process is that not all data is sent to the user at once. Send HTML first, then CSS, while JS is still rendered on the server. This is also called hydration time. Buttons are great, but until event handlers are attached to them, they are just visual effects.\nReact 18 solves this problem!\nFor those elements that rely heavily on JS, we can send alternative components for a period of time instead of sending HTML and CSS. Once the component is ready for shipment, the main component will automatically change it.\nFor using Suspense all you have to do is warp your components in an \u003cSuspense\u003e component:\n\u003cSuspense fallback={\u003cLoading /\u003e}\u003e \u003cComponent /\u003e \u003c/Suspense\u003e In the above snippet, React will show \u003cLoading/\u003e component at first and then replace it with \u003cComponent\u003e when the data gets resolved.\nTransition API This is a new API introduced in this version, which helps to keep the current web page responsive and allows a large number of non-blocking UI updates at the same time.\nAn important use case for startTransition can be when the user starts typing in the search box. The input value should be updated immediately and the search results can wait a few milliseconds (as expected by the user).\nThis API provides a way to distinguish between fast updates and delayed updates. Delayed update (that is, transition from one user interface view to another user interface view) is called transition update.\nFor urgent updates such as typing, hovering, clicking, etc., we usually call props/functions like this:\nsetText(input); For non-urgent or heavy UI updates, we can wrap it in a startTransition API as :\nstartTransition(() =\u003e { setText(input); }); Server Side Rendering Imporvements The server-side rendering has undergone an architectural overhaul in this release, including improvements to the first loading screen time. In the normal version (up to React 17), SSR must load the entire page to start reloading the page.\nThis has changed !\nThis is now called selective hydration. Assuming we have 5-8 different components on the screen, once the code loads and does not block the rest of the page, packaging a component will now start to hydrate a very specific component. By adopting this strategy, the most important part/component of the page can first become interactive (under extremely slow connections), while other components continue to remain hydrated to provide a good user experience.\nStrict Effects coming to Strict Mode React 18 will now be released with strict effects mode. Like strict mode, it will be used for developing builds and improving DX.\nWhen components are included in Strict Effects, React will make sure the side effects are run twice “on purpose” to detect abnormal behaviors / patterns, which is often a problem when using the useEffect mount and cleanup feature.\nTo know more about feature and improvement in React 18 here\n","description":"react core team recently released an alpha version of react18. This version pays more attention to user experience and internal architecture changes, including adaptive concurrency functions. It gives you more control over dom rendering events.","tags":["programming","react"],"title":"React 18 is out !","uri":"/collections/programming/react/react-18-alpha/"},{"content":"React Tutorial - Creating weather App using Material UI In this article we will be building a weather app using react js material UI framework. We will also be using openweathermap API for fetching the weather data from the site.\nThe idea is simple we are going to create a Card component which will consists of a TextField where the user can write the name of the city.\nAfter getting the name of the city from input field we will then fetch this to our openweathermap API. If every thing goes right this API will give respone of weatherReport in json format or else it will give error.\nThen we will only take the values which we will be needing for our App from json respone. And finally we are going to diplay that value.\nNow without further due let’s build our App.\nImplementation We will build three components :\nApp.js : This component will take input from the user and pass that value to weatherAPI components as prop. weatherAPI.js : This component will take the value from App.js and fetch it to openweathermap API. After getting respone the json object is passed throung our Display components. weatherReportDisplay : This component simply dislay the weather information. Now let’s start by writing our first component:\nApp.js\nimport React from \"react\"; import Grid from \"@material-ui/core/Grid\"; import Card from \"@material-ui/core/Card\"; import { makeStyles } from \"@material-ui/core/styles\"; import CardContent from \"@material-ui/core/CardContent\"; import TextField from \"@material-ui/core/TextField\"; import WeatherAPI from \"./weatherAPI\"; import bgImg from \"./images/bg-img.jpg\"; const style = makeStyles((theme) =\u003e ({ root: { marginTop: 50, display: \"flex\", width: 550, height: 250, }, cardcss: { backgroundImage: \"url(\" + bgImg + \")\", backgroundPosition: \"center\", }, })); function App() { const classes = style(); const [city, setCity] = React.useState(null); return ( \u003cGrid className={classes.root} alignItems=\"center\" container justify\u003e \u003cCard className={classes.cardcss}\u003e \u003cCardContent\u003e \u003cTextField autoFocus label=\"City Name\" onChange={(e) =\u003e { setCity(e.target.value); }} /\u003e \u003cWeatherAPI city={city} /\u003e \u003c/CardContent\u003e \u003c/Card\u003e \u003c/Grid\u003e ); } export default App; This statless is simple. We are creating a state name city using new feature of react js called react hooks. Whenever user writes the value in input field the state of the function get’s updated and passed it to weatherAPI component.\nweatherAPI.js\nimport React from 'react'; import LinearProgress from '@material-ui/core/LinearProgress'; import Display from './weatherReportDisplay'; const API_KEY = /* Your API key here */ const UNITS = \"Metric\" const LANG = \"en\" class WeatherAPI extends React.Component { constructor(props) { super(props); this.state = { weatherReport : null, isLoading : true, error : null } } componentDidUpdate() { var URL = \"http://api.openweathermap.org/data/2.5/weather?q=\" + this.props.city + \"\u0026lang=\" + LANG + \"\u0026appid=\" + API_KEY + \"\u0026units=\"+ UNITS fetch(URL).then(response =\u003e{ if(response.ok) {return response.json() } else { throw new Error(\"SOMETHING WENT WRONG\")}}) .then(data =\u003e this.setState( { weatherReport : data, isLoading: false })) .catch(error =\u003e this.setState( {error, isLoading : true })); } render() { if(this.state.isLoading) { if(this.props.city != null) { return ( \u003cdiv\u003e \u003cLinearProgress /\u003e \u003c/div\u003e ) } else return null; } else { return( \u003cDisplay weatherReport = {this.state.weatherReport}/\u003e ) } } } export default WeatherAPI; For simplicity of our app here we have used statefull component. Here we have used componentDidUpdate method which will continusly update the state value of this component whenever input field changes. The state of this component hold 3 value:\nweatherReport Which will hold our json response. isLoading Used for loading bar. error If server respone with any error. In componentDidUpdate method we are using javascript promise object.Which will contain some value either solved or unsolved when fetching data from website.The .json() method of promise object converts it into json format.\nIn the render method we first check that weather we have got response or not by checking isLoading state.Which only set to false whenever we get a response from server.\nIf everythings went fine weatherReport state is passed to Display component.\nweatherReportDisplay.js\nimport React from \"react\"; import Typography from \"@material-ui/core/Typography\"; import CardContent from \"@material-ui/core/CardContent\"; import Box from \"@material-ui/core/Box\"; function Display({ weatherReport }) { var lon = weatherReport.coord.lon; var lat = weatherReport.coord.lat; var weathermain = weatherReport.weather[0].main; var weatherdiscription = weatherReport.weather[0].description; var temp = weatherReport.main.temp; var pressure = weatherReport.main.pressure; var humidity = weatherReport.main.humidity; var wind = weatherReport.wind.speed; var country = weatherReport.sys.country; var city = weatherReport.name; return ( \u003cdiv\u003e \u003cCardContent\u003e \u003cBox display=\"flex\" flexDirection=\"row\"\u003e \u003cBox p={1}\u003e \u003cTypography variant=\"h2\" color=\"textPrimary\"\u003e {city},{country} \u003c/Typography\u003e \u003cTypography variant=\"caption\" color=\"textSecondary\"\u003e {lon}, {lat} \u003c/Typography\u003e \u003c/Box\u003e \u003c/Box\u003e \u003c/CardContent\u003e \u003cCardContent\u003e \u003cBox display=\"flex\" flexDirection=\"row-reverse\"\u003e \u003cBox p={0}\u003e \u003cTypography variant=\"h4\" color=\"textPrimary\"\u003e Temp: {temp} \u003cspan\u003e\u0026#176;\u003c/span\u003e {\"C\"} \u003c/Typography\u003e \u003c/Box\u003e \u003c/Box\u003e \u003c/CardContent\u003e \u003cCardContent\u003e \u003cBox display=\"flex\" flexDirection=\"row-reverse\"\u003e \u003cBox p={0}\u003e \u003cTypography variant=\"h6\" color=\"textSecondary\"\u003e {weatherdiscription} \u003c/Typography\u003e \u003c/Box\u003e \u003c/Box\u003e \u003c/CardContent\u003e \u003cCardContent\u003e \u003cBox display=\"flex\" flexDirection=\"row\"\u003e \u003cBox p={1}\u003e \u003cTypography variant=\"h6\" color=\"textPrimary\"\u003e Humidity: {humidity} % \u003c/Typography\u003e \u003c/Box\u003e \u003cBox p={1}\u003e \u003cTypography variant=\"h6\" color=\"textPrimary\"\u003e pressure: {pressure} pa \u003c/Typography\u003e \u003c/Box\u003e \u003cBox p={1}\u003e \u003cTypography variant=\"h6\" color=\"textPrimary\"\u003e wind: {wind} km/h \u003c/Typography\u003e \u003c/Box\u003e \u003c/Box\u003e \u003c/CardContent\u003e \u003c/div\u003e ); } export default Display; This component displays weather information provided by weatherAPI component. Here from the json object we have chosen certain value for displaying such as temprature,humidity,pressure,etc. The Box component provided by material UI is used for manging layout.\nThat’s it now try to implement the above app yourself.After successfull implemnentation try adding more feature such as weather icon or litter bit of animation.\n","description":"We'll make a weather app with the react js material UI framework in this article.","tags":["programming","react"],"title":"React Tutorial - Creating a simple weather app using React and Material UI","uri":"/collections/programming/react/weather-app-using-react/"},{"content":"5 Crypto you should watch in 2021 The cryptocurrency has exploded in popularity and widespread awareness. Part of the reason is that the value of some of these tokens has skyrocketed. A typical example is the rapid growth of Bitcoin over the years. Bitcoin’s price when it debuted in 2010 was $ 0.0008 and skyrocketed to a recent high of over $ 60,000.\nOf course, Bitcoin is not the only cryptocurrency on the market. There are thousands of similar digital currencies available, and more digital currencies are being created every day, and they may become the next large-scale cryptocurrency.\n1. Internet Computer (ICP) Internet Computer (ICP) is a new token launched in May 2021. It was an immediate success, with a market value of 35 billion U.S. dollars in the first three days. The token’s current market value is 8.7 billion U.S. dollars, and the current transaction price is about 70.55 U.S. dollars. The goal of the project is to overcome the current limitations of the Internet and replace it with the modern Internet. You can buy this token on Binance.\n2. Enjin (ENJ) Enjin is a recently launched cryptocurrency based on Ethereum, designed to provide support for irreplaceable tokens. One use case is that developers use their blockchain technology to manage in-game purchases. The market value of the token is US $ 1 billion and the current transaction price is approximately US $ 1.37. You can buy Enjin on eight different markets, such as Binance.US and Crypto.com.\n3. SafeMoon SafeMoon is an altcoin just launched in March 2021. The coin is a community-driven DeFi token and is launched fairly. In just a few months, there have been more than 2 million headlines. The token’s market value is 2.2 billion U.S. dollars, and the current transaction price is about 0.000003715 U.S. dollars. An unusual feature of tokens is that they punish users who sell their tokens. 10 e is applied to the seller, half of which is reallocated to other holders. You can buy SafeMoon in nine different markets.\n4. NKN NKN is a new blockchain-based token designed for the decentralized internet. Supports millions of nodes, 100 centralized point-to-point operations, and low latency. The market value of NKN is US $ 212 million and the current transaction price is approximately US $ 0.2933. You can buy NKN on Binance.\n5. Diem Although not currently for sale, Facebook’s Diem is one of the most anticipated coins in 2021. Diem, formerly known as Libra, is a centralized, global, and stable cryptocurrency project. This may be a good choice for investors who want a more stable cryptocurrency. Due to its reserves of other currencies and assets, investors expect Diem to be less volatile.\n","description":"The cryptocurrency has exploded in popularity and widespread awareness. Part of the reason is that the value of some of these tokens has skyrocketed. A typical example is the rapid growth of Bitcoin over the years. Bitcoin's price when it debuted in 2010 was $ 0.0008 and skyrocketed to a recent high of over $ 60,000.","tags":["crypto"],"title":"5 Crypto currencies you may want to invest in","uri":"/collections/crypto/crypto-currency-to-invest-in-2021/"},{"content":"Best Android VPN browsers for privacy Use these Android web browsers that come with a free built-in VPN to enhance your privacy and security.\nA virtual private network (VPN for short) encrypts your internet activity and hides your location. Since most people use their portable devices (such as smartphones) to go online, they must have a VPN.\nThis is a list of browsers with built-in VPN on Android, so you don’t need to download additional apps.\nWhy does your Android smartphone need a VPN?\nThere are many reasons why you need a VPN. But primarily revolves around privacy and security. This ensures that the data you are transferring is protected. This layer of protection is essential, especially when using public Wi-Fi.\nAnother thing that makes a VPN service a must-have is that it protects your online activities from your Internet Service Provider (ISP) and anyone trying to track your activities through the connected network.\nVPN also hides your physical location by providing you with a different virtual IP address, thus hiding your identity. Finally, a VPN allows you to access content or websites that you cannot access at your location. The latter is essential when you still want to access the video streaming service when traveling to an unsupported country.\nNo need to download additional apps to route your traffic through the secure tunnel, a browser with a built-in VPN can save you some space. There are different VPN services available. Most are free, and some offer subscription options. The free VPN service is good, but it can’t compare with the paid solution.\n1. Epic Privacy Browser The Epic browser also provides a built-in VPN, but you must download it as an extension. VPN has unlimited bandwidth, you can choose one of the available proxy servers located in different countries/regions, including the United States, United Kingdom, Canada, France, Germany, Singapore, Netherlands, and India.\nEpic’s proxy includes a no-logging strategy, which has always been a key feature of VPN. You can download the extension from the Epic Extensions store by clicking on the mobile extension on the browser homepage.\nAlthough there are a few, they all achieve the same goal of protecting your IP address to protect your privacy. Epic does not limit to, although you will miss the robustness of the VPN service.\n2. Tenta Private VPN browser Tenta is a mobile browser that focuses on security and privacy, has a built-in VPN, which you can use for free but there are certain restrictions. Tenta offers a paid version of its VPN service, which can unlock all your VPN locations, and you can use it on the entire device, not just in the browser without additional downloads.\nThere are various VPN servers available in Japan, the United States, the United Kingdom, Spain, Germany, South Korea, New Zealand, India, Brazil, and other places. The free version will lock it on one of the servers, but the good news is that has no bandwidth limitations.\nTo activate VPN, touch the Tenta browser icon at the bottom and press VPN to browse. In addition to the VPN service, Tenta also provides other features to consolidate its privacy methods, including the ability to lock the browser with a PIN code, avoid browser screenshots, do not track support, and DNS customization options. You can also get a feature that allows you to delete browsing data when you log out.\n3. AVG browser AVG is a free browser with an integrated VPN service. It comes from AVG, a company that makes security software. The VPN service has more than 30, server locations, compatibility with the range of devices, and has different privacy browsing modes. But it is not free. You must pay to use some of the features.\nThe AVG browser is easy to use. After installing the app, click on VPN to browse and everything will be fine.\nAloha also provides phone-wide VPN support and auto-activation in its payment plan.\n4. Tor Browser Tor is not necessarily a VPN service, but it does provide anonymity. This is. It uses a slightly different method to protect your identity by routing your traffic through the open-source Tor network using multilayer encryption.\nThe main feature that makes Tor attractive to privacy freaks is that it blocks trackers, protects your identity online, and can also access sites that are censored at your location or blocked by your internet service provider ( ISP).\nTor is free to use on Android, and there are no restrictions on bandwidth or anything.\n5. Opera browser Opera has a free built-in VPN service and you don’t even need to create an account to get started. Just open the app, click on the Opera logo and select settings, then turn on VPN. The service has no bandwidth limit or intrusive advertisements. You can choose to use VPN in private mode and normal mode, or use it in private mode only.\nThe fact that it is baked in the browser makes it easy to use as well. Provides some options, allowing you to configure virtual locations in Asia, Europe, or America.\n6. Cake web browser The cake uses privacy and security methods similar to most browsers on this list. It includes password protection, non-tracking function, private label time bomb, and most importantly, free unlimited VPN service.\nAlthough Cake doesn’t limit bandwidth, unless you pay, you won’t be able to use all of its VPN features. The paid version provides the ability to select the server and device range of your choice.\n7. Aloha Browser Aloha is another Android browser with a built-in VPN service. Your VPN’s main features include unlimited traffic, hidden IP addresses to avoid tracking, DNS leak prevention, and logs of your activities that will not be saved.\nAloha provides more than 10 VPN servers in Asia, Europe, Africa, and Americas. However, in the free version, you cannot select a specific server location; this is a similar method to the Tenta browser.\nFor many reasons, VPN services are necessary. Maybe you want to bypass geo-restrictions or surf the Internet safely, or anything else you find in the VPN app. If you want a simple solution to surf the Internet anonymously, and you don’t need any serious matching a powerful independent VPN service, then a browser with a built-in VPN service will be useful.\n","description":"Use these Android web browsers that come with a free built-in VPN to enhance your privacy and security.A virtual private network (VPN for short) encrypts your internet activity and hides your location. Since most people use their portable devices (such as smartphones) to go online, they must have a VPN.This is a list of browsers with built-in VPN on Android, so you don’t need to download additional apps.","tags":["techs"],"title":"Best Android VPN browsers for privacy","uri":"/collections/techs/best-android-browser-for-privacy/"},{"content":"Starter guid to SSH What is SSH? SSH or Secure Shell allows people to access your server remotely. It uses encryption technology to ensure that all communications to and from the server are encrypted.\nIt provides you with a way to authenticate remote users, pass input from the client to the host, and pass information to the client.\nLinux and macOS users can use the terminal to connect directly to their remote server via SSH. Windows users should use tools like Putty.\nYou can use shell commands like you actually use a remote computer.\nStarting a SSH session. The SSH command is divided into three parts:\nssh {user}@{host} SSH tells the system that you want to open an encrypted connection. The user is the account you want to access (for example, root === admin). The host is the computer (IP address or domain) you want to access. After entering these, you will be prompted to enter your password. Although nothing is displayed on the screen, your password is being sent.\nSSH encryption technology. Symmetric Encryption Asymmetric Encryption\nHash\nSymmetric encryption This type of encryption uses a key for both parties to encrypt and decrypt the message.\nIt is usually called shared key or shared secret encryption.\nUsually, there is only one key, but you can also have a key pair, one of which can calculate the other.\nBoth the client and the server use the agreed method (key exchange algorithm) to obtain the key, and the key will never be shown to any third party.\nInformation shared between the server and the client can be intercepted by another machine, but the key exchange algorithm cannot be calculated.\nThere are many passwords to create the key, but before secure transmission, choose one based on the order of preference of the two machines.\nAsymmetric encryption Here we use two different keys for encryption and decryption.\nThis is like a one-way relationship. The public key is public, but you cannot find the private key based on the public key. The relationship between the two is complicated.\nThis one-way relationship means that the public key cannot decrypt its own messages, nor can it decrypt any information sent by the private key.\nIt is not used to encrypt the entire session, but only during the key exchange algorithm.\nBefore a secure connection, both machines will create temporary public-private keys and share them to create a shared key.\nOnce the secure symmetric communication is done, the server will use the client’s public key to challenge it and send it to the client for authentication.\nIf the client can decrypt the message, it means they have the private key. Then you can start an SSH session.\nHash A communication method that will never be cracked. The hash generates a unique fixed-length value for each character in the communication that is transmitted.\nThis makes reverse engineering nearly impossible (until quantum computing emerges..)\nIt is easy to generate a hash from the input, so if the client has the correct input, it can compare it with the hash and confirm that it has the correct input.\nSSH uses hashes to verify the validity of messages.\nHMAC : Hash-based message authentication code is used to ensure that the message has not been intercepted or modified.\nWhen choosing an asymmetric encryption algorithm, you must also design a message authentication algorithm, just like the password selection above.\nMac Each message must contain a MAC. This is calculated using the symmetric key, packet sequence number, and message content.\nSSH works on TCP port 22 by default. The server listens for incoming connections on port 22.\nAuthenticate the secure connection and start the shell environment.\nThe client machine must initiate the connection by initiating the TCP handshake with the server to confirm the secure symmetric connection.\nYou must verify that the server ID matches the above record normally stored in the RDA Keystore file.\nThe connection has two stages, first both machines must agree on the encryption standards and then the user has to be authenticated with their password etc.\nCommon SSH command 1. Access a remoter server\nssh username@hostname_or_ip 2. Use a Different Port Number for SSH Connection\nssh test.server.com -p 3322 3. Generate SSH Keys Using SSH Keygen\nssh-keygen -t rsa 4. Copy file remotely usig SCP\nscp file user@remotehost:/home/username/dest 5. Restart SSH service\nsudo ssh service restart ","description":"Welcome to our comprehensive tutorial to SSH (Secure Shell) key configuration.  This lesson will show you how to create SSH keys from scratch, as well as how to manage numerous keys and key pairs.","tags":["progamming"],"title":"Starter guid to SSH (Secure Shell)","uri":"/collections/programming/starter-guid-to-ssh/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 1","uri":"/notes/2020/change/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 2","uri":"/notes/2020/change2/"},{"content":"Journaling is not just a little thing you do to pass the time, to write down your memories—though it can be—it’s a strategy that has helped brilliant, powerful and wise people become better at what they do.\n","title":"Note 3","uri":"/notes/2020/change3/"}]
